{
  "paper": "A Novel End-to-End 3-D Residual and Attention Enhancement Joint Few-Shot Learning Model for Huntington Clinical Assessment.pdf",
  "answers": [
    {
      "id": "Q1",
      "question": "Which data type is used in this study?",
      "answer": "video",
      "raw_answer": "video",
      "choices_ids": [
        "tabular",
        "time-series",
        "images",
        "text",
        "video",
        "audio"
      ],
      "answer_label": "video",
      "chunks_id": [
        11,
        10,
        3
      ],
      "chunks_str": [
        " s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cutting\nmethods.\na) Frame difference method: the frame difference\nmethod was used [45] for data augmentation, by adding images\nwith significant differences between consecutive frames to the\ndataset. Therefore, high-quality images could be more effec-\ntively used by the model. Specifically, the frame difference\nmethod is a method of obtaining the contour of a mov-\ning object by performing differential operations on adjacent\nframes in a video image sequence. When there is abnor-\nmal object movement in the video, there will be significant\ndifferences between frames. Subtract two frames [I (x, y, t)\nand I (x, y, t − 1)] to obtain the absolute value of the pixel\ndifference [D(x, y, t)] between the two images and determine\nwhether it is greater than the threshold to determine whether\nthere is object motion in the image sequence, as shown in the\nfollowing formula:\nD(x, y, t) = |I (x, y, t) − I (x, y, t − 1)|. (8)\nFirst, we performed median filtering, binarization, and dila-\ntion corrosion on the grayscale image to extract the contour of\nthe object. Second, a motion selection frame was performed\nby comparing the difference D(x, y, t) between the front\nand back frames, the area with a motion difference value\nis greater than the set motion threshold was selected. As\nshown in Fig. 5, using the frame difference method allows\nnot only to distinguish the characters from the background\nbut also to extract the parts of the human body that have\nundergone obvious changes. It should be noted that the motion\nselection boxes will have a large number of overlapping areas.\nIn this study, the representative motion selection boxes were\nselected according to the nonmaximum-suppression (NMS)\nalgorithm [46]. The principle of the NMS algorithm is to retain\nthe detection box with the highest confidence score for the\nsame object among multiple detection boxes while suppressing\nother boxes with lower confidence scores. Finally, according to\nthe relationship between the number of motion-selected boxes\nin the image and the set threshold k, it is determined whether\nthe frame image needs to be copied. According to the ablation\nexperimental results, we determine the value of k to be 1,\nwhich can maximize the gain of model performance.\nb) Image cutting method: After processing by using the\nframe difference method, we could obtain a large amount\nof effective image data, and then, these data were subjected\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 5. Pixel intensity changes in a screenshot of an HD patient’s video with\na 1-s interval between the front and back. The",
        "follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut",
        " However,\nshortcomings are to be mentioned. Clinical scoring scales [4],\n[5] require well-trained neurologists to obtain reliable data, and\nthey are not easily adaptable for remote clinical assessment\n1) Although the reliability of biological marker-based [3]\ndiagnostic methods is good, these methods require\nexpensive instruments with inherent issues to be used\non a large scale. Use of wearable devices [7], [8] can\ngenerate data at a low cost. However, this method is a\ncontact method, which is not convenient for long-term\ntracking and analysis of patients, and the probability of\ninhomogeneous assessment is relatively high.\n2) Previously, several machine learning models [13], [17]\nhave been used in HD assessment, including the anal-\nysis models of wearable devices and voice. However,\nsome obvious defects hinder their application in clinical\npractice. For example, voice-based methods need to first\nextract voice features and then use statistics or machine\nlearning models for analysis. This two-stage method\nmay lead to unstable analysis results.\n3) HD is rare disease and the prevalence is low as 0.38 per\n100 000 person-years. Therefore, it is difficult to obtain\na large number of HD video data for traditional machine\nlearning models. Indeed, data scarcity usually results\nin unstable analysis of traditional machine learning\nmodel [22]. How to obtain accurate assessment results\nthrough few-shot data is very important to model devel-\nopment and clinical application.\nBased on the above considerations, we have developed\nan end-to-end few-shot video analysis model. The proposed\nFig. 1. Comparison of six pose examples of HD patients with normal poses,\nincluding: 1—upper bodies, 2—sitting, 3—front-facing standing, 4—side-fac-\ning standing, 5—back-facing standing, and 6—walking. To avoid revealing the\nidentity of the patients, we removed the eye region from their face images.\nmodel utilize facial details and body posture details [9], [23]\nto perform analysis, as shown in Fig. 1. Specifically, we first\ndesign a frame difference method to perform data enhancement\non some key video data. Second, we have embedded two\ntypes of effective attention modules into the 3-D convolutional\nnetwork to enhance the ability of the deep learning model\nto extract HD video features. Finally, we have employed the\nresidual structure to repeatedly stack the extracted features and\nthen use a fully connected layer to obtain the final diagnosis\nresult. Therefore, the main contributions of this article are\nsummarized as follows.\n1) We have developed an effective end-to-end high-\ndefinition diagnosis model based on posture videos.\nCompared with using gait or biomarker data, posture\nvideo-based data are more convenient and can provide\nreliable diagnostic results.\n2) Both spatial–temporal excitation attention and spatial–\nchannel attention modules have been incorporated into\nthe 3-D residual network to enhance its feature extrac-\ntion capabilities. In comparison with other machine\nlearning approaches, superior and more consistent per-\nformance is demonstrated by this architecture.\n3) We have designed a frame difference algorithm to\nimprove the video data through the data enhancement.\nThis new method can overcome issues related to small\nsample size and improve outcomes in this few-shot\nlearning task, thereby providing reliable assessment\noutcomes. Compared with other Huntington evaluation\nmodels, experiments confirm that our model can achieve\nbetter recall and accuracy.\nOverall, HD is a rare genetic disorder with a wide spectrum\nof signs and symptoms. There is an urgent need to develop\nan automatic assessment of HD in clinical practice. Several\nassessments have been developed including the analysis mod-\nels of clinical scoring scales, biological markers and wearable\ndevices, image, and voice. Although the above models can\nobtain promising results, some obvious defects hinder their"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2580
    },
    {
      "id": "Q2",
      "question": "Which type of digital health application is considered in this study?",
      "answer": "Unknown from this paper",
      "raw_answer": "Unknown from this paper.",
      "choices_ids": [
        "Precision Medicine",
        "Health IT",
        "Digital Medicine",
        "Telehealth",
        "Wellness"
      ],
      "answer_label": null,
      "chunks_id": [
        6,
        4,
        7
      ],
      "chunks_str": [
        " the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel level. Third, to better\nextract the spatial, temporal, and channel features from the\nfeature maps, we employed attention modules to optimize\nthe 3DResNet module. Specifically, we designed two bottle-\nneck modules: squeeze excitation (SE) attention modules and\nspatial–temporal excitation (STE) attention modules. By rea-\nsonably cross-embedding different bottleneck modules into\nthe 3DRes-Net model, our model can learn the interdepen-\ndencies among different dimensions, promote useful features,\nand suppress redundant features. Finally, all features were\nprocessed using a fully connected layer, and a softmax layer\nwas employed to obtain the probability output for each disease\ntype.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 2. Pipeline of the proposed 3-D RAJNet, which mainly consists of an improved residual network block, the spatial–channel attention model, and a\nfeature classifier.\nA. Upstream Feature Extraction Module\n1) Feature Pre-Extraction: For the few-shot learning task\nof HD disease clinical assessment, we must perform the\ndata enhancement and feature pre-extraction to avoid model\noverfitting and improve the generalization and robustness of\nthe model. In addition, since video information often include\nirrelevant background noise, we used the frame difference\nmethod [45] to further process the data. The frame difference\nmethod can prevent the model from learning information that\nis irrelevant to the target and further enhance the effective\ndata information. A detailed data enhancement algorithm is\nintroduced in Section IV.\nIn this module, the RGB images extracted from the orig-\ninal video are first converted into a grayscale map, and the\nframe-difference method is then used for data enhancement.\nGiven input video IM ∈ RT ∗C∗H∗W , our model first cuts the\nsize of the input image by half and then stacks it in the depth\ndimension, getting a processed video IM ∈ RT ∗4C∗H/2∗W/2, to\nachieve a larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64",
        " demonstrated by this architecture.\n3) We have designed a frame difference algorithm to\nimprove the video data through the data enhancement.\nThis new method can overcome issues related to small\nsample size and improve outcomes in this few-shot\nlearning task, thereby providing reliable assessment\noutcomes. Compared with other Huntington evaluation\nmodels, experiments confirm that our model can achieve\nbetter recall and accuracy.\nOverall, HD is a rare genetic disorder with a wide spectrum\nof signs and symptoms. There is an urgent need to develop\nan automatic assessment of HD in clinical practice. Several\nassessments have been developed including the analysis mod-\nels of clinical scoring scales, biological markers and wearable\ndevices, image, and voice. Although the above models can\nobtain promising results, some obvious defects hinder their\napplication in clinical practice. In the present study, we have\ndeveloped an effective proficient pose video-based end-to-end\ndiagnostic model for HD. This new method can overcome\nissues related to few-shot and improve outcomes in this\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nfew-shot learning task, thereby providing dependable and\noutstanding diagnostic outcomes. The remainder of this article\nis organized as follows. Section II introduces recent work\nrelated to HD diagnosis. Section III elaborates on the proposed\nframework, followed by extensive experimental results and\nanalysis in Section IV. Last, the conclusion is drawn in\nSection V.\nII. R ELATED WORKS\nA. Biological Markers-Based Model\nBiomarkers are very common detection methods in the clin-\nical assessment of HD [24]. Currently, several new biomarkers\nhave been developed [3] to detect symptoms of ND diseases.\nFor example, blood-based assessments have been reported as\npossible biomarkers for the evaluation of HD [25]. Given that\nlipid composition and disturbances in lipoprotein metabolism\nmay be associated with the pathogenesis of HD, the plasma\nlipoprotein profile has been proposed as predictive biomark-\ners for HD progression [3]. In addition, MRI [26] and\nEEG [27], [28] have been explored biomarkers for HD. Based\non EEG features associated with disease progression in HD,\nan automatic classifier has been developed to distinguish\nhealthy controls (HCs) from HD gene carriers. Similarly, MRI-\nbased [26] imaging biomarkers have been used to determine\nthe degree of morphological changes.\nB. Wearable Device-Based Clinical Assessment Model\nUnlike biomarker-based methods, wearable device-based\nmethods are less costly and more straightforward measure-\nments for disease evaluation. Indeed, wearable or portable\nsensors have been shown to be effective in HD signs assess-\nment signs [7], [29]. Based on the data collected by sensors,\nsuch as stride frequency, stride length, and walking speed,\nseveral algorithms have been used for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through",
        " larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2576
    },
    {
      "id": "Q3",
      "question": "To which ICD-10 code group does the digital health application in this study pertain?",
      "answer": "Unknown from this paper",
      "raw_answer": "Unknown from this paper.",
      "choices_ids": [
        "A00-B99",
        "C00-D48",
        "D50-D89",
        "E00-E90",
        "F00-F99",
        "G00-G99",
        "H00-H59",
        "I00-I99",
        "J00-J99",
        "K00-K93",
        "L00-L99",
        "M00-M99",
        "N00-N99",
        "O00-O99",
        "P00-P96",
        "Q00-Q99",
        "R00-R99",
        "S00-S99",
        "T00-T98",
        "U00-U99",
        "V01-Y98",
        "Z00-Z99"
      ],
      "answer_label": null,
      "chunks_id": [
        7,
        12,
        9
      ],
      "chunks_str": [
        " larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res",
        " the value of k to be 1,\nwhich can maximize the gain of model performance.\nb) Image cutting method: After processing by using the\nframe difference method, we could obtain a large amount\nof effective image data, and then, these data were subjected\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 5. Pixel intensity changes in a screenshot of an HD patient’s video with\na 1-s interval between the front and back. The right image occurs 1 s after\nthe left image.\nTABLE I\nCOMPARATIVE EXPERIMENTAL RESULTS WITH STATE\nOF THE ART AND OTHER TRADITIONAL MODELS\nto additional processing as follows. In particular: 1) crop\nthe original image into four small images, so the shape of\nthe image changes from (1, 512, 512) to (4, 256, 256) and\n2) the images obtained by slicing are reorganized, and every\n20 images were stacked into a 3-D matrix.\nSpecifically, we use indicators, such as recall rate and accu-\nracy, to evaluate the model [47]. Among them, the recall rate\nis of great significance for medical disease clinical assessment\nbecause it can reflect the situation of missed assessment in a\nmodel when diagnosing diseases. In medical disease clinical\nassessment, if a model fails to diagnose a disease, the patient\nmay miss the optimal treatment opportunity, leading to the\ndeterioration of the disease. Specifically, HD, as a relatively\nrare ND disease, has a high probability of missed assessment\nin clinical practice. However, our model has a high recall\nrate and high accuracy, which can reduce the risk of missed\nassessment to a certain extent, and accurately target clinical\npain points.\nB. Model Evaluation\nIn this section, we compare our model with the current\nmain methods for diagnosing HD. The comparison was con-\nducted on the results from different models (2DCNN [48] and\n3DCNN [49]) utilizing the same dataset. The other compared\nmethods can be found in [50], and the resulting outcomes are\ndisplayed in Table I.\nHere, the results marked with * in the 3DCNN and 2DCNN\nmodels are taken from the enhanced data: 1) shortcom-\nings of traditional models: the 2DCNN and 3DCNN obtain\npoor model performance. The SVM, Log. Regr, LDA, and\nQDA: a) invasive data acquisition; b) need to be trained\nfor enough epochs to obtain satisfactory results [50]; and\nc) low model performance and low cost-effectiveness and\n2) our contribution: a) better performance and low-missed\nassessment rate; b) end-to-end assessment; and c) low cost\nand convenience.\n1) Comparison With Other Models:First, the main meth-\nods for evaluating HD previously included scale analysis,\nfacial muscle group analysis model, and gait analysis for\ncomparison. Their shortcomings are very obvious. The scale\nanalysis uses the UHDRS diagnostic confidence level (DCL),\nwhich is based on the evaluation of motor signs is relatively\nlimited. The facial muscle group analysis uses methods, such\nas coordinate analysis and SVM, to analyze the amplitude\ncharacteristics of facial expressions and the shaking of facial\nsmall muscle groups. It is a non-end-to-end model with weak\ngeneralization ability. The gait analysis measures the severity\nof a patient’s illness based on high-level gait characteristics,\nrequiring patients to wear",
        ", which could\neffectively guarantee the robustness and stability of our model.\n1) SE Block: As shown in Fig. 4(a), as a kind of channel\nexcitation, the SE block can explicitly model the interde-\npendencies between feature channels. It can access global\ninformation and determine the importance of each feature\nchannel in two steps. This allows the network to promote\nuseful features and suppress irrelevant ones, enhancing its\nsensitivity to informative features.\nThe squeeze operation performs feature compression along\nthe spatial dimension, turning each 2-D feature channel into a\nreal number that somehow has a global perceptual field. The\noutput dimension matches the number of input feature chan-\nnels. It characterizes the global distribution of the response\nover the feature channels and makes the global perceptual field\navailable to the layers close to the input. This was accom-\nplished by performing global average pooling (GPooling) on\nthe original feature map C ∗W ∗H and obtaining a feature map\nof size 1∗1∗C with a global perceptual field. The processing of\nour channel attention module can be summarized as follows:\nFt+5 = Sig(FC(Relu(FC(GPooling(Ft+4)))) ∗ Ft+4). (6)\n2) STE Block: As shown in Fig. 4(b), the STE is another\nmodule that extracts spatiotemporal features from videos\nby generating a spatiotemporal attention map. Traditional\nspatial–temporal feature extraction mainly uses 3-D convo-\nlution; however, introducing 3-D convolution directly to the\ninput significantly increases the computational effort of the\nmodel. We first use a channel average on Ft+4 to get a\nglobal channel feature F′\nt+4 ∈ RN∗T ∗1∗H∗W , and then, F′\nt+4\nis reshaped into dimensions that can be manipulated by 3-D\nconvolution, i.e. (N, 1, T, H, W ), where N represents the\nbatch size, T denotes the data time dimension, the channel\ndimension C is compressed to 1, and H ∗ W denotes the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\ndata height and width, respectively. Subsequently, we use a\nconvolutional layer to extract the attention map from F′\nt+4.\nThis attention map is multiplied by Ft+4 to obtain the final\nspatial–temporal features Ft+5.\n3) Learning Strategy: In the final part of the model,\nwe leverage multihead attention to enhance the model’s classi-\nfication ability. Subsequently, we used a fully connected layer\nand a softmax function to complete the disease probability\noutput of the model. This process can be summarized as\nfollows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were "
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2687
    },
    {
      "id": "Q4",
      "question": "Does the predictive model perform regression or classification?",
      "answer": "Classification",
      "raw_answer": "Classification",
      "choices_ids": [
        "Regression",
        "Classification"
      ],
      "answer_label": "Classification",
      "chunks_id": [
        5,
        14,
        15
      ],
      "chunks_str": [
        " for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel",
        " the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVIDUAL COMPONENTS\nFig. 6 shows that the AUC areas of both 2DCNN and 3DCNN\nmodels are below 0.5, and their poor recall performance makes\nthem unsuitable for clinical assessment. Our model has an\nAUC area of up to 0.76, completely enveloping the baseline,\nindicating that our model not only outperforms other models\nbut also performs well at any judgment threshold.\nC. Ablation Study\nIn this section, we conducted ablation experiments on data\naugmentation, attention module, and the number of motion\nselection boxes, and the results are shown in Table II. Here,\nSE–ST represents SE block and STE block in our model,\nand SpaChan represents multihead attention enhancement in\nour model. Both the SpaChan module and SE module. Null\nrepresents that the attention module is not used. K represents\nthe motion selection box threshold.\n1) Data Enhancement Comparison: To demonstrate the\neffectiveness of the frame difference data augmentation and\ncutting method, we plotted a performance comparison graph\nTABLE III\nCOMPUTATIONAL EFFICIENCY AND COMPLEXITY OF\nEACH COMPONENT OF THE MODEL\nFig. 7. Performance comparison of different models before and after data\naugmentation. The model * represents the performance improvement of the\nmodel after data enhancement, and the part with diagonal lines on the column\nrepresents a decrease in numerical values.\nof 2DCNN, 3DCNN, and our model before and after data\naugmentation, as shown in Fig. 7.\nAccording to the survey results in Table II, the performance\nof our model has been comprehensively improved after imple-\nmenting data augmentation techniques. Moreover, in order to\nexhibit the efficacy of the frame difference data augmentation\nand cutting techniques, we illustrate the performance com-\nparison charts of 2DCNN, 3DCNN, and our model pre and\npostdata augmentation, as demonstrated in Fig. 7. It appears\nthat the 2DCNN model is incapable of effectively deriving\nbenefits from data augmentation, owing to the model’s limited\ncapacity for feature mining. Nevertheless, other models display\na substantial improvement as a result of data augmentation.\nThe implementation of data augmentation was found to sig-\nnificantly decrease the loss of 3DCNN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same",
        "NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2574
    }
  ]
}
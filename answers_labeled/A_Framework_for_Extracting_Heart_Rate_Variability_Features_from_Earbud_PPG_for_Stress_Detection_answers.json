{
  "paper": "A Framework for Extracting Heart Rate Variability Features from Earbud-PPG for Stress Detection.pdf",
  "answers": [
    {
      "id": "Q1",
      "question": "Which data type is used in this study?",
      "answer": "time-series",
      "raw_answer": "time-series",
      "choices_ids": [
        "tabular",
        "time-series",
        "images",
        "text",
        "video",
        "audio"
      ],
      "answer_label": "time-series",
      "chunks_id": [
        2,
        6,
        3
      ],
      "chunks_str": [
        " device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN",
        " for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI",
        " for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2572
    },
    {
      "id": "Q2",
      "question": "Which type of digital health application is considered in this study?",
      "answer": "Unknown from this paper",
      "raw_answer": "Unknown from this paper.",
      "choices_ids": [
        "Precision Medicine",
        "Health IT",
        "Digital Medicine",
        "Telehealth",
        "Wellness"
      ],
      "answer_label": null,
      "chunks_id": [
        4,
        7,
        2
      ],
      "chunks_str": [
        "Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our",
        " detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI conference on human factors in computing systems. 2016.\n[6] Mishra, Varun, et al. ”Continuous detection of physiological stress with\ncommodity hardware.” ACM transactions on computing for healthcare\n1.2 (2020): 1-30.\n[7] Lee, Joong Hoon, et al. ”Stress monitoring using multimodal bio-sensing\nheadset.” Extended Abstracts of the 2020 CHI Conference on Human\nFactors in Computing Systems. 2020.\n[8] Song, J., Li, D., Ma, X., Teng, G. and Wei, J., 2019. PQR signal quality\nindexes: A method for real-time photoplethysmogram signal quality\nestimation based on noise interferences. Biomedical Signal Processing\nand Control, 47, pp.88-95.\n[9] Lea, C., Vidal, R., Reiter, A., & Hager, G. D. (2016). Temporal convolu-\ntional networks: A unified approach to action segmentation. In Computer\nVision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October\n8-10 and 15-16, 2016, Proceedings, Part III 14 (pp. 47-54). Springer\nInternational Publishing.\n[10] Van Gent, P., Farah, H., Van Nes, N., & Van Arem, B. (2019).\nHeartPy: A novel heart rate algorithm for the analysis of noisy signals.\nTransportation research part F: traffic psychology and behaviour, 66,\n368-378.\n[11] van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2019). Analysing\nnoisy driver physiology real-time using off-the-shelf sensors: Heart rate\nanalysis software from the taking the fast lane project. Journal of Open\nResearch Software, 7(1).\n[12] Jing, L., & Tian, Y . (2020). Self-supervised visual feature learning with\ndeep neural networks: A survey. IEEE transactions on pattern analysis\nand machine intelligence, 43(11), 4037-4058\n[13] Ziegler, A., & Asano, Y . M. (2022). Self-supervised learning of object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[",
        " device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2573
    },
    {
      "id": "Q3",
      "question": "To which ICD-10 code group does the digital health application in this study pertain?",
      "answer": "Unknown from this paper",
      "raw_answer": "Unknown from this paper.",
      "choices_ids": [
        "A00-B99",
        "C00-D48",
        "D50-D89",
        "E00-E90",
        "F00-F99",
        "G00-G99",
        "H00-H59",
        "I00-I99",
        "J00-J99",
        "K00-K93",
        "L00-L99",
        "M00-M99",
        "N00-N99",
        "O00-O99",
        "P00-P96",
        "Q00-Q99",
        "R00-R99",
        "S00-S99",
        "T00-T98",
        "U00-U99",
        "V01-Y98",
        "Z00-Z99"
      ],
      "answer_label": null,
      "chunks_id": [
        4,
        7,
        3
      ],
      "chunks_str": [
        "Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our",
        " detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI conference on human factors in computing systems. 2016.\n[6] Mishra, Varun, et al. ”Continuous detection of physiological stress with\ncommodity hardware.” ACM transactions on computing for healthcare\n1.2 (2020): 1-30.\n[7] Lee, Joong Hoon, et al. ”Stress monitoring using multimodal bio-sensing\nheadset.” Extended Abstracts of the 2020 CHI Conference on Human\nFactors in Computing Systems. 2020.\n[8] Song, J., Li, D., Ma, X., Teng, G. and Wei, J., 2019. PQR signal quality\nindexes: A method for real-time photoplethysmogram signal quality\nestimation based on noise interferences. Biomedical Signal Processing\nand Control, 47, pp.88-95.\n[9] Lea, C., Vidal, R., Reiter, A., & Hager, G. D. (2016). Temporal convolu-\ntional networks: A unified approach to action segmentation. In Computer\nVision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October\n8-10 and 15-16, 2016, Proceedings, Part III 14 (pp. 47-54). Springer\nInternational Publishing.\n[10] Van Gent, P., Farah, H., Van Nes, N., & Van Arem, B. (2019).\nHeartPy: A novel heart rate algorithm for the analysis of noisy signals.\nTransportation research part F: traffic psychology and behaviour, 66,\n368-378.\n[11] van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2019). Analysing\nnoisy driver physiology real-time using off-the-shelf sensors: Heart rate\nanalysis software from the taking the fast lane project. Journal of Open\nResearch Software, 7(1).\n[12] Jing, L., & Tian, Y . (2020). Self-supervised visual feature learning with\ndeep neural networks: A survey. IEEE transactions on pattern analysis\nand machine intelligence, 43(11), 4037-4058\n[13] Ziegler, A., & Asano, Y . M. (2022). Self-supervised learning of object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[",
        " for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2693
    },
    {
      "id": "Q4",
      "question": "Does the predictive model perform regression or classification?",
      "answer": "Classification",
      "raw_answer": "Classification",
      "choices_ids": [
        "Regression",
        "Classification"
      ],
      "answer_label": "Classification",
      "chunks_id": [
        6,
        5,
        3
      ],
      "chunks_str": [
        " for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI",
        "28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our knowledge,\nHeartPy is the most prominent and trusted public library to\nprovide HRV features from PPG signals. For the prediction\nof all the HRV features, we use a 4-layer TCN to get the\ninput representation from the PPG signal that, in turn is\npassed through 2 Fully Connected (FC) layers with hidden\nsizes of 64 to get the output prediction. We train the model\nusing a learning rate of 5e-05 and a dropout value of 0.3.\nWe evaluate the performance of our model against HeartPy\nusing mean absolute error (MAE) with the estimates from the\nBiopac sensor data considered the ground truth. We report\nthese results in Table I. Among all the parameters predicted,\nwe observe that for heart rate alone, HeartPy outperforms our\nproposed framework. For the remaining HRV parameters, our\nmodel performs better than HeartPy prediction. On average,\nthe improvement in HRV parameter estimation compared to\nHeartPy is around 26%.\nTo highlight the significance of fine-tuning, we predicted\nHRV features from the earbud PPG without any pre-training\nstep. The results of this experiment are shown in Table I.\nWith the exception of HR, we observe that transfer learning\nimproves performance for all the HRV features.\nThroughout all our experiments, we consistently observed\nrelatively poorer performance in average HR estimation using\nour proposed framework. This may be related to the fact that\nheart rate estimation is fundamentally different from HRV\nparameter estimation. While HRV estimation depends on the\nspecific sequence of individual peaks, HR is more of an\naverage of the distances between peaks. The framework was\nnot optimized for this averaging task and the removal of\noutliers that would adversely affect the average. Nevertheless,\nheart rate is an important feature and integrating its estimation\nin a unified framework can be the focus of future work.\nB. Stress detection\nIn this section, we report the performance of stress clas-\nsification using HRV parameters obtained from HeartPy and\nour model, respectively. The PPG window from a subject is\nlabeled as stress if it is sampled from the task that induces\nstress, which in our case are Dot-tracking, Speech-Preparation,\nSpeech, and Exciting Music tasks (see Figure 1 for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal",
        " for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2552
    }
  ]
}
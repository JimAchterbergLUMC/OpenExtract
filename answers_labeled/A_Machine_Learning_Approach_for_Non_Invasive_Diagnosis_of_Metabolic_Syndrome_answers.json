{
  "paper": "A Machine Learning Approach for Non-Invasive Diagnosis of Metabolic Syndrome.pdf",
  "answers": [
    {
      "id": "Q1",
      "question": "Which data type is used in this study?",
      "answer": "tabular",
      "raw_answer": "tabular",
      "choices_ids": [
        "tabular",
        "time-series",
        "images",
        "text",
        "video",
        "audio"
      ],
      "answer_label": "tabular",
      "chunks_id": [
        5,
        7,
        2
      ],
      "chunks_str": [
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis",
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates",
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2540
    },
    {
      "id": "Q2",
      "question": "Which type of digital health application is considered in this study?",
      "answer": "Unknown from this paper",
      "raw_answer": "Unknown from this paper.",
      "choices_ids": [
        "Precision Medicine",
        "Health IT",
        "Digital Medicine",
        "Telehealth",
        "Wellness"
      ],
      "answer_label": null,
      "chunks_id": [
        6,
        10,
        2
      ],
      "chunks_str": [
        " classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Discriminative Performance: The primary metric that\nwe use to judge the discriminative power of a model is\nthe Area Under the Curve (AUC) of the receiver operating\ncharacteristic (ROC) curve. Models with ideal discrimination\npower have an AUC of 1.0 and the ones with no discrimi-\nnatory ability have AUC equal 0.5. Along with AUC we also\nreport balanced accuracy (or average class accuracy), precision\n(or positive predictive value) and recall (or true positive\nrate). Balanced accuracy is calculated as the average of the\nproportion of correct predictions for each class individually.\nBalanced accuracy is chosen ahead of regular accuracy as it\nprovides a better overview of the model performance than the\nclassiﬁcation accuracy when the classes are not balanced [25].\nPrecision is the proportion of positive identiﬁcations by the\nmodel that were actually correct. Recall is the proportion of\nactual positives that were identiﬁed correctly.\nIt should be noted, that AUC is the strongest indicator of the\nmodel performance among all the discriminative performance\nmetrics that we report here. AUC is statistically consistent and\nmore discriminating than accuracy [26]. Balanced accuracy,\nprecision and recall are all dependent on the threshold we\nchoose to apply on the probability estimations of the class\nprediction to get the class labels. Choosing an appropriate\ndecision threshold is highly dependent on the desired outcome\nof the predictive model (high speciﬁcity or high sensitivity).\nSince our goal is to give a probability score for MetS, AUC\nis more relevant for us. For computing balanced accuracy,\nprecision and recall, a decision threshold of 0.5 is assumed.\nA 5-fold cross-validation was performed to get a more robust\nestimate of the validation metrics. The hyper-parameters for\nthe different classiﬁers were optimized empirically.\n2) Model Calibration: Model calibration refers to the\nagreement between subgroups of predicted probabilities and\ntheir observed frequencies [27]. For predictive models in\nmedicine, usually the predicted probabilities of the different\nclass labels are important as they show how conﬁdent the\nmodel is about the predictions. Models like LR are usually\nwell calibrated as it directly optimizes the log-loss. But many\nother machine learning models like RF are thought to be\npoorly calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning",
        "-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.",
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2550
    },
    {
      "id": "Q3",
      "question": "To which ICD-10 code group does the digital health application in this study pertain?",
      "answer": "Unknown from this paper",
      "raw_answer": "Unknown from this paper.",
      "choices_ids": [
        "A00-B99",
        "C00-D48",
        "D50-D89",
        "E00-E90",
        "F00-F99",
        "G00-G99",
        "H00-H59",
        "I00-I99",
        "J00-J99",
        "K00-K93",
        "L00-L99",
        "M00-M99",
        "N00-N99",
        "O00-O99",
        "P00-P96",
        "Q00-Q99",
        "R00-R99",
        "S00-S99",
        "T00-T98",
        "U00-U99",
        "V01-Y98",
        "Z00-Z99"
      ],
      "answer_label": null,
      "chunks_id": [
        6,
        5,
        15
      ],
      "chunks_str": [
        " classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Discriminative Performance: The primary metric that\nwe use to judge the discriminative power of a model is\nthe Area Under the Curve (AUC) of the receiver operating\ncharacteristic (ROC) curve. Models with ideal discrimination\npower have an AUC of 1.0 and the ones with no discrimi-\nnatory ability have AUC equal 0.5. Along with AUC we also\nreport balanced accuracy (or average class accuracy), precision\n(or positive predictive value) and recall (or true positive\nrate). Balanced accuracy is calculated as the average of the\nproportion of correct predictions for each class individually.\nBalanced accuracy is chosen ahead of regular accuracy as it\nprovides a better overview of the model performance than the\nclassiﬁcation accuracy when the classes are not balanced [25].\nPrecision is the proportion of positive identiﬁcations by the\nmodel that were actually correct. Recall is the proportion of\nactual positives that were identiﬁed correctly.\nIt should be noted, that AUC is the strongest indicator of the\nmodel performance among all the discriminative performance\nmetrics that we report here. AUC is statistically consistent and\nmore discriminating than accuracy [26]. Balanced accuracy,\nprecision and recall are all dependent on the threshold we\nchoose to apply on the probability estimations of the class\nprediction to get the class labels. Choosing an appropriate\ndecision threshold is highly dependent on the desired outcome\nof the predictive model (high speciﬁcity or high sensitivity).\nSince our goal is to give a probability score for MetS, AUC\nis more relevant for us. For computing balanced accuracy,\nprecision and recall, a decision threshold of 0.5 is assumed.\nA 5-fold cross-validation was performed to get a more robust\nestimate of the validation metrics. The hyper-parameters for\nthe different classiﬁers were optimized empirically.\n2) Model Calibration: Model calibration refers to the\nagreement between subgroups of predicted probabilities and\ntheir observed frequencies [27]. For predictive models in\nmedicine, usually the predicted probabilities of the different\nclass labels are important as they show how conﬁdent the\nmodel is about the predictions. Models like LR are usually\nwell calibrated as it directly optimizes the log-loss. But many\nother machine learning models like RF are thought to be\npoorly calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning",
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis",
        " a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D. G. Altman and P. Royston, “The cost of dichotomising continuous\nvariables,”Bmj, vol. 332, no. 7549, p. 1080, 2006.\n[22] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.\n5–32, 2001.\n[23] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, “Classiﬁca-\ntion and regression trees. belmont, ca: Wadsworth,”International Group,\nvol. 432, pp. 151–166, 1984.\n939\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\n[24] J. H. Friedman, “Greedy function approximation: a gradient boosting\nmachine,”Annals of statistics, pp. 1189–1232, 2001.\n[25] J. D. Kelleher, B. Mac Namee, and A. D’arcy,Fundamentals of machine\nlearning for predictive data analytics: algorithms, worked examples, and\ncase studies. MIT Press, 2015.\n[26] C. X. Ling, J. Huang, H. Zhang et al., “Auc: a statistically consistent\nand more discriminating measure than accuracy,” inIjcai, vol. 3, 2003,\npp. 519–524.\n[27] F. J. Dankers, A. Traverso, L. Wee, and S. M. van Kuijk, “Prediction\nmodeling methodology,” in Fundamentals of Clinical Data Science.\nSpringer, 2019, pp. 101–120.\n[28] A. Niculescu-Mizil and R. Caruana, “Predicting good probabilities\nwith supervised learning,” in Proceedings of the 22nd international\nconference on Machine learning. ACM, 2005, pp. 625–632.\n[29] M. H. DeGroot and S. E. Fienberg, “The comparison and evaluation\nof forecasters,”Journal of the Royal Statistical Society: Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncompar"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2668
    },
    {
      "id": "Q4",
      "question": "Does the predictive model perform regression or classification?",
      "answer": "Classification",
      "raw_answer": "Classification",
      "choices_ids": [
        "Regression",
        "Classification"
      ],
      "answer_label": "Classification",
      "chunks_id": [
        7,
        2,
        10
      ],
      "chunks_str": [
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates",
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        "-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig."
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2525
    }
  ]
}
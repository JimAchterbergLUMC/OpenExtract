{
  "paper": "A Machine Learning Approach to Classifying Self-Reported Health Status in a Cohort of Patients With Heart Disease Using Activity Tracker Data.pdf",
  "answers": [
    {
      "id": "Q1",
      "question": "Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?",
      "answer": "prognostic",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        11,
        3
      ],
      "chunks_str": [
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3",
        " future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart",
        " such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices, including steps,\nheart rate, calories burned, and distance traveled. At the end\nof each week, they were asked to ﬁll out eight PROMIS short\nforms as a self-report assessment of their health status [4].\nA. Activity Data\nThe Fitbit Charge 2 (Fitbit Inc., San Francisco, CA, USA)\nis a popular commercially available activity tracker that can\nrecord a person’s daily activities and health indices like heart\nrate, steps, and sleep ( Table I ). Previous work has validated\nthe accuracy of heart rate monitoring speciﬁcally in the Fitbit\nCharge 2 [29]. The Fitbit hardware and its computational algo-\nrithms for calculating step counts and physical activity have been\nT ABLE I\nSUMMARY OF 17 T YPES OF FEA TURECOLLECTED FROM FITBIT PER DAY\n∗means that feature was eliminated for model input because it was highly sparse or redun-\ndant.\nvalidated using other Fitbit devices [30], [31]. The Fitbit Charge\n2 estimates activity using metabolic equivalents (METs), which\nare calculated based on heart rate and distance traveled [32].\nHeart rate during activity is also provided, however it has been\nshown to be inaccurate during activity [33]. Data quality was\nassured by verifying that there were no extreme outliers based\non subject-speciﬁc inter-quartile range [34]. We aggregated the\ndata for each day to compensate for noise and redundancy. Af-\nter data preprocessing, tracker distance was eliminated because\nit was identical to total distance, and logged activity distance\nand sedentary active distance were also deleted because of high\nsparsity. As a result, there were 14 features per day for each\npatient in our model.\nB. Patient-Reported Outcome Measures\nPatient-Reported Outcomes Measurement Information Sys-\ntems (PROMIS) questionnaires are a library of instruments\ndeveloped and validated to measure many domains of phys-\nical and mental health [15]. This analysis uses data from\neight PROMIS instruments: Global Physical Health and Global\nMental Health, which are two composite scores from the Global-\n10 short form [35]; Fatigue-Short Form 4a; Physical Function-\nShort Form 10a; Emotional Distress-Anxiety-Short Form 6a;\nDepression-Short Form 4a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2573
    },
    {
      "id": "Q2",
      "question": "On what basis were eligible participants included in this study (symptons, previous tests, registry, etc.)?",
      "answer": "patients with SIHD recruited for feasibility study",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        3,
        11,
        13
      ],
      "chunks_str": [
        " such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices, including steps,\nheart rate, calories burned, and distance traveled. At the end\nof each week, they were asked to ﬁll out eight PROMIS short\nforms as a self-report assessment of their health status [4].\nA. Activity Data\nThe Fitbit Charge 2 (Fitbit Inc., San Francisco, CA, USA)\nis a popular commercially available activity tracker that can\nrecord a person’s daily activities and health indices like heart\nrate, steps, and sleep ( Table I ). Previous work has validated\nthe accuracy of heart rate monitoring speciﬁcally in the Fitbit\nCharge 2 [29]. The Fitbit hardware and its computational algo-\nrithms for calculating step counts and physical activity have been\nT ABLE I\nSUMMARY OF 17 T YPES OF FEA TURECOLLECTED FROM FITBIT PER DAY\n∗means that feature was eliminated for model input because it was highly sparse or redun-\ndant.\nvalidated using other Fitbit devices [30], [31]. The Fitbit Charge\n2 estimates activity using metabolic equivalents (METs), which\nare calculated based on heart rate and distance traveled [32].\nHeart rate during activity is also provided, however it has been\nshown to be inaccurate during activity [33]. Data quality was\nassured by verifying that there were no extreme outliers based\non subject-speciﬁc inter-quartile range [34]. We aggregated the\ndata for each day to compensate for noise and redundancy. Af-\nter data preprocessing, tracker distance was eliminated because\nit was identical to total distance, and logged activity distance\nand sedentary active distance were also deleted because of high\nsparsity. As a result, there were 14 features per day for each\npatient in our model.\nB. Patient-Reported Outcome Measures\nPatient-Reported Outcomes Measurement Information Sys-\ntems (PROMIS) questionnaires are a library of instruments\ndeveloped and validated to measure many domains of phys-\nical and mental health [15]. This analysis uses data from\neight PROMIS instruments: Global Physical Health and Global\nMental Health, which are two composite scores from the Global-\n10 short form [35]; Fatigue-Short Form 4a; Physical Function-\nShort Form 10a; Emotional Distress-Anxiety-Short Form 6a;\nDepression-Short Form 4a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on",
        " future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart",
        " students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen Hsich et al. , “Identifying important risk factors for survival\nin systolic heart failure patients using random survival forests,” Circ.\nCardiovascular Qual. Outcomes , vol. 4, no. 1, pp. 39–45, 2014.\n[21] N. Limsopatham, C. Macdonald, and I. Ounis, “Learning to combine\nrepresentations for medical records search,” in Proc. 36th Int. ACM SIGIR\nConf. Res. Develop. Inf. Retrieval , 2013, pp. 833–836.\n[22] J. H. Morra, Zhuowen Tu, L. G. Apostolova, A. E. Green, A. W. Toga, and\nP . M. Thompson, “Comparison of AdaBoost and support vector machines\nfor detecting alzheimer’s disease through automated hippocampal seg-\nmentation,” IEEE Trans. Med. Imag. , vol. 29, no. 1, pp. 30–43, Jan. 2010.\n[23] H. Ishwaran, U. B. Kogalur, E. H. Blackstone, and M. S. Lauer, “Random\nsurvival forests,” Ann. Appl. Statist. , vol. 2, no. 3, pp. 841–860, 2008.\n[24] S. Shen et al. , “A Bayesian model for estimating multi-state disease pro-\ngression,” Comput. Biol. Med. , vol. 81, pp. 111–120, 2017.\n[25] B. Schuster-B ¨ockler and A. Bateman, “An introduction to hidden Markov\nmodels,” Current Protocols Bioinf. , vol. 18, pp. A.3A.1–A.3A.9, 2007.\n[26] E. Birney Clamp and R. M. Durbin, “Genewise and genomewise,” Genome\nRes., vol. 14, no. 4, pp. 988–995, 2004.\n[27] L. A. Cadmus-bertram, B. H. Marcus, R. E. Patterson, B. A. Parker, and B.\nL. Morey, “Physical activity intervention for women,” Am. J. Preventive\nMed., vol. 49, no. 3, pp. 414–418, 2015.\n[28] J. B. Wang et al. , “Wearable sensor/device (ﬁtbit one) and SMS text-\nmessaging prompts to increase physical activity in overweight and obese\nadults: A randomized controlled trial,” T elemedicine e-Health, vol. 21, no.\n10, pp. 18–23, 2014.\n[29] S. Benedetto, C. Caldato, E. Bazzan, D. C. Greenwood, V . Pensabene,\nand P . Actis, “Assessment of the ﬁtbit"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2578
    },
    {
      "id": "Q3",
      "question": "How were participants sampled in this study: by convenience, randomly, or consecutively?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        6,
        2
      ],
      "chunks_str": [
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3",
        " transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENCE ALGORITHMS\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement over GBRT.\n†Signiﬁcant improvement over both GBRT and AdaBoost.\nclassiﬁed state,\nS (yt |yt−1 ,...,y 1 ,x t ,...,x 1 )= P (xt |yt )\n∗\n∑\nP( yt |yt−1 ) ∗S( yt−1 |yt−2 ,..., xt−1 ,... ) (1)\nwhere the weekly PRO score was treated as state yt , with obser-\nvation of features xt . The emission probability, P (xt |yt ), com-\nputed the probability of the observed feature vector xt given\nstate yt , computed from the random forest classiﬁer and P (yt ):\nP (xt |yt ) ∝ P (yt |xt )\nP (yt ) (2)\nAt the ﬁrst-time step, the transition probability distribution is\nundeﬁned, so the state probability was:\nS (y1 |x1 ) ∝ P (x1 |y1 ) P (y1 ) (3)\nFor analysis, states were binarized according to the criteria\ndeﬁned above. Because dichotomizing PRO score values loses\nsome information and precision, a regression analysis was con-\nducted between the median value of HMM stages and actual\nscores for the HMM. This method of predicting PRO scores\nwas compared against multinomial logistic regression to evalu-\nate the accuracy of predicting PRO scores over time.\nIV . R ESULTS\nTable II shows the mean AUC for binary classiﬁcation of\nPRO scores for the seven PROMIS measures using GBRT, Ad-\naBoost and RF. The highest mean AUC was 0.75 using RF for\nclassifying Physical Function, while the lowest was 0.47 using\nAdaBoost for Depression. The results indicated that RF signif-\nicantly outperformed other models in classiﬁcation of Anxiety\nand Depression (p < 0.05), and it was also signiﬁcantly better\nthan GBRT for Global Physical Health and Mental Health (p =\n0.01 and p = 0.01, respectively). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps,",
        " experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2560
    },
    {
      "id": "Q4",
      "question": "How was the dataset described in this study before predictive modeling was performed?",
      "answer": "182 subjects with 1,640 weeks of evaluable data.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        9,
        5
      ],
      "chunks_str": [
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3",
        " used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach",
        "17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig. 2. Histogram of number of weeks of evaluable data for the 182\nsubjects used in the dataset.\nweeks of evaluable data for each patient ranged from two to 12\nweeks as shown in Fig. 2 .\nA. Independent Per Week Model by Machine Learning\nAlgorithms\nSince survey scores were generated per week, a naive ap-\nproach for using this data is to treat each week independently.\nT h el e f tp l o ti n Fig. 3 illustrates the idea of the independent\nmodel as an example for one subject with a number of weeks\nof evaluable data of 12 weeks. The features for each of the\nseven days were appended into a single feature vector, which\nwas then used as the input for binary classiﬁcation of each PRO\nscore. Ensemble methods like Adaboost, GBRT (gradient boost-\ning regression tree) and Random Forest (RF) are relatively ro-\nbust over unbalanced dataset and is capable of generating better\nclassiﬁcation accuracy than other types of machine learning al-\ngorithms [37]. Each of these methods was applied to the dataset\nusing ten-fold cross-validation across subjects in conjunction\nwith grid search to ﬁnd the optimal parameters for each model.\nT-tests were applied to validate the statistical signiﬁcance for\neach comparison of the result with different p values: 0.05, 0.01\nas for different levels of signiﬁcance. A sensitivity analysis was\ncompleted to investigate the model performance against missing\nvalues in feature vector by randomly withholding values from\none to six days within a week.\nB. Hidden Markov Model (HMM) With Forward Algorithm\nIn order to track changes in PRO responses over time, a model\nwas built to incorporate temporal correlations of PRO scores\nacross weeks. As shown in the right part of Figure 3 ,a nH M M\nwas used and formalized such that the state at each time point\ncorresponded to the PRO score for that week, with the features\ncollected for that week treated as observations. The transition\nmatrix was derived by counting the s state transitions from week\nto week. The original number of states for each PRO was found\nby number of unique responses, ranging from 15 to 36. In order\nto make the transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2551
    },
    {
      "id": "Q5",
      "question": "What was the proedure for splitting the dataset into training, validation, and test sets in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        9,
        5
      ],
      "chunks_str": [
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3",
        " used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach",
        "17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig. 2. Histogram of number of weeks of evaluable data for the 182\nsubjects used in the dataset.\nweeks of evaluable data for each patient ranged from two to 12\nweeks as shown in Fig. 2 .\nA. Independent Per Week Model by Machine Learning\nAlgorithms\nSince survey scores were generated per week, a naive ap-\nproach for using this data is to treat each week independently.\nT h el e f tp l o ti n Fig. 3 illustrates the idea of the independent\nmodel as an example for one subject with a number of weeks\nof evaluable data of 12 weeks. The features for each of the\nseven days were appended into a single feature vector, which\nwas then used as the input for binary classiﬁcation of each PRO\nscore. Ensemble methods like Adaboost, GBRT (gradient boost-\ning regression tree) and Random Forest (RF) are relatively ro-\nbust over unbalanced dataset and is capable of generating better\nclassiﬁcation accuracy than other types of machine learning al-\ngorithms [37]. Each of these methods was applied to the dataset\nusing ten-fold cross-validation across subjects in conjunction\nwith grid search to ﬁnd the optimal parameters for each model.\nT-tests were applied to validate the statistical signiﬁcance for\neach comparison of the result with different p values: 0.05, 0.01\nas for different levels of signiﬁcance. A sensitivity analysis was\ncompleted to investigate the model performance against missing\nvalues in feature vector by randomly withholding values from\none to six days within a week.\nB. Hidden Markov Model (HMM) With Forward Algorithm\nIn order to track changes in PRO responses over time, a model\nwas built to incorporate temporal correlations of PRO scores\nacross weeks. As shown in the right part of Figure 3 ,a nH M M\nwas used and formalized such that the state at each time point\ncorresponded to the PRO score for that week, with the features\ncollected for that week treated as observations. The transition\nmatrix was derived by counting the s state transitions from week\nto week. The original number of states for each PRO was found\nby number of unique responses, ranging from 15 to 36. In order\nto make the transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2551
    },
    {
      "id": "Q6",
      "question": "What preprocessing techniques on the included variables/features were applied in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        5,
        10
      ],
      "chunks_str": [
        "). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly",
        "17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig. 2. Histogram of number of weeks of evaluable data for the 182\nsubjects used in the dataset.\nweeks of evaluable data for each patient ranged from two to 12\nweeks as shown in Fig. 2 .\nA. Independent Per Week Model by Machine Learning\nAlgorithms\nSince survey scores were generated per week, a naive ap-\nproach for using this data is to treat each week independently.\nT h el e f tp l o ti n Fig. 3 illustrates the idea of the independent\nmodel as an example for one subject with a number of weeks\nof evaluable data of 12 weeks. The features for each of the\nseven days were appended into a single feature vector, which\nwas then used as the input for binary classiﬁcation of each PRO\nscore. Ensemble methods like Adaboost, GBRT (gradient boost-\ning regression tree) and Random Forest (RF) are relatively ro-\nbust over unbalanced dataset and is capable of generating better\nclassiﬁcation accuracy than other types of machine learning al-\ngorithms [37]. Each of these methods was applied to the dataset\nusing ten-fold cross-validation across subjects in conjunction\nwith grid search to ﬁnd the optimal parameters for each model.\nT-tests were applied to validate the statistical signiﬁcance for\neach comparison of the result with different p values: 0.05, 0.01\nas for different levels of signiﬁcance. A sensitivity analysis was\ncompleted to investigate the model performance against missing\nvalues in feature vector by randomly withholding values from\none to six days within a week.\nB. Hidden Markov Model (HMM) With Forward Algorithm\nIn order to track changes in PRO responses over time, a model\nwas built to incorporate temporal correlations of PRO scores\nacross weeks. As shown in the right part of Figure 3 ,a nH M M\nwas used and formalized such that the state at each time point\ncorresponded to the PRO score for that week, with the features\ncollected for that week treated as observations. The transition\nmatrix was derived by counting the s state transitions from week\nto week. The original number of states for each PRO was found\nby number of unique responses, ranging from 15 to 36. In order\nto make the transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN",
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2540
    },
    {
      "id": "Q7",
      "question": "How is missing data handled in this study?",
      "answer": "Subjects with only one week eliminated.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        2,
        5,
        11
      ],
      "chunks_str": [
        " experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices",
        "17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig. 2. Histogram of number of weeks of evaluable data for the 182\nsubjects used in the dataset.\nweeks of evaluable data for each patient ranged from two to 12\nweeks as shown in Fig. 2 .\nA. Independent Per Week Model by Machine Learning\nAlgorithms\nSince survey scores were generated per week, a naive ap-\nproach for using this data is to treat each week independently.\nT h el e f tp l o ti n Fig. 3 illustrates the idea of the independent\nmodel as an example for one subject with a number of weeks\nof evaluable data of 12 weeks. The features for each of the\nseven days were appended into a single feature vector, which\nwas then used as the input for binary classiﬁcation of each PRO\nscore. Ensemble methods like Adaboost, GBRT (gradient boost-\ning regression tree) and Random Forest (RF) are relatively ro-\nbust over unbalanced dataset and is capable of generating better\nclassiﬁcation accuracy than other types of machine learning al-\ngorithms [37]. Each of these methods was applied to the dataset\nusing ten-fold cross-validation across subjects in conjunction\nwith grid search to ﬁnd the optimal parameters for each model.\nT-tests were applied to validate the statistical signiﬁcance for\neach comparison of the result with different p values: 0.05, 0.01\nas for different levels of signiﬁcance. A sensitivity analysis was\ncompleted to investigate the model performance against missing\nvalues in feature vector by randomly withholding values from\none to six days within a week.\nB. Hidden Markov Model (HMM) With Forward Algorithm\nIn order to track changes in PRO responses over time, a model\nwas built to incorporate temporal correlations of PRO scores\nacross weeks. As shown in the right part of Figure 3 ,a nH M M\nwas used and formalized such that the state at each time point\ncorresponded to the PRO score for that week, with the features\ncollected for that week treated as observations. The transition\nmatrix was derived by counting the s state transitions from week\nto week. The original number of states for each PRO was found\nby number of unique responses, ranging from 15 to 36. In order\nto make the transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN",
        " future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2564
    },
    {
      "id": "Q8",
      "question": "How are outliers handled in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        11,
        12,
        15
      ],
      "chunks_str": [
        " future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart",
        " activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart failure self-management: A pilot study,” JMIR mHealth\nuHealth, vol. 3, no. 2, Apr. 2015, Art. no. e33.\n[12] T. M. Hale, K. Jethwani, M. S. Kandola, F. Saldana, and J. C. Kvedar, “A\nremote medication monitoring system for chronic heart failure patients to\nreduce readmissions: A two-arm randomized pilot study,” J. Med. Internet\nRes., vol. 18, no. 5, Apr. 2016, Art. no. e91.\n[13] P . A. Pilkonis, L. Y u, N. E. Dodds, K. L. Johnston, C. C. Maihoefer,\nand S. M. Lawrence, “V alidation of the depression item bank from the\npatient-reported outcomes measurement information system (PROMIS)\nin a three-month observational study,” J. Psychiatric Res. , vol. 56, no. 1,\npp. 112–119, 2014.\n[14] D. Cella et al. , “Initial adult health item banks and ﬁrst wave testing of the\npatient-reported outcomes measurement information system (PROMIS)\nnetwork: 2005-2008,” J. Clin. Epidemiol. , vol. 63, no. 11, pp. 1179–1194,\n2011.\n[15] H. Liu et al. , “Representativeness of the patient-reported outcomes mea-\nsurement information system internet panel,” J. Clin. Epidemiol. , vol. 63,\nno. 11, pp. 1169–1178, 2010.\n[16] B. L. Egleston, S. M. Miller, and N. J. Meropol, “The impact of misclas-\nsiﬁcation due to survey response fatigue on estimation and identiﬁability\nof treatment effects,” Statist. Med. , vol. 30, no. 30, pp. 3560–3572, 2011.\n[17] S. R. Porter, M. E. Whitcomb, and W. H. Weitzer, “Multiple surveys\nof students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen H",
        " information system (PROMIS) gastrointestinal\nsymptom scales,” Am. J. Gastroenterol. , vol. 109, no. 11, pp. 1804–1814,\n2014.\n[37] J. O. Ogutu, H. P . Piepho, and T. Schulz-Streeck, “A comparison of random\nforests, boosting and support vector machines for genomic selection,”\nBMC Proc. , vol. 5, no. Suppl. 3, pp. 3–7, 2011.\n[38] R. J. Petrella, J. J. Koval, and D. A. Cunningham, “A self-paced step test\nto predict aerobic ﬁtness in older adults in the primary care clinic,” J. Am.\nGeriatrics Soc. , vol. 49, pp. 632–638, 2001.\n[39] M. Kachuee, M. M. Kiani, H. Mohammadzade, and M. Shabany, “Cuff-\nless blood pressure estimation algorithms for continuous health-care mon-\nitoring,” IEEE Trans. Biomed. Eng. , vol. 64, no. 4, pp. 859–869, Apr. 2017.\n[40] F. Fallo et al. , “Circadian blood pressure patterns and life stress,” Psy-\nchotherapy Psychosomatics , vol. 71, no. 6, pp. 350–356, 2002.\n[41] A. Stolcke and S. Omohundro, “Hidden Markov model induction by\nBayesian model merging,” Neural Inf. Process. Syst. , vol. 5, no. Ml,\npp. 11–18, 1993.\n[42] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzel, “Learning to diag-\nnose with LSTM recurrent neural networks,” in Proc Int. Conf. Learn.\nRepresentations, 2015, pp. 1–18, arXiv: 1511.03677(2015).\n[43] A. Rajkomar et al. , “Scalable and accurate deep learning for electronic\nhealth records,” npj Digit. Med. , vol. 1, 2018, Art. no. 18.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. "
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2294
    },
    {
      "id": "Q9",
      "question": "Which prediction models were used in this study?",
      "answer": "HMMs and machine learning models.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        9,
        3,
        15
      ],
      "chunks_str": [
        " used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach",
        " such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices, including steps,\nheart rate, calories burned, and distance traveled. At the end\nof each week, they were asked to ﬁll out eight PROMIS short\nforms as a self-report assessment of their health status [4].\nA. Activity Data\nThe Fitbit Charge 2 (Fitbit Inc., San Francisco, CA, USA)\nis a popular commercially available activity tracker that can\nrecord a person’s daily activities and health indices like heart\nrate, steps, and sleep ( Table I ). Previous work has validated\nthe accuracy of heart rate monitoring speciﬁcally in the Fitbit\nCharge 2 [29]. The Fitbit hardware and its computational algo-\nrithms for calculating step counts and physical activity have been\nT ABLE I\nSUMMARY OF 17 T YPES OF FEA TURECOLLECTED FROM FITBIT PER DAY\n∗means that feature was eliminated for model input because it was highly sparse or redun-\ndant.\nvalidated using other Fitbit devices [30], [31]. The Fitbit Charge\n2 estimates activity using metabolic equivalents (METs), which\nare calculated based on heart rate and distance traveled [32].\nHeart rate during activity is also provided, however it has been\nshown to be inaccurate during activity [33]. Data quality was\nassured by verifying that there were no extreme outliers based\non subject-speciﬁc inter-quartile range [34]. We aggregated the\ndata for each day to compensate for noise and redundancy. Af-\nter data preprocessing, tracker distance was eliminated because\nit was identical to total distance, and logged activity distance\nand sedentary active distance were also deleted because of high\nsparsity. As a result, there were 14 features per day for each\npatient in our model.\nB. Patient-Reported Outcome Measures\nPatient-Reported Outcomes Measurement Information Sys-\ntems (PROMIS) questionnaires are a library of instruments\ndeveloped and validated to measure many domains of phys-\nical and mental health [15]. This analysis uses data from\neight PROMIS instruments: Global Physical Health and Global\nMental Health, which are two composite scores from the Global-\n10 short form [35]; Fatigue-Short Form 4a; Physical Function-\nShort Form 10a; Emotional Distress-Anxiety-Short Form 6a;\nDepression-Short Form 4a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on",
        " information system (PROMIS) gastrointestinal\nsymptom scales,” Am. J. Gastroenterol. , vol. 109, no. 11, pp. 1804–1814,\n2014.\n[37] J. O. Ogutu, H. P . Piepho, and T. Schulz-Streeck, “A comparison of random\nforests, boosting and support vector machines for genomic selection,”\nBMC Proc. , vol. 5, no. Suppl. 3, pp. 3–7, 2011.\n[38] R. J. Petrella, J. J. Koval, and D. A. Cunningham, “A self-paced step test\nto predict aerobic ﬁtness in older adults in the primary care clinic,” J. Am.\nGeriatrics Soc. , vol. 49, pp. 632–638, 2001.\n[39] M. Kachuee, M. M. Kiani, H. Mohammadzade, and M. Shabany, “Cuff-\nless blood pressure estimation algorithms for continuous health-care mon-\nitoring,” IEEE Trans. Biomed. Eng. , vol. 64, no. 4, pp. 859–869, Apr. 2017.\n[40] F. Fallo et al. , “Circadian blood pressure patterns and life stress,” Psy-\nchotherapy Psychosomatics , vol. 71, no. 6, pp. 350–356, 2002.\n[41] A. Stolcke and S. Omohundro, “Hidden Markov model induction by\nBayesian model merging,” Neural Inf. Process. Syst. , vol. 5, no. Ml,\npp. 11–18, 1993.\n[42] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzel, “Learning to diag-\nnose with LSTM recurrent neural networks,” in Proc Int. Conf. Learn.\nRepresentations, 2015, pp. 1–18, arXiv: 1511.03677(2015).\n[43] A. Rajkomar et al. , “Scalable and accurate deep learning for electronic\nhealth records,” npj Digit. Med. , vol. 1, 2018, Art. no. 18.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. "
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2280
    },
    {
      "id": "Q10",
      "question": "What considerations were given to model selection and hyperparameter tuning in this study?",
      "answer": "Random forest selected; hyperparameter tuning not detailed.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        11,
        0,
        6
      ],
      "chunks_str": [
        " future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart",
        "878 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nA Machine Learning Approach to Classifying\nSelf-Reported Health Status in a Cohort of\nPatients With Heart Disease Using\nActivity T racker Data\nYiwen Meng , William Speier , Member, IEEE, Chrisandra Shufelt, Sandy Joung, Jennifer E Van Eyk,\nC. Noel Bairey Merz, Mayra Lopez, Brennan Spiegel, and Corey W. Arnold\nAbstract— Constructing statistical models using per-\nsonal sensor data could allow for tracking health status\nover time, thereby enabling the possibility of early inter-\nvention. The goal of this study was to use machine learning\nalgorithms to classify patient-reported outcomes (PROs)\nusing activity tracker data in a cohort of patients with stable\nischemic heart disease (SIHD). A population of 182 patients\nwith SIHD were monitored over a period of 12 weeks. Each\nsubject received a Fitbit Charge 2 device to record daily\nactivity data, and each subject completed eight Patient-\nReported Outcomes Measurement Information Systems\nshort form at the end of each week as a self-assessment\nof their health status. Two models were built to classify\nPRO scores using activity tracker data. The ﬁrst model\ntreated each week independently, whereas the second\nused a hidden Markov model (HMM) to take advantage\nof correlations between successive weeks. Retrospective\nanalysis compared the classiﬁcation accuracy of the two\nmodels and the importance of each feature. In the inde-\npendent model, a random forest classiﬁer achieved a mean\narea under curve (AUC) of 0.76 for classifying the physical\nManuscript received January 9, 2019; revised March 28, 2019 and\nMay 16, 2019; accepted May 29, 2019. Date of publication June 11,\n2019; date of current version March 6, 2020. This work was supported in\npart by the California Initiative to Advance Precision Medicine (CIAPM)\n(BS, NBM, and JVE), in part by the National Heart, Lung, and Blood In-\nstitute (NIH/NHLBI R56HL135425, R01HL141773 CWA; K23HL127262,\nCS), in part by the National Center for Research Resources (NIH/NCRR\nUL1RR033176), in part by the National Center for Advancing T ransla-\ntional Sciences (NCA TS) and UCLA Clinical T ranslational Science In-\nstitute (NIH/NCA TS UL1TR000124), in part by the Advanced Clinical\nBiosystems Research Institute (JVE), in part by the Erika Glazer En-\ndowed Chair in Women’s Heart Health (NBM and JVE), and in part by\nthe Barbra Streisand Women’s Cardiovascular Research and Education\nProgram. (Corresponding author: Corey W. Arnold.)\nY . Meng, W. Speier, and C. W. Arnold are with the Computational Inte-\ngrated Diagnostics Laboratory, Department of Bioengineering, Depart-\nment of Radiology, and Department of Pathology, University of California\nLos Angeles, Los Angeles, CA 90024 USA (e-mail: , lanyexiaosa@\nucla.edu; speier@ucla.edu; cwarnold@ucla.edu).\nC. Shufelt, S. Joung, and J. E. V . Eyk, and C. N. B. Merz are\nwith Barbra Streisand Women’s Heart Center, Smidt Heart Insti-\ntute, Los Angeles, CA 90048 USA (e-mail: , Chrisandra.Shufelt@\ncshs",
        " transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENCE ALGORITHMS\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement over GBRT.\n†Signiﬁcant improvement over both GBRT and AdaBoost.\nclassiﬁed state,\nS (yt |yt−1 ,...,y 1 ,x t ,...,x 1 )= P (xt |yt )\n∗\n∑\nP( yt |yt−1 ) ∗S( yt−1 |yt−2 ,..., xt−1 ,... ) (1)\nwhere the weekly PRO score was treated as state yt , with obser-\nvation of features xt . The emission probability, P (xt |yt ), com-\nputed the probability of the observed feature vector xt given\nstate yt , computed from the random forest classiﬁer and P (yt ):\nP (xt |yt ) ∝ P (yt |xt )\nP (yt ) (2)\nAt the ﬁrst-time step, the transition probability distribution is\nundeﬁned, so the state probability was:\nS (y1 |x1 ) ∝ P (x1 |y1 ) P (y1 ) (3)\nFor analysis, states were binarized according to the criteria\ndeﬁned above. Because dichotomizing PRO score values loses\nsome information and precision, a regression analysis was con-\nducted between the median value of HMM stages and actual\nscores for the HMM. This method of predicting PRO scores\nwas compared against multinomial logistic regression to evalu-\nate the accuracy of predicting PRO scores over time.\nIV . R ESULTS\nTable II shows the mean AUC for binary classiﬁcation of\nPRO scores for the seven PROMIS measures using GBRT, Ad-\naBoost and RF. The highest mean AUC was 0.75 using RF for\nclassifying Physical Function, while the lowest was 0.47 using\nAdaBoost for Depression. The results indicated that RF signif-\nicantly outperformed other models in classiﬁcation of Anxiety\nand Depression (p < 0.05), and it was also signiﬁcantly better\nthan GBRT for Global Physical Health and Mental Health (p =\n0.01 and p = 0.01, respectively). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps,"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2570
    },
    {
      "id": "Q11",
      "question": "Is the performance of the predictive models benchmarked or compared to a baseline?",
      "answer": "Yes, compared to other methods.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        9,
        7,
        10
      ],
      "chunks_str": [
        " used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach",
        "). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly",
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2528
    },
    {
      "id": "Q12",
      "question": "Which type of explainability techniques are used?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        6,
        2,
        10
      ],
      "chunks_str": [
        " transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENCE ALGORITHMS\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement over GBRT.\n†Signiﬁcant improvement over both GBRT and AdaBoost.\nclassiﬁed state,\nS (yt |yt−1 ,...,y 1 ,x t ,...,x 1 )= P (xt |yt )\n∗\n∑\nP( yt |yt−1 ) ∗S( yt−1 |yt−2 ,..., xt−1 ,... ) (1)\nwhere the weekly PRO score was treated as state yt , with obser-\nvation of features xt . The emission probability, P (xt |yt ), com-\nputed the probability of the observed feature vector xt given\nstate yt , computed from the random forest classiﬁer and P (yt ):\nP (xt |yt ) ∝ P (yt |xt )\nP (yt ) (2)\nAt the ﬁrst-time step, the transition probability distribution is\nundeﬁned, so the state probability was:\nS (y1 |x1 ) ∝ P (x1 |y1 ) P (y1 ) (3)\nFor analysis, states were binarized according to the criteria\ndeﬁned above. Because dichotomizing PRO score values loses\nsome information and precision, a regression analysis was con-\nducted between the median value of HMM stages and actual\nscores for the HMM. This method of predicting PRO scores\nwas compared against multinomial logistic regression to evalu-\nate the accuracy of predicting PRO scores over time.\nIV . R ESULTS\nTable II shows the mean AUC for binary classiﬁcation of\nPRO scores for the seven PROMIS measures using GBRT, Ad-\naBoost and RF. The highest mean AUC was 0.75 using RF for\nclassifying Physical Function, while the lowest was 0.47 using\nAdaBoost for Depression. The results indicated that RF signif-\nicantly outperformed other models in classiﬁcation of Anxiety\nand Depression (p < 0.05), and it was also signiﬁcantly better\nthan GBRT for Global Physical Health and Mental Health (p =\n0.01 and p = 0.01, respectively). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps,",
        " experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices",
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2552
    },
    {
      "id": "Q13",
      "question": "Which evaluation metrics or outcome measures are used to assess the predictive models?",
      "answer": "AUC, ROCAUC, classification accuracy.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        4,
        10
      ],
      "chunks_str": [
        "). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly",
        "a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n880 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nFig. 1. Distribution of normal and abnormal (moderate to severe) class\nfor each PRO measure.\nFor function (i.e., Global Physical Health, Global Mental\nHealth, and Physical Function), scores less than 40 are classi-\nﬁed as moderate to severe, meaning less functional ability than\nnormal. For this study, PRO scores were predicted in two ways:\nregression was used to predict PRO scores from patient activity\ntracker data, and classiﬁcation was used to determine whether\nsubjects’ PRO scores were above the threshold for at least mod-\nerate severity. The distributions of PRO scores are shown in\nFig. 1 . Because of a lack of moderate or severe cases for social\nisolation (<2%), this variable was eliminated for analysis in our\nmodel.\nIII. M ETHODS\nMissing data is a common concern when dealing with activity\ntracker data and can result from subjects either forgetting to wear\ntheir devices or removing them for charging. Patients were asked\nto ﬁll out eight PROMIS questionnaires at the end of each week\nfor a 12-week monitoring period. In total, 19.1 percent of weeks\nhad missing PRO data and 16.6 percent of weeks had missing\nvalues from the activity tracker in four or more days. If data was\navailable for at least four days in a week, missing values were\npermuted by using the average value of the rest of the week for\nsteps or resting heart rate. Weeks with missing survey scores, as\nwell as those without step and resting heart rate data for more\nthan three days, were removed from the analysis.\nA correlation analysis between subjects’ missing Fitbit data\nand their average Global Physical Health and Global Mental\nHealth scores shows a slight negative relationship ( −0.11 and\n−0.09, respectively) that was not statistically signiﬁcant (p =\n0.13 and p = 0.23, respectively). The correlation coefﬁcient be-\ntween number of missing PROs and the average global health\nscores are −0.17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig",
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2545
    },
    {
      "id": "Q14",
      "question": "What considerations were given to selected evaluation metrics or outcome measures in this study?",
      "answer": "AUC, feature importance, sensitivity to missing data.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        4,
        8
      ],
      "chunks_str": [
        "). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly",
        "a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n880 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nFig. 1. Distribution of normal and abnormal (moderate to severe) class\nfor each PRO measure.\nFor function (i.e., Global Physical Health, Global Mental\nHealth, and Physical Function), scores less than 40 are classi-\nﬁed as moderate to severe, meaning less functional ability than\nnormal. For this study, PRO scores were predicted in two ways:\nregression was used to predict PRO scores from patient activity\ntracker data, and classiﬁcation was used to determine whether\nsubjects’ PRO scores were above the threshold for at least mod-\nerate severity. The distributions of PRO scores are shown in\nFig. 1 . Because of a lack of moderate or severe cases for social\nisolation (<2%), this variable was eliminated for analysis in our\nmodel.\nIII. M ETHODS\nMissing data is a common concern when dealing with activity\ntracker data and can result from subjects either forgetting to wear\ntheir devices or removing them for charging. Patients were asked\nto ﬁll out eight PROMIS questionnaires at the end of each week\nfor a 12-week monitoring period. In total, 19.1 percent of weeks\nhad missing PRO data and 16.6 percent of weeks had missing\nvalues from the activity tracker in four or more days. If data was\navailable for at least four days in a week, missing values were\npermuted by using the average value of the rest of the week for\nsteps or resting heart rate. Weeks with missing survey scores, as\nwell as those without step and resting heart rate data for more\nthan three days, were removed from the analysis.\nA correlation analysis between subjects’ missing Fitbit data\nand their average Global Physical Health and Global Mental\nHealth scores shows a slight negative relationship ( −0.11 and\n−0.09, respectively) that was not statistically signiﬁcant (p =\n0.13 and p = 0.23, respectively). The correlation coefﬁcient be-\ntween number of missing PROs and the average global health\nscores are −0.17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig",
        "FEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly higher than those from the independent model in\nall domains other than Fatigue and Sleep Disturbance. Depres-\nsion achieved the highest increase from 0.57 to 0.61. We also\ncompared the R 2 value of the regression analysis between HMM\nand multinomial logistic regression. The value were 0.079 and\n0.1526 from HMM in Global Physical Health and Physical\nFunction. They were signiﬁcantly better than the values achieved\nFig. 4. Plot of ROCAUC for each type of PRO after randomly withhold\nfeature values from one day to six days within a week.\nT ABLE V\nMEAN AND STA N DA R DDEVIA TION OF AUC VALUES BETWEEN THE\nINDEPENDENT WEEK MODEL AND THE HIDDEN MARKOV MODEL\nBold values are the highest AUC for a given PRO.\n∗Signiﬁcant improvement over the independent model.\nby the multinomial logit model (0.0016 and 0.0026, respec-\ntively; p < 0.001 for both). This result suggested that HMM\ncould also track the minor change of PRO score with higher\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 883\nprecision over time than baseline models like multinomial lo-\ngistic regression.\nV. D ISCUSSION\nIn general, the AUCs related to classifying physical health\nwere relatively higher than mental health PROs, such as Global\nMental Health, Anxiety and Depression. This result makes in-\ntuitive sense, as collected data, such as steps, total distance, and\ncalorie expenditure, are more directly related to physical health\nthan mental health. It might therefore be useful to develop hard-\nware to record data more related to mental health for future stud-\nies. For instance, there has been effort to develop non-invasive\nand continuous blood pressure tracking [39] using wearable de-\nvices, which may improve the performance of classifying men-\ntal health [40]. Also, only Anxiety and Depression measured by\nPROMIS instruments were used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2539
    },
    {
      "id": "Q15",
      "question": "How were robustness, confidence or statistical significance of the results assessed in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        4,
        2
      ],
      "chunks_str": [
        "iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3",
        "a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n880 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nFig. 1. Distribution of normal and abnormal (moderate to severe) class\nfor each PRO measure.\nFor function (i.e., Global Physical Health, Global Mental\nHealth, and Physical Function), scores less than 40 are classi-\nﬁed as moderate to severe, meaning less functional ability than\nnormal. For this study, PRO scores were predicted in two ways:\nregression was used to predict PRO scores from patient activity\ntracker data, and classiﬁcation was used to determine whether\nsubjects’ PRO scores were above the threshold for at least mod-\nerate severity. The distributions of PRO scores are shown in\nFig. 1 . Because of a lack of moderate or severe cases for social\nisolation (<2%), this variable was eliminated for analysis in our\nmodel.\nIII. M ETHODS\nMissing data is a common concern when dealing with activity\ntracker data and can result from subjects either forgetting to wear\ntheir devices or removing them for charging. Patients were asked\nto ﬁll out eight PROMIS questionnaires at the end of each week\nfor a 12-week monitoring period. In total, 19.1 percent of weeks\nhad missing PRO data and 16.6 percent of weeks had missing\nvalues from the activity tracker in four or more days. If data was\navailable for at least four days in a week, missing values were\npermuted by using the average value of the rest of the week for\nsteps or resting heart rate. Weeks with missing survey scores, as\nwell as those without step and resting heart rate data for more\nthan three days, were removed from the analysis.\nA correlation analysis between subjects’ missing Fitbit data\nand their average Global Physical Health and Global Mental\nHealth scores shows a slight negative relationship ( −0.11 and\n−0.09, respectively) that was not statistically signiﬁcant (p =\n0.13 and p = 0.23, respectively). The correlation coefﬁcient be-\ntween number of missing PROs and the average global health\nscores are −0.17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig",
        " experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2560
    },
    {
      "id": "Q16",
      "question": "What limitations of the study were discussed?",
      "answer": "Unknown from this paper",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        14,
        12,
        13
      ],
      "chunks_str": [
        "L. Morey, “Physical activity intervention for women,” Am. J. Preventive\nMed., vol. 49, no. 3, pp. 414–418, 2015.\n[28] J. B. Wang et al. , “Wearable sensor/device (ﬁtbit one) and SMS text-\nmessaging prompts to increase physical activity in overweight and obese\nadults: A randomized controlled trial,” T elemedicine e-Health, vol. 21, no.\n10, pp. 18–23, 2014.\n[29] S. Benedetto, C. Caldato, E. Bazzan, D. C. Greenwood, V . Pensabene,\nand P . Actis, “Assessment of the ﬁtbit charge 2 for monitoring heart rate,”\nPLoS One , vol. 13, no. 2, 2018, Art. no. e0192691.\n[30] M. A. Tully, C. McBride, L. Heron, and R. F. Hunter, “The validation of\nFibit ZipTM physical activity monitor as a measure of free-living physical\nactivity,” BMC Res Notes , vol. 7, 2014, Art. no. 952.\n[31] K. M. Diaz et al. , “Fitbit: An accurate and reliable device for wireless\nphysical activity tracking,” Int. J. Cardiol. , vol. 185, pp. 138–140, 2015.\n[32] R. K. Reddy et al. , “Accuracy of wrist-worn activity monitors during com-\nmon daily physical activities and types of structured exercise: Evaluation\nstudy,” JMIR Mhealth Uhealth , vol. 6, 2018, Art. no. e10338.\n[33] E. Jo, K. Lewis, D. Directo, M. J. Kim, and B. A. Dolezal, “V alidation of\nbiofeedback wearables for photoplethysmographic heart rate tracking,” J.\nSports Sci. Med. , vol. 15, pp. 540–547, 2016.\n[34] A. Ghasemi and S. Zahediasl, “Normality tests for statistical analysis:\nA guide for non-statisticians,” Int. J. Endocrinol. Metab. , vol. 10, no. 2,\npp. 486–489, 2012.\n[35] R. D. Hays, J. B. Bjorner, D. A. Revicki, K. L. Spritzer, and D. Cella,\n“Development of physical and mental health summary scores from the\npatient-reported outcomes measurement information system (PROMIS)\nglobal items,” Qual. Life Res. , vol. 18, no. 7, pp. 873–880, 2009.\n[36] B. M. R. Spiegel et al. , “Development of the NIH patient-reported\noutcomes measurement information system (PROMIS) gastrointestinal\nsymptom scales,” Am. J. Gastroenterol. , vol. 109, no. 11, pp. 1804–1814,\n2014.\n[37] J. O. Ogutu, H. P . Piepho, and T. Schulz-Streeck, “A comparison of random\nforests, boosting and support vector machines for genomic selection,”\nBMC Proc. , vol. 5, no. Suppl. 3, pp. 3–7, 2011.\n[38] R. J. Petrella, J. J. Koval, and D. A. Cunningham, “A self-paced step test\nto predict aerobic ﬁtness in older adults in the primary",
        " activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart failure self-management: A pilot study,” JMIR mHealth\nuHealth, vol. 3, no. 2, Apr. 2015, Art. no. e33.\n[12] T. M. Hale, K. Jethwani, M. S. Kandola, F. Saldana, and J. C. Kvedar, “A\nremote medication monitoring system for chronic heart failure patients to\nreduce readmissions: A two-arm randomized pilot study,” J. Med. Internet\nRes., vol. 18, no. 5, Apr. 2016, Art. no. e91.\n[13] P . A. Pilkonis, L. Y u, N. E. Dodds, K. L. Johnston, C. C. Maihoefer,\nand S. M. Lawrence, “V alidation of the depression item bank from the\npatient-reported outcomes measurement information system (PROMIS)\nin a three-month observational study,” J. Psychiatric Res. , vol. 56, no. 1,\npp. 112–119, 2014.\n[14] D. Cella et al. , “Initial adult health item banks and ﬁrst wave testing of the\npatient-reported outcomes measurement information system (PROMIS)\nnetwork: 2005-2008,” J. Clin. Epidemiol. , vol. 63, no. 11, pp. 1179–1194,\n2011.\n[15] H. Liu et al. , “Representativeness of the patient-reported outcomes mea-\nsurement information system internet panel,” J. Clin. Epidemiol. , vol. 63,\nno. 11, pp. 1169–1178, 2010.\n[16] B. L. Egleston, S. M. Miller, and N. J. Meropol, “The impact of misclas-\nsiﬁcation due to survey response fatigue on estimation and identiﬁability\nof treatment effects,” Statist. Med. , vol. 30, no. 30, pp. 3560–3572, 2011.\n[17] S. R. Porter, M. E. Whitcomb, and W. H. Weitzer, “Multiple surveys\nof students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen H",
        " students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen Hsich et al. , “Identifying important risk factors for survival\nin systolic heart failure patients using random survival forests,” Circ.\nCardiovascular Qual. Outcomes , vol. 4, no. 1, pp. 39–45, 2014.\n[21] N. Limsopatham, C. Macdonald, and I. Ounis, “Learning to combine\nrepresentations for medical records search,” in Proc. 36th Int. ACM SIGIR\nConf. Res. Develop. Inf. Retrieval , 2013, pp. 833–836.\n[22] J. H. Morra, Zhuowen Tu, L. G. Apostolova, A. E. Green, A. W. Toga, and\nP . M. Thompson, “Comparison of AdaBoost and support vector machines\nfor detecting alzheimer’s disease through automated hippocampal seg-\nmentation,” IEEE Trans. Med. Imag. , vol. 29, no. 1, pp. 30–43, Jan. 2010.\n[23] H. Ishwaran, U. B. Kogalur, E. H. Blackstone, and M. S. Lauer, “Random\nsurvival forests,” Ann. Appl. Statist. , vol. 2, no. 3, pp. 841–860, 2008.\n[24] S. Shen et al. , “A Bayesian model for estimating multi-state disease pro-\ngression,” Comput. Biol. Med. , vol. 81, pp. 111–120, 2017.\n[25] B. Schuster-B ¨ockler and A. Bateman, “An introduction to hidden Markov\nmodels,” Current Protocols Bioinf. , vol. 18, pp. A.3A.1–A.3A.9, 2007.\n[26] E. Birney Clamp and R. M. Durbin, “Genewise and genomewise,” Genome\nRes., vol. 14, no. 4, pp. 988–995, 2004.\n[27] L. A. Cadmus-bertram, B. H. Marcus, R. E. Patterson, B. A. Parker, and B.\nL. Morey, “Physical activity intervention for women,” Am. J. Preventive\nMed., vol. 49, no. 3, pp. 414–418, 2015.\n[28] J. B. Wang et al. , “Wearable sensor/device (ﬁtbit one) and SMS text-\nmessaging prompts to increase physical activity in overweight and obese\nadults: A randomized controlled trial,” T elemedicine e-Health, vol. 21, no.\n10, pp. 18–23, 2014.\n[29] S. Benedetto, C. Caldato, E. Bazzan, D. C. Greenwood, V . Pensabene,\nand P . Actis, “Assessment of the ﬁtbit"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE"
    }
  ]
}
{
  "paper": "1-s2.0-S2666693623000737-main.pdf",
  "answers": [
    {
      "id": "Q1",
      "question": "Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?",
      "answer": "diagnostic",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        0,
        3
      ],
      "chunks_str": [
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        "Data-efﬁcient machine learning methods in the\nME-TIME study: Rationale and design of a longitudinal\nstudy to detect atrialﬁbrillation and heart failure from\nwearables\nArman Naseri, MSc,*† David Tax, PhD,† Pim van der Harst, MD, PhD,‡\nMarcel Reinders, PhD,† Ivo van der Bilt, MD, PhD*‡\nFrom the*Department of Cardiology, Haga Teaching Hospital, The Hague, The Netherlands,†Pattern\nRecognition and Bioinformatics, Delft University of Technology, Delft, The Netherlands, and\n‡Department of Cardiology, University Medical Center Utrecht, Utrecht, The Netherlands.\nBACKGROUND Smartwatches enable continuous and noninvasive\ntime series monitoring of cardiovascular biomarkers like heart\nrate (from photoplethysmograms), step counter, skin temperature,\net cetera; as such, they have promise in assisting in early detection\nand prevention of cardiovascular disease. Although these bio-\nmarkers may not be directly useful to physicians, a machine learning\n(ML) model could ﬁnd clinically relevant patterns. Unfortunately,\nML models typically need supervised (ie, annotated) data, and la-\nbeling of large amounts of continuous data is very labor intensive.\nTherefore, ML methods that are data efﬁcient, ie, needing a low\nnumber of labels, are required to detect potential clinical value in\npatterns found in wearable data.\nOBJECTIVE The primary study objective of the ME-TIME (Machine\nLearning Enabled Time Series Analysis in Medicine) study is to\ndesign an ML model that can detect atrialﬁbrillation (AF) and heart\nfailure (HF) from wearable data in a data-ef ﬁcient manner. To\nachieve this, self-supervised and weakly supervised learning tech-\nniques are used.\nMETHODS Two hundred subjects (100 reference, 50 AF, and 50 HF)\nare being invited to participate in wearing a Fitbitﬁtness tracker for\n3 months. Interested volunteers are sent a questionnaire to deter-\nmine their health, in particular cardiovascular health. Volunteers\nwithout any (history of) serious illness are assigned to the reference\ngroup. Participants with AF and HF are recruited in the Haga teach-\ning hospital in The Hague, The Netherlands.\nRESULTS Enrollment commenced on May 1, 2022, and as of the\ntime of this report, 62 subjects have been included in the study. Pre-\nliminary analysis of the data reveals signiﬁcant inter-subject vari-\nability. Notably, we identi ﬁed heart rate recovery curves and\ntime-delayed correlations between heart rate and step count as po-\ntential strong indicators for heart disease.\nCONCLUSION Using self-supervised and multiple-instance\nlearning techniques, we hypothesize that patterns speciﬁct oA F\nand HF can be found in continuous data obtained from smart-\nwatches.\nKEYWORDS Wearables; mHealth; Atrial ﬁbrillation; Heart failure;\nSmartwatch; Arti ﬁcial intelligence; Machine learning; Multiple-\ninstance learning; Self-supervised learning\n(Cardiovascular Digital Health Journal 2023;4:165–172)\n© 2023\nHeart Rhythm Society. This is an open access article under the CC\nBY license (http://creativecommons.org/licenses/by/4.0/).\nIntroduction\nCardiovascular disease is one of the leading causes of mortal-\nity globally1 and cardiovascular healthcare accounts for a\nlarge portion of global healthcare costs and expenses. Early\ndetection and prevention will decrease the burden of cardio-\nvascular disease and will therefore decrease mortality,\nmorbidity, and costs. Cardiovascular monitoring using big\ndata from wearables and machine learning can drastically in-\ncrease the availability and ef ﬁciency of cardiovascular\nhealthcare globally, at the fraction of the costs of conven-\ntional medical-grade devices.\nElectrocardi",
        "\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2554
    },
    {
      "id": "Q2",
      "question": "On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        4,
        7,
        5
      ],
      "chunks_str": [
        " of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains",
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2554
    },
    {
      "id": "Q3",
      "question": "How were participants sampled in this study: by convenience, randomly, or consecutively?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        4,
        5,
        7
      ],
      "chunks_str": [
        " of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2549
    },
    {
      "id": "Q4",
      "question": "How was the dataset described in this study before predictive modeling was performed?",
      "answer": "Data from 22 subjects with large inter-subject variability.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        5,
        6
      ],
      "chunks_str": [
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        " and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2550
    },
    {
      "id": "Q5",
      "question": "What was the proedure for splitting the dataset into training, validation, and test sets in this study?",
      "answer": "20% external test set, internal test from final 20% data.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        6,
        7
      ],
      "chunks_str": [
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        " and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation",
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2561
    },
    {
      "id": "Q6",
      "question": "What preprocessing techniques on the included variables/features were applied in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        5,
        9
      ],
      "chunks_str": [
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        "-correlation function\nbetween the heart rate window and its corresponding step\ncounter window is indicative of heart disease. To calculate\nthe correlation, we consider varying window sizes and time\ndifferences (lags) between heart rate and steps. The computed\ncross-correlation matrix for the healthy group, along with the\nAF and HF patient groups, as shown inFigure 7, shows that\nthe heart rate is correlated with the step counter with 1-minute\ndelay.\nDiscussion\nBy building a suitable infrastructure with Cloud technology,\nbig data acquired in the study is used to develop data-efﬁcient\nmodels using methods from multiple instance and self-\nsupervised learning.\nWe aim to examine the inﬂuence of inter-subject vari-\nability on predicting cardiovascular disease and will explore\npotential methods to mitigate these variabilities.\n13,14 We\nexpect that patterns indicative of cardiovascular disease\nbecome apparent within a timeframe of minutes, hours, or\nmore, considering that consumer-grade wearables have a\nslower sampling rate compared to the gold standard. We\nhave shown 1 example of such a pattern: the acceleration-\ndeceleration curve. Preselecting windows based on such\nTable 2 Confusion matrix of per-week healthy vs atrialﬁbrillation\nclassiﬁcation of the MILES model with peak aligned curves\nconcatenated with step counter data, with true and predicted labels\nshown vertically and horizontally, respectively\nTrue/Predicted label AF Healthy\nAF 11 14\nHealthy 7 33\nAF 5 atrial ﬁbrillation.\nFigure 7 Cross-correlation matrices between windowed heart rate data and\nnumber of steps for healthy and the persistent atrialﬁbrillation (AF) group.\nRows are window sizes and columns lag between heart rate and step counter.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 171\n\npatterns furthermore mitigates searching through substantial\namounts of data that may not provide much information\nabout cardiovascular disease. Inspecting the cross-\ncorrelation for several combinations of window size and\nlag show a different proﬁle for healthy and AF group individ-\nuals, showing that it is meaningful to analyze step counter\nand heart rate together. Big data for heart disease detection\nrequires substantial labeling efforts from physicians.\nUsing self-supervised learning and MIL, a model can be\ntrained with much fewer labels. Ourﬁndings demonstrate\nthis by employing MILES to achieve high speciﬁcity, which\ncan aid in ruling out heart disease in individuals experiencing\nsymptoms similar to heart disease but without the condition\n(ie, false-positives).\nConclusion\nThe ongoing ME-TIME study is a longitudinal observational\nstudy that uses machine learning with time series data from\nconsumer-grade smartwatches to detect atrial ﬁbrillation\nand heart failure. This will contribute to cost-effective cardio-\nvascular monitoring of outpatients, thereby reducing exacer-\nbation of cardiovascular disease and effectively increasing\ncapacity of global cardiovascular healthcare.\nFunding Sources\nThis research did not receive any speciﬁc grant from funding\nagencies in the public, commercial, or not-for-proﬁt sectors.\nDisclosures\nThe authors declare no conﬂict of interest.\nAuthorship\nAll authors attest they meet the current ICMJE criteria for\nauthorship.\nPatient Consent\nInformed consent was obtained from all subjects involved in\nthe study.\nInstitutional Review Board Statement\nThe study was conducted in accordance with the Declaration\nof Helsinki, and approved by the Institutional Review Board\n(or Ethics Committee) of METC-LDD (protocol code\nNL73708.058.20) for studies involving humans.\nReferences\n1. Roth GA, Mensah GA, Johnson CO, et al. Global burden of cardiovascular dis-\neases and risk factors, 1990–2019: update from the GBD 2019 study. J"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2538
    },
    {
      "id": "Q7",
      "question": "How is missing data handled in this study?",
      "answer": "Linear interpolation for short gaps, exclusion for longer gaps.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        4,
        5,
        7
      ],
      "chunks_str": [
        " of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2547
    },
    {
      "id": "Q8",
      "question": "How are outliers handled in this study?",
      "answer": "Artifacts removed, linear interpolation applied.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        4,
        6,
        7
      ],
      "chunks_str": [
        " of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains",
        " and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation",
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2544
    },
    {
      "id": "Q9",
      "question": "Which prediction models were used in this study?",
      "answer": "Multiple-instance learning models.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        0,
        6
      ],
      "chunks_str": [
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        "Data-efﬁcient machine learning methods in the\nME-TIME study: Rationale and design of a longitudinal\nstudy to detect atrialﬁbrillation and heart failure from\nwearables\nArman Naseri, MSc,*† David Tax, PhD,† Pim van der Harst, MD, PhD,‡\nMarcel Reinders, PhD,† Ivo van der Bilt, MD, PhD*‡\nFrom the*Department of Cardiology, Haga Teaching Hospital, The Hague, The Netherlands,†Pattern\nRecognition and Bioinformatics, Delft University of Technology, Delft, The Netherlands, and\n‡Department of Cardiology, University Medical Center Utrecht, Utrecht, The Netherlands.\nBACKGROUND Smartwatches enable continuous and noninvasive\ntime series monitoring of cardiovascular biomarkers like heart\nrate (from photoplethysmograms), step counter, skin temperature,\net cetera; as such, they have promise in assisting in early detection\nand prevention of cardiovascular disease. Although these bio-\nmarkers may not be directly useful to physicians, a machine learning\n(ML) model could ﬁnd clinically relevant patterns. Unfortunately,\nML models typically need supervised (ie, annotated) data, and la-\nbeling of large amounts of continuous data is very labor intensive.\nTherefore, ML methods that are data efﬁcient, ie, needing a low\nnumber of labels, are required to detect potential clinical value in\npatterns found in wearable data.\nOBJECTIVE The primary study objective of the ME-TIME (Machine\nLearning Enabled Time Series Analysis in Medicine) study is to\ndesign an ML model that can detect atrialﬁbrillation (AF) and heart\nfailure (HF) from wearable data in a data-ef ﬁcient manner. To\nachieve this, self-supervised and weakly supervised learning tech-\nniques are used.\nMETHODS Two hundred subjects (100 reference, 50 AF, and 50 HF)\nare being invited to participate in wearing a Fitbitﬁtness tracker for\n3 months. Interested volunteers are sent a questionnaire to deter-\nmine their health, in particular cardiovascular health. Volunteers\nwithout any (history of) serious illness are assigned to the reference\ngroup. Participants with AF and HF are recruited in the Haga teach-\ning hospital in The Hague, The Netherlands.\nRESULTS Enrollment commenced on May 1, 2022, and as of the\ntime of this report, 62 subjects have been included in the study. Pre-\nliminary analysis of the data reveals signiﬁcant inter-subject vari-\nability. Notably, we identi ﬁed heart rate recovery curves and\ntime-delayed correlations between heart rate and step count as po-\ntential strong indicators for heart disease.\nCONCLUSION Using self-supervised and multiple-instance\nlearning techniques, we hypothesize that patterns speciﬁct oA F\nand HF can be found in continuous data obtained from smart-\nwatches.\nKEYWORDS Wearables; mHealth; Atrial ﬁbrillation; Heart failure;\nSmartwatch; Arti ﬁcial intelligence; Machine learning; Multiple-\ninstance learning; Self-supervised learning\n(Cardiovascular Digital Health Journal 2023;4:165–172)\n© 2023\nHeart Rhythm Society. This is an open access article under the CC\nBY license (http://creativecommons.org/licenses/by/4.0/).\nIntroduction\nCardiovascular disease is one of the leading causes of mortal-\nity globally1 and cardiovascular healthcare accounts for a\nlarge portion of global healthcare costs and expenses. Early\ndetection and prevention will decrease the burden of cardio-\nvascular disease and will therefore decrease mortality,\nmorbidity, and costs. Cardiovascular monitoring using big\ndata from wearables and machine learning can drastically in-\ncrease the availability and ef ﬁciency of cardiovascular\nhealthcare globally, at the fraction of the costs of conven-\ntional medical-grade devices.\nElectrocardi",
        " and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2535
    },
    {
      "id": "Q10",
      "question": "What considerations were given to model selection and hyperparameter tuning in this study?",
      "answer": "Hyperparameter tuning for threshold determination.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        5,
        8
      ],
      "chunks_str": [
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        " a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\ndisease (n515), respectively. The mean and standard deviation are shown per time point (5 second intervals) for both groups.\n170 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\ntesting, a subject can signiﬁcantly deviate from the subjects\non which the model was trained.\nHeart rate peak alignment in acceleration-\ndeceleration curves indicate difference between 7\nAF patients and 15 healthy controls\nNext, we explored the heart rate recovery curves after activity\n(acceleration-deceleration curves).12 First a peak is detected,\nwhereafter the start (onset) and end (recovery) points associ-\nated to that peak are determined by the minimum heart rate\nvalue 5 minutes before and 15 minutes after the peak. The\ncurves are preprocessed by aligning the peaks on the time\naxis. Additionally, for every subject, the amplitude of the\ncurves is rescaled by the average peak value across all curves\nfor that individual.Figure 6shows the curves for light activ-\nity, deﬁned by a maximum of 20 steps in the 5 minutes pre-\nceding the peak and fewer than 10 steps in the 15 minutes\nafter the peak. There are 2 noticeable differences in heart\nrate patterns between persistent AF patients (in red) and\nhealthy participants (in blue). The standard deviation for\nAF patients is considerably smaller than that of healthy indi-\nviduals, and their heart rate recovery is slower, as observed at\nthe 6-minute mark. These distinctions could potentially serve\nas clinical indicators for atrialﬁbrillation.\nMIL can detect healthy cardiovascular outcomes\nThe peak aligned acceleration-deceleration curves are\nconcatenated with their corresponding step counter data\nand grouped per week to form bags. The MILES (Multiple-\nInstance Learning via Embedded Instance Selection) model\nis then used to classify every week as healthy or AF. The re-\nsults inTable 2show that even though the sensitivity is low,\nthe speciﬁcity is decent. This shows potential in avoiding un-\nnecessary visits to a cardiologist for patients who have symp-\ntoms that are wrongly suspected to be related to heart\nproblems.\nStep counter and heart rate are correlated with a\ntime delay\nNext, we examined whether the cross-correlation function\nbetween the heart rate window and its corresponding step\ncounter window is indicative of heart disease. To calculate\nthe correlation, we consider varying window sizes and time\ndifferences (lags) between heart rate and steps. The computed\ncross-correlation matrix for the healthy group, along with the\nAF and HF patient groups, as shown inFigure 7, shows that\nthe heart rate is correlated with the step counter with 1-minute\ndelay.\nDiscussion\nBy building a suitable infrastructure with Cloud technology,\nbig data acquired in the study is used to develop data-efﬁcient\nmodels using methods from multiple instance and self-\nsupervised learning.\nWe aim to examine the inﬂuence of inter-subject vari-\nability on predicting cardiovascular disease and will explore\n"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2540
    },
    {
      "id": "Q11",
      "question": "How was data augmentation or generation used in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        3,
        7,
        5
      ],
      "chunks_str": [
        "\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are",
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2549
    },
    {
      "id": "Q12",
      "question": "Is the performance of the predictive models benchmarked or compared to a baseline?",
      "answer": "Yes, compared to baseline.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        6,
        5,
        7
      ],
      "chunks_str": [
        " and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2545
    },
    {
      "id": "Q13",
      "question": "Which type of explainability techniques are used?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        4,
        8,
        2
      ],
      "chunks_str": [
        " of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains",
        " a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\ndisease (n515), respectively. The mean and standard deviation are shown per time point (5 second intervals) for both groups.\n170 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\ntesting, a subject can signiﬁcantly deviate from the subjects\non which the model was trained.\nHeart rate peak alignment in acceleration-\ndeceleration curves indicate difference between 7\nAF patients and 15 healthy controls\nNext, we explored the heart rate recovery curves after activity\n(acceleration-deceleration curves).12 First a peak is detected,\nwhereafter the start (onset) and end (recovery) points associ-\nated to that peak are determined by the minimum heart rate\nvalue 5 minutes before and 15 minutes after the peak. The\ncurves are preprocessed by aligning the peaks on the time\naxis. Additionally, for every subject, the amplitude of the\ncurves is rescaled by the average peak value across all curves\nfor that individual.Figure 6shows the curves for light activ-\nity, deﬁned by a maximum of 20 steps in the 5 minutes pre-\nceding the peak and fewer than 10 steps in the 15 minutes\nafter the peak. There are 2 noticeable differences in heart\nrate patterns between persistent AF patients (in red) and\nhealthy participants (in blue). The standard deviation for\nAF patients is considerably smaller than that of healthy indi-\nviduals, and their heart rate recovery is slower, as observed at\nthe 6-minute mark. These distinctions could potentially serve\nas clinical indicators for atrialﬁbrillation.\nMIL can detect healthy cardiovascular outcomes\nThe peak aligned acceleration-deceleration curves are\nconcatenated with their corresponding step counter data\nand grouped per week to form bags. The MILES (Multiple-\nInstance Learning via Embedded Instance Selection) model\nis then used to classify every week as healthy or AF. The re-\nsults inTable 2show that even though the sensitivity is low,\nthe speciﬁcity is decent. This shows potential in avoiding un-\nnecessary visits to a cardiologist for patients who have symp-\ntoms that are wrongly suspected to be related to heart\nproblems.\nStep counter and heart rate are correlated with a\ntime delay\nNext, we examined whether the cross-correlation function\nbetween the heart rate window and its corresponding step\ncounter window is indicative of heart disease. To calculate\nthe correlation, we consider varying window sizes and time\ndifferences (lags) between heart rate and steps. The computed\ncross-correlation matrix for the healthy group, along with the\nAF and HF patient groups, as shown inFigure 7, shows that\nthe heart rate is correlated with the step counter with 1-minute\ndelay.\nDiscussion\nBy building a suitable infrastructure with Cloud technology,\nbig data acquired in the study is used to develop data-efﬁcient\nmodels using methods from multiple instance and self-\nsupervised learning.\nWe aim to examine the inﬂuence of inter-subject vari-\nability on predicting cardiovascular disease and will explore\n",
        " learning is used, where each observation of the data has\na class label. This must be done with ECGs, since photople-\nthysmography or derived signals are difﬁcult to interpret by a\nclinician. This so-called labeling or annotating of signals by\nphysicians is infeasible for the large amounts of (continuous)\ndata required, and therefore semi-automated\n4,5 and fully\nautomated2,3 ECG labeling systems6 have been developed.\nHowever, these still require a lot of manual labor from contin-\nuously monitored users.\nTherefore, the objective of the ME-TIME is (early) detec-\ntion and prevention of heart disease by leveraging time series\ndata from smartwatches, a cloud-based infrastructure, and\nmachine learning algorithms speciﬁcally designed to func-\ntion effectively with minimal labeling efforts.\nMethods\nStudy design and data collection\nME-TIME (registered at ClinicalTrials.gov; ID:\nNCT05802563) is designed as an observational cohort study\nconsisting of 3 data subject groups, as depicted inFigure 1.\nThe ﬁrst group consists of patients with systolic heart failure\n(HF group); the second group consists of patients with docu-\nmented atrial ﬁbrillation (AF group); and the third group,\nserving as a reference, consists of healthy volunteers. The\nrationale for creating distinct AF and HF groups comes\nfrom their unique pathophysiological characteristics. Conse-\nquently, heart rate patterns that are indicative of these dis-\neases might also be different. The HF group consists of 50\nstudy participants with systolic heart failure, deﬁned as a\nleft ventricular ejection fraction,35% without documented\natrial ﬁbrillation. The AF group consists of 50 patients with\ndocumented atrialﬁbrillation (paroxysmal, persistent, or per-\nmanent) without systolic heart failure. Ejection fractions will\nbe assessed from echocardiograms that are made within 1\nyear of inclusion, and if this is not available an echocardio-\ngram will be performed. The reference group consists of\n100 participants without any prior medical history and\nwithout medication use. Potential study subjects that meet\n2 31\nRef AF HF\n4 5\nFigure 1 Data analysis pipeline for the ME-TIME study. Included participants (image 1) are given a smartwatch (image 2), which is connected to our data\nacquisition and storage platform (image 3). The resulting data are then preprocessed (image 4) and put into the data-efﬁcient machine learning model (image\n5). AF5 atrial ﬁbrillation group; HF5 heart failure group; Ref5 reference group.\nAB C\nFigure 2 Pipeline for the study’s proposed approach.a: Segmentation of the time series of each subject (1 healthy and 2 atrialﬁbrillation) using a sliding win-\ndow. Only the label of the entire subject is available, instead of each individual window.b: The windows are inputs to an autoencoder and are compressed to a\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\n"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2523
    },
    {
      "id": "Q14",
      "question": "Which evaluation metrics or outcome measures are used to assess the predictive models?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        3,
        6
      ],
      "chunks_str": [
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if",
        "\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are",
        " and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2554
    },
    {
      "id": "Q15",
      "question": "What considerations were given to selected evaluation metrics or outcome measures in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        3,
        5
      ],
      "chunks_str": [
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd",
        "\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2553
    },
    {
      "id": "Q16",
      "question": "How were robustness, confidence or statistical significance of the results assessed in this study?",
      "answer": "Leave-p-subjects-out cross-validation with stratification.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        3,
        5
      ],
      "chunks_str": [
        " commercial, or not-for-proﬁt sectors.\nDisclosures\nThe authors declare no conﬂict of interest.\nAuthorship\nAll authors attest they meet the current ICMJE criteria for\nauthorship.\nPatient Consent\nInformed consent was obtained from all subjects involved in\nthe study.\nInstitutional Review Board Statement\nThe study was conducted in accordance with the Declaration\nof Helsinki, and approved by the Institutional Review Board\n(or Ethics Committee) of METC-LDD (protocol code\nNL73708.058.20) for studies involving humans.\nReferences\n1. Roth GA, Mensah GA, Johnson CO, et al. Global burden of cardiovascular dis-\neases and risk factors, 1990–2019: update from the GBD 2019 study. J Am\nColl Cardiol 2020;76:2982–3021.\n2. Tison GH, Sanchez JM, Ballinger B, et al. Passive detection of atrialﬁbrillation\nusing a commercially available smartwatch. JAMA Cardiol 2018;3:409–416.\n3. Wasserlauf J, You C, Patel R, Valys A, Albert D, Passman R. Smartwatch perfor-\nmance for the detection and quantiﬁcation of atrialﬁbrillation. Circ Arrhythm\nElectrophysiol 2019;12:e006834.\n4. Zhu L, Nathan V, Kuang J, et al. Atrialﬁbrillation detection and atrialﬁbrillation\nburden estimation via wearables. IEEE J Biomed Health Inform 2021;\n26:2063–2074.\n5. Lubitz SA, Faranesh AZ, Selvaggi C, et al. Detection of atrialﬁbrillation in a large\npopulation using wearable devices: the Fitbit heart study. Circulation 2022;\n146:1415–1424.\n6. Hall A, Mitchell ARJ, Wood L, Holland C. Effectiveness of a single lead Alive-\nCor electrocardiogram application for the screening of atrialﬁbrillation: a system-\natic review. Medicine (Baltimore) 2020;99:e21388.\n7. Torres-Soto J, Ashley EA. Multi-task deep learning for cardiac rhythm detection\nin wearable devices. NPJ Digit Med 2020;3:116.\n8. Mienye ID, Sun Y, Wang Z. Improved sparse autoencoder based artiﬁcial neural\nnetwork approach for prediction of heart disease. Informatics in Medicine Un-\nlocked 2020;18:100307.\n9. Spathis D, Perez-Pozuelo I, Brage S, Wareham NJ, Mascolo C. Self-supervised\ntransfer learning of physiological representations from free-living wearable\ndata. In: Proceedings of the Conference on Health. Inference, and Learning;\n2021. p. 69–78.\n10. Quer G, Gouda P, Galarnyk M, Topol EJ, Steinhubl SR. Inter-and intraindividual\nvariability in daily resting heart rate and its associations with age, sex, sleep, BMI,\nand time of year: retrospective, longitudinal cohort study of 92,457 adults. PLoS\nOne 2020;15:e0227709.\n11. McInnes L, Healy J, Melville J. Umap: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint arXiv:1802.03426 2018.\n12. Coote JH. Recovery of heart rate following intense dynamic exercise. Exp Physiol\n2010;95:431–440.\n13. Gyawali PK, Horacek BM, Sapp JL, Wang L. Sequential factorized autoencoder\nfor localizing the origin of ventricular activation from 12-lead electrocardiograms.\nIEEE Trans Biomed Eng",
        "\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are",
        " single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2538
    },
    {
      "id": "Q17",
      "question": "What limitations of the study were discussed?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        1,
        7
      ],
      "chunks_str": [
        " commercial, or not-for-proﬁt sectors.\nDisclosures\nThe authors declare no conﬂict of interest.\nAuthorship\nAll authors attest they meet the current ICMJE criteria for\nauthorship.\nPatient Consent\nInformed consent was obtained from all subjects involved in\nthe study.\nInstitutional Review Board Statement\nThe study was conducted in accordance with the Declaration\nof Helsinki, and approved by the Institutional Review Board\n(or Ethics Committee) of METC-LDD (protocol code\nNL73708.058.20) for studies involving humans.\nReferences\n1. Roth GA, Mensah GA, Johnson CO, et al. Global burden of cardiovascular dis-\neases and risk factors, 1990–2019: update from the GBD 2019 study. J Am\nColl Cardiol 2020;76:2982–3021.\n2. Tison GH, Sanchez JM, Ballinger B, et al. Passive detection of atrialﬁbrillation\nusing a commercially available smartwatch. JAMA Cardiol 2018;3:409–416.\n3. Wasserlauf J, You C, Patel R, Valys A, Albert D, Passman R. Smartwatch perfor-\nmance for the detection and quantiﬁcation of atrialﬁbrillation. Circ Arrhythm\nElectrophysiol 2019;12:e006834.\n4. Zhu L, Nathan V, Kuang J, et al. Atrialﬁbrillation detection and atrialﬁbrillation\nburden estimation via wearables. IEEE J Biomed Health Inform 2021;\n26:2063–2074.\n5. Lubitz SA, Faranesh AZ, Selvaggi C, et al. Detection of atrialﬁbrillation in a large\npopulation using wearable devices: the Fitbit heart study. Circulation 2022;\n146:1415–1424.\n6. Hall A, Mitchell ARJ, Wood L, Holland C. Effectiveness of a single lead Alive-\nCor electrocardiogram application for the screening of atrialﬁbrillation: a system-\natic review. Medicine (Baltimore) 2020;99:e21388.\n7. Torres-Soto J, Ashley EA. Multi-task deep learning for cardiac rhythm detection\nin wearable devices. NPJ Digit Med 2020;3:116.\n8. Mienye ID, Sun Y, Wang Z. Improved sparse autoencoder based artiﬁcial neural\nnetwork approach for prediction of heart disease. Informatics in Medicine Un-\nlocked 2020;18:100307.\n9. Spathis D, Perez-Pozuelo I, Brage S, Wareham NJ, Mascolo C. Self-supervised\ntransfer learning of physiological representations from free-living wearable\ndata. In: Proceedings of the Conference on Health. Inference, and Learning;\n2021. p. 69–78.\n10. Quer G, Gouda P, Galarnyk M, Topol EJ, Steinhubl SR. Inter-and intraindividual\nvariability in daily resting heart rate and its associations with age, sex, sleep, BMI,\nand time of year: retrospective, longitudinal cohort study of 92,457 adults. PLoS\nOne 2020;15:e0227709.\n11. McInnes L, Healy J, Melville J. Umap: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint arXiv:1802.03426 2018.\n12. Coote JH. Recovery of heart rate following intense dynamic exercise. Exp Physiol\n2010;95:431–440.\n13. Gyawali PK, Horacek BM, Sapp JL, Wang L. Sequential factorized autoencoder\nfor localizing the origin of ventricular activation from 12-lead electrocardiograms.\nIEEE Trans Biomed Eng",
        " Digital Health Journal 2023;4:165–172)\n© 2023\nHeart Rhythm Society. This is an open access article under the CC\nBY license (http://creativecommons.org/licenses/by/4.0/).\nIntroduction\nCardiovascular disease is one of the leading causes of mortal-\nity globally1 and cardiovascular healthcare accounts for a\nlarge portion of global healthcare costs and expenses. Early\ndetection and prevention will decrease the burden of cardio-\nvascular disease and will therefore decrease mortality,\nmorbidity, and costs. Cardiovascular monitoring using big\ndata from wearables and machine learning can drastically in-\ncrease the availability and ef ﬁciency of cardiovascular\nhealthcare globally, at the fraction of the costs of conven-\ntional medical-grade devices.\nElectrocardiogram (ECG) monitoring, such as Holters or\nimplantable loop recorders, are the gold standard for moni-\ntoring of outpatients with known or suspected arrhythmias.\nHowever, they are burdensome, can only be used for a\nlimited period of time, and are expensive. Implantable loop\nrecorders are invasive and have to be manually activated\nand analyzed in the hospital. This severely limits the use of\nthese devices for long-term home monitoring of patients,\nand they have suboptimal patient comfort. For patients with\nchronic cardiovascular diseases, such as atrial ﬁbrillation\nand heart failure, this implies frequent hospital visits and\nAddress reprint requests and correspondence:Mr Arman Naseri, Haga\nHospital, The Hague, Netherlands. E-mail address: a.naserijahfari@\nhagaziekenhuis.nl.\n2666-6936/© 2023 Heart Rhythm Society. This is an open access article under the CC BY license\n(http://creativecommons.org/licenses/by/4.0/).\nhttps://doi.org/10.1016/j.cvdhj.2023.09.001\n\nsometimes even hospital admissions (associated with higher\nmortality) that can be prevented by continuous and adequate\nhome monitoring.\nWith the widespread availability of reliable, consumer-\ngrade wearables such as smartwatches, continuous moni-\ntoring of, for example, heart rate with photoplethysmography\nand step counting with accelerometers is possible. This moni-\ntoring is easy, patient friendly, and cost effective. Combining\nthe power of large amounts of data (big data) and novel ma-\nchine learning techniques, these time series can be used to\ndetect and perhaps even predict cardiovascular disease, there-\nfore improving patient care. There are some caveats, howev-\ner, as not all wearables have the same characteristics and\nquality. Consequently, they have been used with moderate\nsuccess.\n2,3 They also provide less informative diagnostic sig-\nnals as compared to, for example, electrocardiography or\nother commonly used cardiologic diagnostic modalities.\nThe challenge but also the strength of machine learning\nmodels is that they learn by example and therefore large\namounts of data are needed for which the cardiovascular\noutcome (class label) has been determined. Typically, super-\nvised learning is used, where each observation of the data has\na class label. This must be done with ECGs, since photople-\nthysmography or derived signals are difﬁcult to interpret by a\nclinician. This so-called labeling or annotating of signals by\nphysicians is infeasible for the large amounts of (continuous)\ndata required, and therefore semi-automated\n4,5 and fully\nautomated2,3 ECG labeling systems6 have been developed.\nHowever, these still require a lot of manual labor from contin-\nuously monitored users.\nTherefore, the objective of the ME-TIME is (early) detec-\ntion and prevention of heart disease by leveraging time series\ndata from smartwatches, a cloud-based infrastructure, and\nmachine learning algorithms",
        ", is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2519
    }
  ]
}
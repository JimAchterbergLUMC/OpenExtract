{
  "paper": "A Machine Learning Approach for Non-Invasive Diagnosis of Metabolic Syndrome.pdf",
  "answers": [
    {
      "id": "Q1",
      "question": "Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?",
      "answer": "Diagnostic.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        7,
        2
      ],
      "chunks_str": [
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis",
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates",
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2532
    },
    {
      "id": "Q2",
      "question": "On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        14,
        15
      ],
      "chunks_str": [
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis",
        "wang, J. Jang, J. Leem, J.-Y . Park,\nH.-K. Kim, and W. Lee, “Serum bilirubin as a predictor of incident\nmetabolic syndrome: a 4-year retrospective longitudinal study of 6205\ninitially healthy korean men,” Diabetes & metabolism, vol. 40, no. 4,\npp. 305–309, 2014.\n[13] J. Olza, C. M. Aguilera, M. Gil-Campos, R. Leis, G. Bueno, M. Valle,\nR. Ca˜nete, R. Tojo, L. A. Moreno, and´A. Gil, “A continuous metabolic\nsyndrome score is associated with speciﬁc biomarkers of inﬂamma-\ntion and cvd risk in prepubertal children,” Annals of Nutrition and\nMetabolism, vol. 66, no. 2-3, pp. 72–79, 2015.\n[14] A. Bener, M. T. Yousafzai, S. Darwish, A. O. Al-Hamaq, E. A. Nasralla,\nand M. Abdul-Ghani, “Obesity index that better predict metabolic\nsyndrome: body mass index, waist circumference, waist hip ratio, or\nwaist height ratio,”Journal of obesity, vol. 2013, 2013.\n[15] G. Sagun, A. Oguz, E. Karagoz, A. T. Filizer, G. Tamer, B. Mesciet al.,\n“Application of alternative anthropometric measurements to predict\nmetabolic syndrome,”Clinics, vol. 69, no. 5, pp. 347–353, 2014.\n[16] D.-Y . Hsiung, C.-W. Liu, P.-C. Cheng, and W.-F. Ma, “Using non-\ninvasive assessment methods to predict the risk of metabolic syndrome,”\nApplied Nursing Research, vol. 28, no. 2, pp. 72–77, 2015.\n[17] M. Romero-Salda ˜na, F. J. Fuentes-Jim ´enez, M. Vaquero-Abell ´an,\nC. ´Alvarez-Fern´andez, G. Molina-Recio, and J. L´opez-Miranda, “New\nnon-invasive method for early detection of metabolic syndrome in the\nworking population,” European Journal of Cardiovascular Nursing,\nvol. 15, no. 7, pp. 549–558, 2016.\n[18] M. Romero-Salda ˜na, P. Tauler, M. Vaquero-Abell ´an, A.-A. L ´opez-\nGonz´alez, F.-J. Fuentes-Jim ´enez, A. Aguil ´o, C. ´Alvarez-Fern´andez,\nG. Molina-Recio, and M. Bennasar-Veny, “Validation of a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D.",
        " a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D. G. Altman and P. Royston, “The cost of dichotomising continuous\nvariables,”Bmj, vol. 332, no. 7549, p. 1080, 2006.\n[22] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.\n5–32, 2001.\n[23] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, “Classiﬁca-\ntion and regression trees. belmont, ca: Wadsworth,”International Group,\nvol. 432, pp. 151–166, 1984.\n939\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\n[24] J. H. Friedman, “Greedy function approximation: a gradient boosting\nmachine,”Annals of statistics, pp. 1189–1232, 2001.\n[25] J. D. Kelleher, B. Mac Namee, and A. D’arcy,Fundamentals of machine\nlearning for predictive data analytics: algorithms, worked examples, and\ncase studies. MIT Press, 2015.\n[26] C. X. Ling, J. Huang, H. Zhang et al., “Auc: a statistically consistent\nand more discriminating measure than accuracy,” inIjcai, vol. 3, 2003,\npp. 519–524.\n[27] F. J. Dankers, A. Traverso, L. Wee, and S. M. van Kuijk, “Prediction\nmodeling methodology,” in Fundamentals of Clinical Data Science.\nSpringer, 2019, pp. 101–120.\n[28] A. Niculescu-Mizil and R. Caruana, “Predicting good probabilities\nwith supervised learning,” in Proceedings of the 22nd international\nconference on Machine learning. ACM, 2005, pp. 625–632.\n[29] M. H. DeGroot and S. E. Fienberg, “The comparison and evaluation\nof forecasters,”Journal of the Royal Statistical Society: Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncompar"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2542
    },
    {
      "id": "Q3",
      "question": "How were participants sampled in this study: by convenience, randomly, or consecutively?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        6,
        4,
        7
      ],
      "chunks_str": [
        " classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Discriminative Performance: The primary metric that\nwe use to judge the discriminative power of a model is\nthe Area Under the Curve (AUC) of the receiver operating\ncharacteristic (ROC) curve. Models with ideal discrimination\npower have an AUC of 1.0 and the ones with no discrimi-\nnatory ability have AUC equal 0.5. Along with AUC we also\nreport balanced accuracy (or average class accuracy), precision\n(or positive predictive value) and recall (or true positive\nrate). Balanced accuracy is calculated as the average of the\nproportion of correct predictions for each class individually.\nBalanced accuracy is chosen ahead of regular accuracy as it\nprovides a better overview of the model performance than the\nclassiﬁcation accuracy when the classes are not balanced [25].\nPrecision is the proportion of positive identiﬁcations by the\nmodel that were actually correct. Recall is the proportion of\nactual positives that were identiﬁed correctly.\nIt should be noted, that AUC is the strongest indicator of the\nmodel performance among all the discriminative performance\nmetrics that we report here. AUC is statistically consistent and\nmore discriminating than accuracy [26]. Balanced accuracy,\nprecision and recall are all dependent on the threshold we\nchoose to apply on the probability estimations of the class\nprediction to get the class labels. Choosing an appropriate\ndecision threshold is highly dependent on the desired outcome\nof the predictive model (high speciﬁcity or high sensitivity).\nSince our goal is to give a probability score for MetS, AUC\nis more relevant for us. For computing balanced accuracy,\nprecision and recall, a decision threshold of 0.5 is assumed.\nA 5-fold cross-validation was performed to get a more robust\nestimate of the validation metrics. The hyper-parameters for\nthe different classiﬁers were optimized empirically.\n2) Model Calibration: Model calibration refers to the\nagreement between subgroups of predicted probabilities and\ntheir observed frequencies [27]. For predictive models in\nmedicine, usually the predicted probabilities of the different\nclass labels are important as they show how conﬁdent the\nmodel is about the predictions. Models like LR are usually\nwell calibrated as it directly optimizes the log-loss. But many\nother machine learning models like RF are thought to be\npoorly calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning",
        ")\nGender M = 918 F = 1396 M=4 4 1F=5 0 0\nAge 55.20 ±8.47 57.47 ±8.16\nHeight(in cm) 170.79 ±9.04 171.16 ±9.03\nWeight(in KG) 80.31 ±15.96 88.38 ±15.46\nSystolic blood pres-\nsure(in mmHg) 127.87 ±16.27 134.09 ±15.11\nDiastolic blood pres-\nsure(in mmHg) 79.23 ±10.04 82.46 ±9.96\nWaist circumference\n(in cm) 91.46 ±13.64 100.56 ±11.83\nBlood sugar (in\nmg/dL) 112.37 ±25.74 120.98 ±31.54\nTriglycerides (in\nmg/dL) 173.78 ±94.61 232.93 ±100.16\nHDL-C (in mg/dL) 59.86 ±18.18 49.80 ±15.85\nHbA1c (in Percent-\nage) 5.43 ±0.59 5.65 ±0.73\nBlood Sugar Medica-\ntion Y = 103, N = 2086 Y=9 9 ,N=7 7 7\nTriglyceride Medica-\ntion Y = 72, N = 2117 Y=6 8 ,N=8 0 8\nHDL Medication Y = 167, N = 2022 Y = 145, N = 731\nM = Male, F= Female,Y=Y e s ,N=n o\naccording to the Deurenberg equation [17]. Deurenberg, West-\nstrate and Seidell proposed (1) for estimating BFP where sex\nis 0 for females and 1 for males [19].\nBFP =1 .20 ∗BMI +0 .23 ∗age −10.8 ∗sex −5.4 (1)\nAfter computing the new features, we construct a few\nsubsets of the whole feature space. Although it may seem\ncounter intuitive to use a subset of the features instead of all, it\nis often observed that only a few of those features are actually\nrelated to the target concept. Irrelevant and redundant features\nmay confuse the machine learning algorithm by shrouding the\ndistributions of the small set of truly relevant features for\nthe task at hand [20]. In this situation, feature selection is\nimportant to speed up learning and to improve concept quality.\nIt should be noted that the feature subsets here are created\nmanually mostly based on prior work and domain knowledge.\nThese smaller subsets are necessary to compare our work\nwith the results of Romero-Salda˜na et al. and establish some\nbaselines [17]. We deﬁne the following feature sets:\n1) Set A: age, sex, height, weight, systolic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts",
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2514
    },
    {
      "id": "Q4",
      "question": "How was the dataset described in this study before predictive modeling was performed?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        2,
        8
      ],
      "chunks_str": [
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates",
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        " self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates a balanced accuracy\nof 0.73 (recall: 0.55, precision: 0.80) on the whole dataset.\nThe results are in accordance with the later work that was\npublished to validate this method on a bigger dataset [18]. The\nlow recall again underlines the fact that the ﬁxed rule based\napproach suffers from a high number of false negatives, i.e it\n936\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE III\nCOMPREHENSIVE PERFORMANCE COMPARISON OF ALL THE CLASSIFIERS ACROSS DIFFERENT FEATURE SETS\nClassiﬁers\nFeature Sets\nSet A Set B Set C [17] Set D\nAUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR\nLogistic Re-\ngression\n0.88\n(+/-\n0.02)\n0.78\n(+/-\n0.05)\n0.78\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.87\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.77\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.81\n(+/-\n0.01)\n0.73\n(+/-\n0.03)\n0.71\n(+/-\n0.03)\n0.56\n(+/-\n0.10)\n0.85\n(+/-\n0.03)\n0.76\n(+/-\n0.03)\n0.75\n(+/-\n0.05)\n0.67\n(+/-\n0.10)\nRandom For-\nest [22]\n0.90\n(+/-\n0.02)\n0.82\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.80\n(+/-\n0.10)\n0.86\n(+/-\n0.01)\n0.79\n(+/-\n0.02)\n0.74\n(+/-\n0.06)\n0.76\n(+/-\n0.09)\n0.84\n(+/-\n0.02)\n0.76\n(+/-\n0.01)\n0.72\n(+/-\n0.06)\n0.70\n(+/-\n0.09)\n0.81\n(+/-\n0.04)\n0.73\n(+/-\n0.04)\n0.68\n(+/-\n0.05)\n0.68\n(+/-\n0.06)\nGBM [24]\n0.89\n(+/-\n0.02)\n0.80\n(+/-\n0.03)\n0.78\n(+/-\n0.05)\n0.74\n(+/-\n0.13)\n0.87\n(+/-\n0.02)\n0.79\n(+/-\n0.03)\n0.76\n(+/"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2533
    },
    {
      "id": "Q5",
      "question": "What was the proedure for splitting the dataset into training, validation, and test sets in this study?",
      "answer": "20% test set, rest for training.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        8,
        2,
        10
      ],
      "chunks_str": [
        " self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates a balanced accuracy\nof 0.73 (recall: 0.55, precision: 0.80) on the whole dataset.\nThe results are in accordance with the later work that was\npublished to validate this method on a bigger dataset [18]. The\nlow recall again underlines the fact that the ﬁxed rule based\napproach suffers from a high number of false negatives, i.e it\n936\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE III\nCOMPREHENSIVE PERFORMANCE COMPARISON OF ALL THE CLASSIFIERS ACROSS DIFFERENT FEATURE SETS\nClassiﬁers\nFeature Sets\nSet A Set B Set C [17] Set D\nAUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR\nLogistic Re-\ngression\n0.88\n(+/-\n0.02)\n0.78\n(+/-\n0.05)\n0.78\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.87\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.77\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.81\n(+/-\n0.01)\n0.73\n(+/-\n0.03)\n0.71\n(+/-\n0.03)\n0.56\n(+/-\n0.10)\n0.85\n(+/-\n0.03)\n0.76\n(+/-\n0.03)\n0.75\n(+/-\n0.05)\n0.67\n(+/-\n0.10)\nRandom For-\nest [22]\n0.90\n(+/-\n0.02)\n0.82\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.80\n(+/-\n0.10)\n0.86\n(+/-\n0.01)\n0.79\n(+/-\n0.02)\n0.74\n(+/-\n0.06)\n0.76\n(+/-\n0.09)\n0.84\n(+/-\n0.02)\n0.76\n(+/-\n0.01)\n0.72\n(+/-\n0.06)\n0.70\n(+/-\n0.09)\n0.81\n(+/-\n0.04)\n0.73\n(+/-\n0.04)\n0.68\n(+/-\n0.05)\n0.68\n(+/-\n0.06)\nGBM [24]\n0.89\n(+/-\n0.02)\n0.80\n(+/-\n0.03)\n0.78\n(+/-\n0.05)\n0.74\n(+/-\n0.13)\n0.87\n(+/-\n0.02)\n0.79\n(+/-\n0.03)\n0.76\n(+/",
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        "-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig."
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2546
    },
    {
      "id": "Q6",
      "question": "What preprocessing techniques on the included variables/features were applied in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        2,
        10,
        8
      ],
      "chunks_str": [
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        "-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.",
        " self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates a balanced accuracy\nof 0.73 (recall: 0.55, precision: 0.80) on the whole dataset.\nThe results are in accordance with the later work that was\npublished to validate this method on a bigger dataset [18]. The\nlow recall again underlines the fact that the ﬁxed rule based\napproach suffers from a high number of false negatives, i.e it\n936\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE III\nCOMPREHENSIVE PERFORMANCE COMPARISON OF ALL THE CLASSIFIERS ACROSS DIFFERENT FEATURE SETS\nClassiﬁers\nFeature Sets\nSet A Set B Set C [17] Set D\nAUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR\nLogistic Re-\ngression\n0.88\n(+/-\n0.02)\n0.78\n(+/-\n0.05)\n0.78\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.87\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.77\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.81\n(+/-\n0.01)\n0.73\n(+/-\n0.03)\n0.71\n(+/-\n0.03)\n0.56\n(+/-\n0.10)\n0.85\n(+/-\n0.03)\n0.76\n(+/-\n0.03)\n0.75\n(+/-\n0.05)\n0.67\n(+/-\n0.10)\nRandom For-\nest [22]\n0.90\n(+/-\n0.02)\n0.82\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.80\n(+/-\n0.10)\n0.86\n(+/-\n0.01)\n0.79\n(+/-\n0.02)\n0.74\n(+/-\n0.06)\n0.76\n(+/-\n0.09)\n0.84\n(+/-\n0.02)\n0.76\n(+/-\n0.01)\n0.72\n(+/-\n0.06)\n0.70\n(+/-\n0.09)\n0.81\n(+/-\n0.04)\n0.73\n(+/-\n0.04)\n0.68\n(+/-\n0.05)\n0.68\n(+/-\n0.06)\nGBM [24]\n0.89\n(+/-\n0.02)\n0.80\n(+/-\n0.03)\n0.78\n(+/-\n0.05)\n0.74\n(+/-\n0.13)\n0.87\n(+/-\n0.02)\n0.79\n(+/-\n0.03)\n0.76\n(+/"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2534
    },
    {
      "id": "Q7",
      "question": "How is missing data handled in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        4,
        12
      ],
      "chunks_str": [
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis",
        ")\nGender M = 918 F = 1396 M=4 4 1F=5 0 0\nAge 55.20 ±8.47 57.47 ±8.16\nHeight(in cm) 170.79 ±9.04 171.16 ±9.03\nWeight(in KG) 80.31 ±15.96 88.38 ±15.46\nSystolic blood pres-\nsure(in mmHg) 127.87 ±16.27 134.09 ±15.11\nDiastolic blood pres-\nsure(in mmHg) 79.23 ±10.04 82.46 ±9.96\nWaist circumference\n(in cm) 91.46 ±13.64 100.56 ±11.83\nBlood sugar (in\nmg/dL) 112.37 ±25.74 120.98 ±31.54\nTriglycerides (in\nmg/dL) 173.78 ±94.61 232.93 ±100.16\nHDL-C (in mg/dL) 59.86 ±18.18 49.80 ±15.85\nHbA1c (in Percent-\nage) 5.43 ±0.59 5.65 ±0.73\nBlood Sugar Medica-\ntion Y = 103, N = 2086 Y=9 9 ,N=7 7 7\nTriglyceride Medica-\ntion Y = 72, N = 2117 Y=6 8 ,N=8 0 8\nHDL Medication Y = 167, N = 2022 Y = 145, N = 731\nM = Male, F= Female,Y=Y e s ,N=n o\naccording to the Deurenberg equation [17]. Deurenberg, West-\nstrate and Seidell proposed (1) for estimating BFP where sex\nis 0 for females and 1 for males [19].\nBFP =1 .20 ∗BMI +0 .23 ∗age −10.8 ∗sex −5.4 (1)\nAfter computing the new features, we construct a few\nsubsets of the whole feature space. Although it may seem\ncounter intuitive to use a subset of the features instead of all, it\nis often observed that only a few of those features are actually\nrelated to the target concept. Irrelevant and redundant features\nmay confuse the machine learning algorithm by shrouding the\ndistributions of the small set of truly relevant features for\nthe task at hand [20]. In this situation, feature selection is\nimportant to speed up learning and to improve concept quality.\nIt should be noted that the feature subsets here are created\nmanually mostly based on prior work and domain knowledge.\nThese smaller subsets are necessary to compare our work\nwith the results of Romero-Salda˜na et al. and establish some\nbaselines [17]. We deﬁne the following feature sets:\n1) Set A: age, sex, height, weight, systolic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts",
        "[22] 0.139 0.138 0.138\nGBMs [24] 0.156 0.138 0.137\nEnsemble 0.153 0.141 0.140\nThough showing promising results for a relatively unex-\nplored problem, our study has certain limitations. We do\nacknowledge, our training cohort contains subjects with pre-\ndominately European origin, with an age range of 40 to\n70. This limits the generalizability of our models to other\npopulations. Secondly it is a retrospective study with all the\nusual limitations. Finally, we also acknowledge the limitations\nrising from using MetS as the target variable, instead of\nusing the secondary complications such as CVDs and type\n2 diabetes directly. Training on the secondary complications\ndirectly would have allowed us to circumvent the problems\nrising from the dichotomous deﬁnition currently used for MetS\n[36].\nThis study can be used as a prototype for developing a\nnon-invasive risk score for the diagnosis of MetS. In future,\nwe plan to validate our methods on different cohorts and also\non electronic health record (EHR) databases. EHR databases\nwith longitudinal patient data will also allow us to train our\nmodels with the secondary complications as target variable.\nFinally we would also like to add more features to our cohort,\nspecially those collected from wearable sensors such as heart\nrate, HRV and daily activity information to see if that improves\nthe diagnostic accuracy.\nACKNOWLEDGMENT\nWe would like to thank Prof. M. Rapp, Prof. P. M. Wippert,\nProf. H. V ¨oller and Dr. K. Bonaventura from University\nof Potsdam for helping us with the study design and data\ncollection.\nREFERENCES\n[1] R. H. Eckel, S. M. Grundy, and P. Z. Zimmet, “The metabolic syn-\ndrome,”The Lancet, vol. 365, no. 9468, pp. 1415–1428, 2005.\n[2] P. Zimmet, K. Alberti, and J. Shaw, “Global and societal implications\nof the diabetes epidemic,”Nature, vol. 414, no. 6865, p. 782, 2001.\n[3] P. Z. Zimmet, K. G. M. Alberti, and J. E. Shaw, “Mainstreaming\nthe metabolic syndrome: a deﬁnitive deﬁnition,” Medical Journal of\nAustralia, vol. 183, no. 4, p. 175, 2005.\n[4] World Health Organization and others, “Deﬁnition, diagnosis and clas-\nsiﬁcation of diabetes mellitus and its complications: report of a who\nconsultation. part 1, diagnosis and classiﬁcation of diabetes mellitus,”\nGeneva: World health organization, Tech. Rep., 1999.\n[5] Expert Panel on Detection, Evaluation and others, “Executive summary\nof the third report of the national cholesterol education program (ncep)\nexpert panel on detection, evaluation, and treatment of high blood\ncholesterol in adults (adult treatment panel iii).”JAMA, vol. 285, no. 19,\np. 2486, 2001.\n[6] K. G. M. M. Alberti, P. Zimmet, and J. Shaw, “Metabolic syndrome-a\nnew world-wide deﬁnition. a consensus statement from the international\ndiabetes federation,” Diabetic medicine, vol. 23, no. 5, pp. 469–480,\n2006.\n[7] S. M. Grundy, J. I."
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2506
    },
    {
      "id": "Q8",
      "question": "How are outliers handled in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        14,
        16
      ],
      "chunks_str": [
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates",
        "wang, J. Jang, J. Leem, J.-Y . Park,\nH.-K. Kim, and W. Lee, “Serum bilirubin as a predictor of incident\nmetabolic syndrome: a 4-year retrospective longitudinal study of 6205\ninitially healthy korean men,” Diabetes & metabolism, vol. 40, no. 4,\npp. 305–309, 2014.\n[13] J. Olza, C. M. Aguilera, M. Gil-Campos, R. Leis, G. Bueno, M. Valle,\nR. Ca˜nete, R. Tojo, L. A. Moreno, and´A. Gil, “A continuous metabolic\nsyndrome score is associated with speciﬁc biomarkers of inﬂamma-\ntion and cvd risk in prepubertal children,” Annals of Nutrition and\nMetabolism, vol. 66, no. 2-3, pp. 72–79, 2015.\n[14] A. Bener, M. T. Yousafzai, S. Darwish, A. O. Al-Hamaq, E. A. Nasralla,\nand M. Abdul-Ghani, “Obesity index that better predict metabolic\nsyndrome: body mass index, waist circumference, waist hip ratio, or\nwaist height ratio,”Journal of obesity, vol. 2013, 2013.\n[15] G. Sagun, A. Oguz, E. Karagoz, A. T. Filizer, G. Tamer, B. Mesciet al.,\n“Application of alternative anthropometric measurements to predict\nmetabolic syndrome,”Clinics, vol. 69, no. 5, pp. 347–353, 2014.\n[16] D.-Y . Hsiung, C.-W. Liu, P.-C. Cheng, and W.-F. Ma, “Using non-\ninvasive assessment methods to predict the risk of metabolic syndrome,”\nApplied Nursing Research, vol. 28, no. 2, pp. 72–77, 2015.\n[17] M. Romero-Salda ˜na, F. J. Fuentes-Jim ´enez, M. Vaquero-Abell ´an,\nC. ´Alvarez-Fern´andez, G. Molina-Recio, and J. L´opez-Miranda, “New\nnon-invasive method for early detection of metabolic syndrome in the\nworking population,” European Journal of Cardiovascular Nursing,\nvol. 15, no. 7, pp. 549–558, 2016.\n[18] M. Romero-Salda ˜na, P. Tauler, M. Vaquero-Abell ´an, A.-A. L ´opez-\nGonz´alez, F.-J. Fuentes-Jim ´enez, A. Aguil ´o, C. ´Alvarez-Fern´andez,\nG. Molina-Recio, and M. Bennasar-Veny, “Validation of a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D.",
        ": Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncomparisons to regularized likelihood methods,” Advances in large\nmargin classiﬁers, vol. 10, no. 3, pp. 61–74, 1999.\n[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay, “Scikit-learn: Machine learning in Python,”Journal of Machine\nLearning Research, vol. 12, pp. 2825–2830, 2011.\n[34] J. D. Hunter, “Matplotlib: A 2d graphics environment,” Computing in\nScience & Engineering, vol. 9, no. 3, pp. 90–95, 2007.\n[35] V . Valev and A. Asaithambi, “Multidimensional pattern recognition\nproblems and combining classiﬁers,” Pattern Recognition Letters,\nvol. 22, no. 12, pp. 1291–1297, 2001.\n[36] G.-D. Kang, L. Guo, Z.-R. Guo, X.-S. Hu, M. Wu, and H.-T. Yang,\n“Continuous metabolic syndrome risk score for predicting cardiovascular\ndisease in the chinese population,” Asia Paciﬁc journal of clinical\nnutrition, vol. 21, no. 1, pp. 88–96, 2012.\n940\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. "
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2275
    },
    {
      "id": "Q9",
      "question": "Which prediction models were used in this study?",
      "answer": "Logistic Regression, Random Forest, Gradient Boosting Machines, Ensemble Classifier",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        2,
        5
      ],
      "chunks_str": [
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates",
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2530
    },
    {
      "id": "Q10",
      "question": "What considerations were given to model selection and hyperparameter tuning in this study?",
      "answer": "Compared algorithms and feature subsets; used cross-validation and calibration.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        2,
        5,
        7
      ],
      "chunks_str": [
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis",
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2533
    },
    {
      "id": "Q11",
      "question": "How was data augmentation or generation used in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        12,
        5,
        16
      ],
      "chunks_str": [
        "[22] 0.139 0.138 0.138\nGBMs [24] 0.156 0.138 0.137\nEnsemble 0.153 0.141 0.140\nThough showing promising results for a relatively unex-\nplored problem, our study has certain limitations. We do\nacknowledge, our training cohort contains subjects with pre-\ndominately European origin, with an age range of 40 to\n70. This limits the generalizability of our models to other\npopulations. Secondly it is a retrospective study with all the\nusual limitations. Finally, we also acknowledge the limitations\nrising from using MetS as the target variable, instead of\nusing the secondary complications such as CVDs and type\n2 diabetes directly. Training on the secondary complications\ndirectly would have allowed us to circumvent the problems\nrising from the dichotomous deﬁnition currently used for MetS\n[36].\nThis study can be used as a prototype for developing a\nnon-invasive risk score for the diagnosis of MetS. In future,\nwe plan to validate our methods on different cohorts and also\non electronic health record (EHR) databases. EHR databases\nwith longitudinal patient data will also allow us to train our\nmodels with the secondary complications as target variable.\nFinally we would also like to add more features to our cohort,\nspecially those collected from wearable sensors such as heart\nrate, HRV and daily activity information to see if that improves\nthe diagnostic accuracy.\nACKNOWLEDGMENT\nWe would like to thank Prof. M. Rapp, Prof. P. M. Wippert,\nProf. H. V ¨oller and Dr. K. Bonaventura from University\nof Potsdam for helping us with the study design and data\ncollection.\nREFERENCES\n[1] R. H. Eckel, S. M. Grundy, and P. Z. Zimmet, “The metabolic syn-\ndrome,”The Lancet, vol. 365, no. 9468, pp. 1415–1428, 2005.\n[2] P. Zimmet, K. Alberti, and J. Shaw, “Global and societal implications\nof the diabetes epidemic,”Nature, vol. 414, no. 6865, p. 782, 2001.\n[3] P. Z. Zimmet, K. G. M. Alberti, and J. E. Shaw, “Mainstreaming\nthe metabolic syndrome: a deﬁnitive deﬁnition,” Medical Journal of\nAustralia, vol. 183, no. 4, p. 175, 2005.\n[4] World Health Organization and others, “Deﬁnition, diagnosis and clas-\nsiﬁcation of diabetes mellitus and its complications: report of a who\nconsultation. part 1, diagnosis and classiﬁcation of diabetes mellitus,”\nGeneva: World health organization, Tech. Rep., 1999.\n[5] Expert Panel on Detection, Evaluation and others, “Executive summary\nof the third report of the national cholesterol education program (ncep)\nexpert panel on detection, evaluation, and treatment of high blood\ncholesterol in adults (adult treatment panel iii).”JAMA, vol. 285, no. 19,\np. 2486, 2001.\n[6] K. G. M. M. Alberti, P. Zimmet, and J. Shaw, “Metabolic syndrome-a\nnew world-wide deﬁnition. a consensus statement from the international\ndiabetes federation,” Diabetic medicine, vol. 23, no. 5, pp. 469–480,\n2006.\n[7] S. M. Grundy, J. I.",
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis",
        ": Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncomparisons to regularized likelihood methods,” Advances in large\nmargin classiﬁers, vol. 10, no. 3, pp. 61–74, 1999.\n[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay, “Scikit-learn: Machine learning in Python,”Journal of Machine\nLearning Research, vol. 12, pp. 2825–2830, 2011.\n[34] J. D. Hunter, “Matplotlib: A 2d graphics environment,” Computing in\nScience & Engineering, vol. 9, no. 3, pp. 90–95, 2007.\n[35] V . Valev and A. Asaithambi, “Multidimensional pattern recognition\nproblems and combining classiﬁers,” Pattern Recognition Letters,\nvol. 22, no. 12, pp. 1291–1297, 2001.\n[36] G.-D. Kang, L. Guo, Z.-R. Guo, X.-S. Hu, M. Wu, and H.-T. Yang,\n“Continuous metabolic syndrome risk score for predicting cardiovascular\ndisease in the chinese population,” Asia Paciﬁc journal of clinical\nnutrition, vol. 21, no. 1, pp. 88–96, 2012.\n940\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. "
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2272
    },
    {
      "id": "Q12",
      "question": "Is the performance of the predictive models benchmarked or compared to a baseline?",
      "answer": "Yes, compared to a rule-based baseline.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        2,
        8,
        7
      ],
      "chunks_str": [
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        " self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates a balanced accuracy\nof 0.73 (recall: 0.55, precision: 0.80) on the whole dataset.\nThe results are in accordance with the later work that was\npublished to validate this method on a bigger dataset [18]. The\nlow recall again underlines the fact that the ﬁxed rule based\napproach suffers from a high number of false negatives, i.e it\n936\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE III\nCOMPREHENSIVE PERFORMANCE COMPARISON OF ALL THE CLASSIFIERS ACROSS DIFFERENT FEATURE SETS\nClassiﬁers\nFeature Sets\nSet A Set B Set C [17] Set D\nAUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR\nLogistic Re-\ngression\n0.88\n(+/-\n0.02)\n0.78\n(+/-\n0.05)\n0.78\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.87\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.77\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.81\n(+/-\n0.01)\n0.73\n(+/-\n0.03)\n0.71\n(+/-\n0.03)\n0.56\n(+/-\n0.10)\n0.85\n(+/-\n0.03)\n0.76\n(+/-\n0.03)\n0.75\n(+/-\n0.05)\n0.67\n(+/-\n0.10)\nRandom For-\nest [22]\n0.90\n(+/-\n0.02)\n0.82\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.80\n(+/-\n0.10)\n0.86\n(+/-\n0.01)\n0.79\n(+/-\n0.02)\n0.74\n(+/-\n0.06)\n0.76\n(+/-\n0.09)\n0.84\n(+/-\n0.02)\n0.76\n(+/-\n0.01)\n0.72\n(+/-\n0.06)\n0.70\n(+/-\n0.09)\n0.81\n(+/-\n0.04)\n0.73\n(+/-\n0.04)\n0.68\n(+/-\n0.05)\n0.68\n(+/-\n0.06)\nGBM [24]\n0.89\n(+/-\n0.02)\n0.80\n(+/-\n0.03)\n0.78\n(+/-\n0.05)\n0.74\n(+/-\n0.13)\n0.87\n(+/-\n0.02)\n0.79\n(+/-\n0.03)\n0.76\n(+/",
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2539
    },
    {
      "id": "Q13",
      "question": "Which type of explainability techniques are used?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        6,
        10,
        5
      ],
      "chunks_str": [
        " classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Discriminative Performance: The primary metric that\nwe use to judge the discriminative power of a model is\nthe Area Under the Curve (AUC) of the receiver operating\ncharacteristic (ROC) curve. Models with ideal discrimination\npower have an AUC of 1.0 and the ones with no discrimi-\nnatory ability have AUC equal 0.5. Along with AUC we also\nreport balanced accuracy (or average class accuracy), precision\n(or positive predictive value) and recall (or true positive\nrate). Balanced accuracy is calculated as the average of the\nproportion of correct predictions for each class individually.\nBalanced accuracy is chosen ahead of regular accuracy as it\nprovides a better overview of the model performance than the\nclassiﬁcation accuracy when the classes are not balanced [25].\nPrecision is the proportion of positive identiﬁcations by the\nmodel that were actually correct. Recall is the proportion of\nactual positives that were identiﬁed correctly.\nIt should be noted, that AUC is the strongest indicator of the\nmodel performance among all the discriminative performance\nmetrics that we report here. AUC is statistically consistent and\nmore discriminating than accuracy [26]. Balanced accuracy,\nprecision and recall are all dependent on the threshold we\nchoose to apply on the probability estimations of the class\nprediction to get the class labels. Choosing an appropriate\ndecision threshold is highly dependent on the desired outcome\nof the predictive model (high speciﬁcity or high sensitivity).\nSince our goal is to give a probability score for MetS, AUC\nis more relevant for us. For computing balanced accuracy,\nprecision and recall, a decision threshold of 0.5 is assumed.\nA 5-fold cross-validation was performed to get a more robust\nestimate of the validation metrics. The hyper-parameters for\nthe different classiﬁers were optimized empirically.\n2) Model Calibration: Model calibration refers to the\nagreement between subgroups of predicted probabilities and\ntheir observed frequencies [27]. For predictive models in\nmedicine, usually the predicted probabilities of the different\nclass labels are important as they show how conﬁdent the\nmodel is about the predictions. Models like LR are usually\nwell calibrated as it directly optimizes the log-loss. But many\nother machine learning models like RF are thought to be\npoorly calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning",
        "-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.",
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2510
    },
    {
      "id": "Q14",
      "question": "Which evaluation metrics or outcome measures are used to assess the predictive models?",
      "answer": "AUC, balanced accuracy, recall, Brier score",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        2,
        10
      ],
      "chunks_str": [
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates",
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        "-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig."
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2529
    },
    {
      "id": "Q15",
      "question": "What considerations were given to selected evaluation metrics or outcome measures in this study?",
      "answer": "AUC, balanced accuracy, recall, calibration, Brier score.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        2,
        12,
        7
      ],
      "chunks_str": [
        " obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was",
        "[22] 0.139 0.138 0.138\nGBMs [24] 0.156 0.138 0.137\nEnsemble 0.153 0.141 0.140\nThough showing promising results for a relatively unex-\nplored problem, our study has certain limitations. We do\nacknowledge, our training cohort contains subjects with pre-\ndominately European origin, with an age range of 40 to\n70. This limits the generalizability of our models to other\npopulations. Secondly it is a retrospective study with all the\nusual limitations. Finally, we also acknowledge the limitations\nrising from using MetS as the target variable, instead of\nusing the secondary complications such as CVDs and type\n2 diabetes directly. Training on the secondary complications\ndirectly would have allowed us to circumvent the problems\nrising from the dichotomous deﬁnition currently used for MetS\n[36].\nThis study can be used as a prototype for developing a\nnon-invasive risk score for the diagnosis of MetS. In future,\nwe plan to validate our methods on different cohorts and also\non electronic health record (EHR) databases. EHR databases\nwith longitudinal patient data will also allow us to train our\nmodels with the secondary complications as target variable.\nFinally we would also like to add more features to our cohort,\nspecially those collected from wearable sensors such as heart\nrate, HRV and daily activity information to see if that improves\nthe diagnostic accuracy.\nACKNOWLEDGMENT\nWe would like to thank Prof. M. Rapp, Prof. P. M. Wippert,\nProf. H. V ¨oller and Dr. K. Bonaventura from University\nof Potsdam for helping us with the study design and data\ncollection.\nREFERENCES\n[1] R. H. Eckel, S. M. Grundy, and P. Z. Zimmet, “The metabolic syn-\ndrome,”The Lancet, vol. 365, no. 9468, pp. 1415–1428, 2005.\n[2] P. Zimmet, K. Alberti, and J. Shaw, “Global and societal implications\nof the diabetes epidemic,”Nature, vol. 414, no. 6865, p. 782, 2001.\n[3] P. Z. Zimmet, K. G. M. Alberti, and J. E. Shaw, “Mainstreaming\nthe metabolic syndrome: a deﬁnitive deﬁnition,” Medical Journal of\nAustralia, vol. 183, no. 4, p. 175, 2005.\n[4] World Health Organization and others, “Deﬁnition, diagnosis and clas-\nsiﬁcation of diabetes mellitus and its complications: report of a who\nconsultation. part 1, diagnosis and classiﬁcation of diabetes mellitus,”\nGeneva: World health organization, Tech. Rep., 1999.\n[5] Expert Panel on Detection, Evaluation and others, “Executive summary\nof the third report of the national cholesterol education program (ncep)\nexpert panel on detection, evaluation, and treatment of high blood\ncholesterol in adults (adult treatment panel iii).”JAMA, vol. 285, no. 19,\np. 2486, 2001.\n[6] K. G. M. M. Alberti, P. Zimmet, and J. Shaw, “Metabolic syndrome-a\nnew world-wide deﬁnition. a consensus statement from the international\ndiabetes federation,” Diabetic medicine, vol. 23, no. 5, pp. 469–480,\n2006.\n[7] S. M. Grundy, J. I.",
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2534
    },
    {
      "id": "Q16",
      "question": "How were robustness, confidence or statistical significance of the results assessed in this study?",
      "answer": "Cross-validation, AUC, Brier score, calibration plots.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        15,
        3,
        7
      ],
      "chunks_str": [
        " a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D. G. Altman and P. Royston, “The cost of dichotomising continuous\nvariables,”Bmj, vol. 332, no. 7549, p. 1080, 2006.\n[22] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.\n5–32, 2001.\n[23] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, “Classiﬁca-\ntion and regression trees. belmont, ca: Wadsworth,”International Group,\nvol. 432, pp. 151–166, 1984.\n939\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\n[24] J. H. Friedman, “Greedy function approximation: a gradient boosting\nmachine,”Annals of statistics, pp. 1189–1232, 2001.\n[25] J. D. Kelleher, B. Mac Namee, and A. D’arcy,Fundamentals of machine\nlearning for predictive data analytics: algorithms, worked examples, and\ncase studies. MIT Press, 2015.\n[26] C. X. Ling, J. Huang, H. Zhang et al., “Auc: a statistically consistent\nand more discriminating measure than accuracy,” inIjcai, vol. 3, 2003,\npp. 519–524.\n[27] F. J. Dankers, A. Traverso, L. Wee, and S. M. van Kuijk, “Prediction\nmodeling methodology,” in Fundamentals of Clinical Data Science.\nSpringer, 2019, pp. 101–120.\n[28] A. Niculescu-Mizil and R. Caruana, “Predicting good probabilities\nwith supervised learning,” in Proceedings of the 22nd international\nconference on Machine learning. ACM, 2005, pp. 625–632.\n[29] M. H. DeGroot and S. E. Fienberg, “The comparison and evaluation\nof forecasters,”Journal of the Royal Statistical Society: Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncompar",
        " pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was carried out in a mobile study center,\nwhich was placed in cities and rural areas across the state\nof Brandenburg, Germany (between 2016 and 2019). Partic-\nipants (aged 40 to 70 years old) were examined in mobile\nexamination units to diagnose MetS. Waist circumference was\nmeasured using a tape measure (Seca, Germany). The arterial\nblood pressure was measured manually by the auscultatory\nmethod using a sphygmomanometer with a stethoscope. Cap-\nillary blood was taken from the ﬁnger bulb for the analyses\nof blood glucose (in mg/dl), triglycerides and HDL choles-\nterol (in mg/dl) using the Alere Cholestech LDX analyzer\n(Alere, Germany). Additionally, body mass was measured\nusing an electronic digital scale (Seca 899, Seca, Germany).\nParticipants were asked for any drug treatment because of\ndiagnosed diabetes, arterial hypertension, hypertriglyceridemia\nor hypercholesterolemia. The study was approved by the\nethics committee of the University of Potsdam (ethics approval\nnumber: 40/2016).\nB. Cohort Description\nUsing the setting described above data from 2477 individ-\nuals were collected. After removing rows with missing values\nfor variables that would have been required to deﬁne the target\nvariable (in this case, presence of MetS) we end up with 2314\nsubjects. Following the diagnostic criteria described in section\nI, 941 subjects (40.66%) were identiﬁed as having MetS. A\nmore detailed description of the cohort can be found in Table\nII.\nC. Feature Engineering and Selection\nFollowing the work of Romero-Salda˜na et al. three addi-\ntional features were derived from the existing anthropometric\nvariables: waist to height ratio (WtHR), body mass index\n(BMI) in KG/m2 and body fat percentage (BFP) calculated\n934\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE II\nCOHORT DESCRIPTION\nFeature All (N= 2314) MetS (N = 941)\nGender M = 918 F = 1396 M=4 4 1F=5 0 0\nAge 55.20 ±8.47 57.47 ±8.16\nHeight(in cm) 170.79 ±9.04 171.16 ±9.03\nWeight(in KG) 80.31 ±15.96 88.38 ±15.46\nSystolic blood pres-\nsure(in mmHg) 127.87 ±16.27 134.09 ±15.11\nDiastolic blood pres-\nsure(in mmHg) 79.23 ±10.04 82.46 ±9.96\nWaist circumference\n(in cm) 91.46 ±13.64 100.",
        " calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2541
    },
    {
      "id": "Q17",
      "question": "What limitations of the study were discussed?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        14,
        5,
        0
      ],
      "chunks_str": [
        "wang, J. Jang, J. Leem, J.-Y . Park,\nH.-K. Kim, and W. Lee, “Serum bilirubin as a predictor of incident\nmetabolic syndrome: a 4-year retrospective longitudinal study of 6205\ninitially healthy korean men,” Diabetes & metabolism, vol. 40, no. 4,\npp. 305–309, 2014.\n[13] J. Olza, C. M. Aguilera, M. Gil-Campos, R. Leis, G. Bueno, M. Valle,\nR. Ca˜nete, R. Tojo, L. A. Moreno, and´A. Gil, “A continuous metabolic\nsyndrome score is associated with speciﬁc biomarkers of inﬂamma-\ntion and cvd risk in prepubertal children,” Annals of Nutrition and\nMetabolism, vol. 66, no. 2-3, pp. 72–79, 2015.\n[14] A. Bener, M. T. Yousafzai, S. Darwish, A. O. Al-Hamaq, E. A. Nasralla,\nand M. Abdul-Ghani, “Obesity index that better predict metabolic\nsyndrome: body mass index, waist circumference, waist hip ratio, or\nwaist height ratio,”Journal of obesity, vol. 2013, 2013.\n[15] G. Sagun, A. Oguz, E. Karagoz, A. T. Filizer, G. Tamer, B. Mesciet al.,\n“Application of alternative anthropometric measurements to predict\nmetabolic syndrome,”Clinics, vol. 69, no. 5, pp. 347–353, 2014.\n[16] D.-Y . Hsiung, C.-W. Liu, P.-C. Cheng, and W.-F. Ma, “Using non-\ninvasive assessment methods to predict the risk of metabolic syndrome,”\nApplied Nursing Research, vol. 28, no. 2, pp. 72–77, 2015.\n[17] M. Romero-Salda ˜na, F. J. Fuentes-Jim ´enez, M. Vaquero-Abell ´an,\nC. ´Alvarez-Fern´andez, G. Molina-Recio, and J. L´opez-Miranda, “New\nnon-invasive method for early detection of metabolic syndrome in the\nworking population,” European Journal of Cardiovascular Nursing,\nvol. 15, no. 7, pp. 549–558, 2016.\n[18] M. Romero-Salda ˜na, P. Tauler, M. Vaquero-Abell ´an, A.-A. L ´opez-\nGonz´alez, F.-J. Fuentes-Jim ´enez, A. Aguil ´o, C. ´Alvarez-Fern´andez,\nG. Molina-Recio, and M. Bennasar-Veny, “Validation of a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D.",
        "olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis",
        "A Machine Learning Approach for Non-Invasive\nDiagnosis of Metabolic Syndrome\nSuparno Datta∗†, Anne Schraplau‡, Harry Freitas da Cruz∗†, Jan Philipp Sachs∗†,\nFrank Mayer‡ and Erwin B¨ottinger∗†\n∗Digital Health Center\nHasso Plattner Institute gGmbH and University of Potsdam\nPotsdam, Germany\n† Hasso Plattner Institute for Digital Health at Mount Sinai\nIcahn School of Medicine at Mount Sinai\nNew York, United States\n‡University Outpatient Clinic, Sports Medicine and Sports Orthopaedics\nUniversity of Potsdam\nPotsdam, Germany\nEmail: suparno.datta@hpi.de\nAbstract—The metabolic syndrome is one of the major public\nhealth challenges worldwide. Prevalence of metabolic syndrome\nvastly increases the risk of type 2 diabetes and cardiovascular\ndiseases (CVDs). Metabolic Syndrome in general is under-\ndiagnosed and often goes undetected for years. In this paper\nwe present a machine learning based method for early detection\nof metabolic syndrome which uses only non-invasive features. We\ntrain and test our model based on data collected from a German\npopulation consisting of 2314 subjects (male = 918, female =\n1396). Out of 2314 subjects 941 were diagnosed with metabolic\nsyndrome (male = 441, female = 500). Features we consider\ninclude different anthropometric features (such as height, weight,\nwaist circumference), medications, age, gender etc.; machine\nlearning techniques we employed included gradient boosting\nmachines, random forest, logistic regression and an ensemble\nmodel. We compare our models against the ones that were\nproposed in previous literature and outperform them in our\ncohort. We achieve area under the curve values (AUCs) of up\nto 0.90 with the ensemble classiﬁer . The results achieved suggest\nthat machine learning can be a valuable tool to predict metabolic\nsyndrome with high discriminative power without relying on\nany invasive bio-markers, which signiﬁcantly facilitates early\ndetection.\nIndex T erms—Metabolic Syndrome, Machine learning, Predic-\ntive models, Biomedical informatics\nI. I NTRODUCTION\nOver the past three decades, there has been a stagger-\ning increase in the number of people with metabolic syn-\ndrome (MetS) worldwide [1]. MetS is characterized by hy-\nperglycemia, high blood pressure, central obesity and dyslipi-\ndemia. The increasing prevalence of MetS is associated with\nthe global epidemic of obesity and diabetes [2]. As a several-\nfold increase in the risk of diabetes and cardiovascular diseases\n(CVDs) is commonly associated with MetS, there is urgent\nneed for strategies to prevent this emerging global epidemic\nthat results in a considerable long-term burden for the public\nhealth system [1]–[3].\nA number of expert groups over the time have devel-\noped clinical criteria for MetS. The most widely accepted\nof these were produced by the World Health Organization\n(WHO), the National Cholesterol Education Program Third\nAdult Treatment Panel (NCEP ATP III), the International\nDiabetes Federation (IDF) and the American Heart Associa-\ntion/National Heart, Lung, and Blood Institute (AHA/NHLBI)\n[4]–[7]. To circumvent the inconsistencies arising from the\nmultiple criteria and to establish a uniﬁed working diagnostic\ntool for MetS, IDF and AHA/NHLBI together proposed a\nnew harmonized deﬁnition for this syndrome in 2009 [8].\nAccording to the new deﬁnition, the presence of any 3 of\n5 risk factors mentioned in Table I constitutes a diagnosis of\nMetS."
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2510
    }
  ]
}
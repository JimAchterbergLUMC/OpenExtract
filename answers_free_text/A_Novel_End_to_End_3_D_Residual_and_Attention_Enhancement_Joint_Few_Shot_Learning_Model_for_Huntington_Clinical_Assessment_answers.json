{
  "paper": "A Novel End-to-End 3-D Residual and Attention Enhancement Joint Few-Shot Learning Model for Huntington Clinical Assessment.pdf",
  "answers": [
    {
      "id": "Q1",
      "question": "Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        14,
        5
      ],
      "chunks_str": [
        "follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut",
        " the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVIDUAL COMPONENTS\nFig. 6 shows that the AUC areas of both 2DCNN and 3DCNN\nmodels are below 0.5, and their poor recall performance makes\nthem unsuitable for clinical assessment. Our model has an\nAUC area of up to 0.76, completely enveloping the baseline,\nindicating that our model not only outperforms other models\nbut also performs well at any judgment threshold.\nC. Ablation Study\nIn this section, we conducted ablation experiments on data\naugmentation, attention module, and the number of motion\nselection boxes, and the results are shown in Table II. Here,\nSE–ST represents SE block and STE block in our model,\nand SpaChan represents multihead attention enhancement in\nour model. Both the SpaChan module and SE module. Null\nrepresents that the attention module is not used. K represents\nthe motion selection box threshold.\n1) Data Enhancement Comparison: To demonstrate the\neffectiveness of the frame difference data augmentation and\ncutting method, we plotted a performance comparison graph\nTABLE III\nCOMPUTATIONAL EFFICIENCY AND COMPLEXITY OF\nEACH COMPONENT OF THE MODEL\nFig. 7. Performance comparison of different models before and after data\naugmentation. The model * represents the performance improvement of the\nmodel after data enhancement, and the part with diagonal lines on the column\nrepresents a decrease in numerical values.\nof 2DCNN, 3DCNN, and our model before and after data\naugmentation, as shown in Fig. 7.\nAccording to the survey results in Table II, the performance\nof our model has been comprehensively improved after imple-\nmenting data augmentation techniques. Moreover, in order to\nexhibit the efficacy of the frame difference data augmentation\nand cutting techniques, we illustrate the performance com-\nparison charts of 2DCNN, 3DCNN, and our model pre and\npostdata augmentation, as demonstrated in Fig. 7. It appears\nthat the 2DCNN model is incapable of effectively deriving\nbenefits from data augmentation, owing to the model’s limited\ncapacity for feature mining. Nevertheless, other models display\na substantial improvement as a result of data augmentation.\nThe implementation of data augmentation was found to sig-\nnificantly decrease the loss of 3DCNN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same",
        " for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2578
    },
    {
      "id": "Q2",
      "question": "On what basis were eligible participants included in this study (symptons, previous tests, registry, etc.)?",
      "answer": "Recruited from neurology department with informed consent.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        5,
        22
      ],
      "chunks_str": [
        "follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut",
        " for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel",
        " al., “Comparison of walking protocols and gait\nassessment systems for machine learning-based classification of Parkin-\nson’s disease,” Sensors, vol. 19, no. 24, p. 5363, Dec. 2019.\n[37] R. Li et al., “Fine-grained intoxicated gait classification using a bilinear\nCNN,” IEEE Sensors J., vol. 23, no. 23, pp. 29733–29748, Dec. 2023.\n[38] A. Mühlbäck et al., “Establishing normative data for the evaluation of\ncognitive performance in Huntington’s disease considering the impact\nof gender, age, language, and education,” J. Neurol., vol. 270, no. 10,\npp. 4903–4913, Oct. 2023.\n[39] R. Alkhatib, M. O. Diab, C. Corbier, and M. E. Badaoui, “Machine\nlearning algorithm for gait analysis and classification on early detection\nof Parkinson,” IEEE Sensors Lett., vol. 4, no. 6, pp. 1–4, Jun. 2020.\n[40] G. Vignoud et al., “Video-based automated assessment of movement\nparameters consistent with MDS-UPDRS III in Parkinson’s disease,”\nJ. Parkinson’s Disease, vol. 12, no. 7, pp. 2211–2222, 2022.\n[41] B. Jin, Y . Qu, L. Zhang, and Z. Gao, “Diagnosing Parkinson disease\nthrough facial expression recognition: Video analysis,” J. Med. Internet\nRes., vol. 22, no. 7, Jul. 2020, Art. no. e18697.\n[42] I. C. Duta, L. Liu, F. Zhu, and L. Shao, “Improved residual networks for\nimage and video recognition,” in Proc. 25th Int. Conf. Pattern Recognit.\n(ICPR), Jan. 2021, pp. 9415–9422.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[44] D. Tran, H. Wang, M. Feiszli, and L. Torresani, “Video classification\nwith channel-separated convolutional networks,” in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2019, pp. 5551–5560.\n[45] N. Singla, “Motion detection based on frame difference method,” Int.\nJ. Inf. Comput. Technol., vol. 4, no. 15, pp. 1559–1565, 2014.\n[46] A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in\nProc. 18th Int. Conf. Pattern Recognit. (ICPR), 2006, pp. 850–855.\n[47] A. Tharwat, “Classification assessment methods,” Appl. Comput. Infor-\nmat., vol. 17, no. 1, pp. 168–192, Jan. 2021.\n[48] Y . L. Chang, C. S. Chan, and P. Remagnino, “Action recognition on\ncontinuous video,” Neural Comput. Appl., vol. 33, no. 4, pp. 1233–1243,\nFeb. 2021.\n[49] C. Li, Q. Zhong, D. X"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2576
    },
    {
      "id": "Q3",
      "question": "How were participants sampled in this study: by convenience, randomly, or consecutively?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        7,
        16
      ],
      "chunks_str": [
        "follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut",
        " larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res",
        " to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2562
    },
    {
      "id": "Q4",
      "question": "How was the dataset described in this study before predictive modeling was performed?",
      "answer": "Multiframe image data with T=10, H=512, W=512.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        15,
        7,
        10
      ],
      "chunks_str": [
        "NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead",
        " larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res",
        "follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2575
    },
    {
      "id": "Q5",
      "question": "What was the proedure for splitting the dataset into training, validation, and test sets in this study?",
      "answer": "Training set and test set at a ratio of 7:3.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        7,
        5,
        11
      ],
      "chunks_str": [
        " larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res",
        " for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel",
        " s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cutting\nmethods.\na) Frame difference method: the frame difference\nmethod was used [45] for data augmentation, by adding images\nwith significant differences between consecutive frames to the\ndataset. Therefore, high-quality images could be more effec-\ntively used by the model. Specifically, the frame difference\nmethod is a method of obtaining the contour of a mov-\ning object by performing differential operations on adjacent\nframes in a video image sequence. When there is abnor-\nmal object movement in the video, there will be significant\ndifferences between frames. Subtract two frames [I (x, y, t)\nand I (x, y, t − 1)] to obtain the absolute value of the pixel\ndifference [D(x, y, t)] between the two images and determine\nwhether it is greater than the threshold to determine whether\nthere is object motion in the image sequence, as shown in the\nfollowing formula:\nD(x, y, t) = |I (x, y, t) − I (x, y, t − 1)|. (8)\nFirst, we performed median filtering, binarization, and dila-\ntion corrosion on the grayscale image to extract the contour of\nthe object. Second, a motion selection frame was performed\nby comparing the difference D(x, y, t) between the front\nand back frames, the area with a motion difference value\nis greater than the set motion threshold was selected. As\nshown in Fig. 5, using the frame difference method allows\nnot only to distinguish the characters from the background\nbut also to extract the parts of the human body that have\nundergone obvious changes. It should be noted that the motion\nselection boxes will have a large number of overlapping areas.\nIn this study, the representative motion selection boxes were\nselected according to the nonmaximum-suppression (NMS)\nalgorithm [46]. The principle of the NMS algorithm is to retain\nthe detection box with the highest confidence score for the\nsame object among multiple detection boxes while suppressing\nother boxes with lower confidence scores. Finally, according to\nthe relationship between the number of motion-selected boxes\nin the image and the set threshold k, it is determined whether\nthe frame image needs to be copied. According to the ablation\nexperimental results, we determine the value of k to be 1,\nwhich can maximize the gain of model performance.\nb) Image cutting method: After processing by using the\nframe difference method, we could obtain a large amount\nof effective image data, and then, these data were subjected\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 5. Pixel intensity changes in a screenshot of an HD patient’s video with\na 1-s interval between the front and back. The"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2577
    },
    {
      "id": "Q6",
      "question": "What preprocessing techniques on the included variables/features were applied in this study?",
      "answer": "Frame difference method and convolutional network module.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        7,
        6
      ],
      "chunks_str": [
        " for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel",
        " larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res",
        " the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel level. Third, to better\nextract the spatial, temporal, and channel features from the\nfeature maps, we employed attention modules to optimize\nthe 3DResNet module. Specifically, we designed two bottle-\nneck modules: squeeze excitation (SE) attention modules and\nspatial–temporal excitation (STE) attention modules. By rea-\nsonably cross-embedding different bottleneck modules into\nthe 3DRes-Net model, our model can learn the interdepen-\ndencies among different dimensions, promote useful features,\nand suppress redundant features. Finally, all features were\nprocessed using a fully connected layer, and a softmax layer\nwas employed to obtain the probability output for each disease\ntype.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 2. Pipeline of the proposed 3-D RAJNet, which mainly consists of an improved residual network block, the spatial–channel attention model, and a\nfeature classifier.\nA. Upstream Feature Extraction Module\n1) Feature Pre-Extraction: For the few-shot learning task\nof HD disease clinical assessment, we must perform the\ndata enhancement and feature pre-extraction to avoid model\noverfitting and improve the generalization and robustness of\nthe model. In addition, since video information often include\nirrelevant background noise, we used the frame difference\nmethod [45] to further process the data. The frame difference\nmethod can prevent the model from learning information that\nis irrelevant to the target and further enhance the effective\ndata information. A detailed data enhancement algorithm is\nintroduced in Section IV.\nIn this module, the RGB images extracted from the orig-\ninal video are first converted into a grayscale map, and the\nframe-difference method is then used for data enhancement.\nGiven input video IM ∈ RT ∗C∗H∗W , our model first cuts the\nsize of the input image by half and then stacks it in the depth\ndimension, getting a processed video IM ∈ RT ∗4C∗H/2∗W/2, to\nachieve a larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2557
    },
    {
      "id": "Q7",
      "question": "How is missing data handled in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        7,
        22
      ],
      "chunks_str": [
        "follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut",
        " larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res",
        " al., “Comparison of walking protocols and gait\nassessment systems for machine learning-based classification of Parkin-\nson’s disease,” Sensors, vol. 19, no. 24, p. 5363, Dec. 2019.\n[37] R. Li et al., “Fine-grained intoxicated gait classification using a bilinear\nCNN,” IEEE Sensors J., vol. 23, no. 23, pp. 29733–29748, Dec. 2023.\n[38] A. Mühlbäck et al., “Establishing normative data for the evaluation of\ncognitive performance in Huntington’s disease considering the impact\nof gender, age, language, and education,” J. Neurol., vol. 270, no. 10,\npp. 4903–4913, Oct. 2023.\n[39] R. Alkhatib, M. O. Diab, C. Corbier, and M. E. Badaoui, “Machine\nlearning algorithm for gait analysis and classification on early detection\nof Parkinson,” IEEE Sensors Lett., vol. 4, no. 6, pp. 1–4, Jun. 2020.\n[40] G. Vignoud et al., “Video-based automated assessment of movement\nparameters consistent with MDS-UPDRS III in Parkinson’s disease,”\nJ. Parkinson’s Disease, vol. 12, no. 7, pp. 2211–2222, 2022.\n[41] B. Jin, Y . Qu, L. Zhang, and Z. Gao, “Diagnosing Parkinson disease\nthrough facial expression recognition: Video analysis,” J. Med. Internet\nRes., vol. 22, no. 7, Jul. 2020, Art. no. e18697.\n[42] I. C. Duta, L. Liu, F. Zhu, and L. Shao, “Improved residual networks for\nimage and video recognition,” in Proc. 25th Int. Conf. Pattern Recognit.\n(ICPR), Jan. 2021, pp. 9415–9422.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[44] D. Tran, H. Wang, M. Feiszli, and L. Torresani, “Video classification\nwith channel-separated convolutional networks,” in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2019, pp. 5551–5560.\n[45] N. Singla, “Motion detection based on frame difference method,” Int.\nJ. Inf. Comput. Technol., vol. 4, no. 15, pp. 1559–1565, 2014.\n[46] A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in\nProc. 18th Int. Conf. Pattern Recognit. (ICPR), 2006, pp. 850–855.\n[47] A. Tharwat, “Classification assessment methods,” Appl. Comput. Infor-\nmat., vol. 17, no. 1, pp. 168–192, Jan. 2021.\n[48] Y . L. Chang, C. S. Chan, and P. Remagnino, “Action recognition on\ncontinuous video,” Neural Comput. Appl., vol. 33, no. 4, pp. 1233–1243,\nFeb. 2021.\n[49] C. Li, Q. Zhong, D. X"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2552
    },
    {
      "id": "Q8",
      "question": "How are outliers handled in this study?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        21,
        19,
        23
      ],
      "chunks_str": [
        ", P. van Someren, R. A. C. Roos, and\nJ. G. van Dijk, “EEG may serve as a biomarker in Huntington’s disease\nusing machine learning automatic classification,” Sci. Rep., vol. 8, no. 1,\nOct. 2018, Art. no. 16090.\n[29] A. Mengarelli, A. Tigrini, S. Fioretti, and F. Verdini, “Identification of\nneurodegenerative diseases from gait rhythm through time domain and\ntime-dependent spectral descriptors,” IEEE J. Biomed. Health Informat.,\nvol. 26, no. 12, pp. 5974–5982, Dec. 2022.\n[30] J. Klucken et al., “Unbiased and mobile gait analysis detects motor\nimpairment in Parkinson’s disease,” PLoS ONE, vol. 8, no. 2, Feb. 2013,\nArt. no. e56956.\n[31] B. A. MacWilliams, K. L. Carroll, A. K. Stotts, L. M. Kerr, and\nM. H. Schwartz, “Discrimination between hereditary spastic paraplegia\nand cerebral palsy based on gait analysis data: A machine learning\napproach,” Gait Posture, vol. 98, pp. 34–38, Oct. 2022.\n[32] L. van de Venis, B. P. C. van de Warrenburg, V . Weerdesteyn,\nB. J. H. van Lith, A. C. H. Geurts, and J. Nonnekes, “Improving gait\nadaptability in patients with hereditary spastic paraplegia (Move-HSP):\nStudy protocol for a randomized controlled trial,” Trials, vol. 22, no. 1,\npp. 1–10, Dec. 2021.\n[33] Y . Hutabarat, D. Owaki, and M. Hayashibe, “Recent advances in\nquantitative gait analysis using wearable sensors: A review,” IEEE\nSensors J., vol. 21, no. 23, pp. 26470–26487, Dec. 2021.\n[34] S. Moon et al., “Classification of Parkinson’s disease and essential\ntremor based on balance and gait characteristics from wearable motion\nsensors via machine learning techniques: A data-driven approach,”\nJ. NeuroEng. Rehabil., vol. 17, no. 1, pp. 1–8, Dec. 2020.\n[35] C. G. Goetz et al., “Movement disorder society-sponsored revision\nof the unified Parkinson’s disease rating scale (MDS-UPDRS): Scale\npresentation and clinimetric testing results,” Movement Disorders,\nOff. J. Movement Disorder Soc., vol. 23, no. 15, pp. 2129–2170,\n2008.\n[36] R. Z. U. Rehman et al., “Comparison of walking protocols and gait\nassessment systems for machine learning-based classification of Parkin-\nson’s disease,” Sensors, vol. 19, no. 24, p. 5363, Dec. 2019.\n[37] R. Li et al., “Fine-grained intoxicated gait classification using a bilinear\nCNN,” IEEE Sensors J., vol. 23, no. 23, pp. 29733–29748, Dec. 2023.\n[38] A. Mühlbäck et al., “Establishing normative data for the evaluation of\ncognitive performance in Huntington’s disease considering the impact\nof gender, age, language, and education,” J. Neurol., vol. 270, no. 10",
        " Eskofier, J. Klucken, and E. Nöth, “Multimodal assessment of\nParkinson’s disease: A deep learning approach,” IEEE J. Biomed. Health\nInformat., vol. 23, no. 4, pp. 1618–1630, Jul. 2019.\n[13] F. D. Acosta-Escalante, E. Beltrán-Naturi, M. C. Boll,\nJ. A. Hernández-Nolasco, and P. Pancardo García, “Meta-classifiers\nin Huntington’s disease patients classification, using iPhone’s\nmovement sensors placed at the ankles,” IEEE Access, vol. 6,\npp. 30942–30957, 2018.\n[14] A. Mannini, D. Trojaniello, A. Cereatti, and A. Sabatini, “A machine\nlearning framework for gait classification using inertial sensors: Applica-\ntion to elderly, post-stroke and Huntington’s disease patients,” Sensors,\nvol. 16, no. 1, p. 134, Jan. 2016.\n[15] Y .-W. Ma, J.-L. Chen, Y .-J. Chen, and Y .-H. Lai, “Explainable deep\nlearning architecture for early diagnosis of Parkinson’s disease,” Soft\nComput., vol. 27, no. 5, pp. 2729–2738, Mar. 2023.\n[16] A. Shcherbak, E. Kovalenko, and A. Somov, “Detection and classifica-\ntion of early stages of Parkinson’s disease through wearable sensors and\nmachine learning,” IEEE Trans. Instrum. Meas., vol. 72, pp. 1–9, 2023.\n[17] A. Talitckii et al., “Comparative study of wearable sensors, video, and\nhandwriting to detect Parkinson’s disease,” IEEE Trans. Instrum. Meas.,\nvol. 71, pp. 1–10, 2022.\n[18] A. Lauraitis, R. Maskeliunas, R. Damaševicius, D. Polap, and\nM. Wozniak, “A smartphone application for automated decision support\nin cognitive task based evaluation of central nervous system motor disor-\nders,” IEEE J. Biomed. Health Informat., vol. 23, no. 5, pp. 1865–1876,\nSep. 2019.\n[19] R. Riad et al., “Predicting clinical scores in Huntington’s disease:\nA lightweight speech test,” J. Neurol., vol. 269, no. 9, pp. 5008–5021,\n2022.\n[20] Z. Chang et al., “Accurate detection of cerebellar smooth pursuit eye\nmovement abnormalities via mobile phone video and machine learning,”\nSci. Rep., vol. 10, no. 1, Oct. 2020, Art. no. 18641.\n[21] W. Huang, W. Xu, R. Wan, P. Zhang, Y . Zha, and M. Pang, “Auto\ndiagnosis of Parkinson’s disease via a deep learning model based on\nmixed emotional facial expressions,” IEEE J. Biomed. Health Informat.,\nvol. 28, no. 5, pp. 2547–2557, May 2024.\n[22] Z. Zha, H. Tang, Y . Sun, and J. Tang, “Boosting few-shot fine-grained\nrecognition with background suppression and foreground alignment,”\nIEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 8, pp. 3947–3961,\nAug.",
        " “Efficient non-maximum suppression,” in\nProc. 18th Int. Conf. Pattern Recognit. (ICPR), 2006, pp. 850–855.\n[47] A. Tharwat, “Classification assessment methods,” Appl. Comput. Infor-\nmat., vol. 17, no. 1, pp. 168–192, Jan. 2021.\n[48] Y . L. Chang, C. S. Chan, and P. Remagnino, “Action recognition on\ncontinuous video,” Neural Comput. Appl., vol. 33, no. 4, pp. 1233–1243,\nFeb. 2021.\n[49] C. Li, Q. Zhong, D. Xie, and S. Pu, “Collaborative spatiotemporal\nfeature learning for video action recognition,” in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 7864–7873.\n[50] S. Maddury, “Automated Huntington’s disease prognosis via biomedical\nsignals and shallow machine learning,” 2023, arXiv:2302.03605.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. "
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2037
    },
    {
      "id": "Q9",
      "question": "Which prediction models were used in this study?",
      "answer": "RAJNet, 3-D ResNet-based backbone network",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        5,
        15,
        13
      ],
      "chunks_str": [
        " for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel",
        "NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead",
        "-end assessment; and c) low cost\nand convenience.\n1) Comparison With Other Models:First, the main meth-\nods for evaluating HD previously included scale analysis,\nfacial muscle group analysis model, and gait analysis for\ncomparison. Their shortcomings are very obvious. The scale\nanalysis uses the UHDRS diagnostic confidence level (DCL),\nwhich is based on the evaluation of motor signs is relatively\nlimited. The facial muscle group analysis uses methods, such\nas coordinate analysis and SVM, to analyze the amplitude\ncharacteristics of facial expressions and the shaking of facial\nsmall muscle groups. It is a non-end-to-end model with weak\ngeneralization ability. The gait analysis measures the severity\nof a patient’s illness based on high-level gait characteristics,\nrequiring patients to wear specialized pressure sensor equip-\nment. This is not only inconvenient for HD patients in remote\nareas but also has a relatively high cost. However, our model is\nan end-to-end 3-D multihead attention residual network, which\neliminates the inconvenience of wearing specialized pressure\nsensor devices to obtain corresponding data in gait analysis\nmodels. We also integrate many effective modules into our\nproposed model, such as data enhancement based on the frame\ndifference method and multihead attention module. Therefore,\nour model can achieve higher accuracy more conveniently and\nefficiently.\nSecond, as shown in Table I, the performance of our\nmodel is far better than classic models, such as 2DCNN\nand 3DCNN in terms of accuracy, recall, accuracy, and loss\nvalue performance. Given that the 2DCNN and 3DCNN model\nnetworks are relatively simple and difficult to extract effective\ninformation, they are lack of specificity analysis in diagnosing\nHD. After adding a specific attention module that captures\npatient features to our model, our model has significantly\nimproved the data utilization.\nThird, compared with the performance of the HD diagnostic\nmodels based on near-infrared spectroscopy data in Table I,\nour model has the highest accuracy, followed by LDA [50],\nQDA [50], Log.Regr. [50], and SVM [50]. Meanwhile, our\nmodel demonstrates significantly higher accuracy compared\nto the second-ranked LDA model. In addition, the acquisition\nof near-infrared spectroscopy data requires complex operations\nwhich may lead to poor repeatability. Moreover, these models\nrequire many training iterations to obtain satisfactory results,\nwhich places high demands on computer performance. Our\nmodel, by contrast, is end-to-end and can be operated by\nsimply obtaining images or videos of HD patients. Most\nimportantly, our model can achieve excellent performance with\na small amount of epoch training.\n2) ROC Curve Comparison:In order to better demonstrate\nthe high recall and accuracy performance of our model,\nwe plotted the receiver operating characteristic (ROC) curves\nof 2DCNN, 3DCNN, and our model in Fig. 6. The ROC\ncurve is of great significance in measuring the performance\nof binary classification models under different thresholds. The\nx-axis represents the false positive rate, the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVID"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2568
    },
    {
      "id": "Q10",
      "question": "What considerations were given to model selection and hyperparameter tuning in this study?",
      "answer": "Motion selection box threshold, attention modules comparison.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        16,
        15,
        8
      ],
      "chunks_str": [
        " to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA",
        "NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead",
        "  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the ResNet module, the network changes the\nlearning objective. Rather learning a complete output H(x),\nit only learns the difference between output H(x) and input\nx (the residual).\nTo extract deep information from few-shot data, we used\nan extended basic residual module as the basic block of the\nbackbone (bottleneck). As shown in (5), the basic residual\nmodule first receives the input information into two layers of\n3-D convolution Conv3\nB with a kernel size of three to process\nthe feature map Ft+1 and then adds the output value with the\noriginal input elementwise\nFt+4 = ReLU\n(\nConv3\nB\n(\nConv3\nB (Ft+3) ⊕ Ft+3\n))\n. (5)\nCompared with the basic residual module, the improved bot-\ntleneck prioritizes refined feature extraction. This is achieved\nby adding two further 3-D convolutions, each featuring a\none-unit convolutional kernel. This approach facilitates the\nextraction of more detailed information and expands the\nfeature space during the small sample learning processes.\nMoreover, we utilized the SE-block and STE-block modules\nto enhance the attention and increase the robustness of the\nbottleneck module. The intricate structure of the attention\nmodule is presented in Section III-C.\nC. Attention-Based Optimization Modules\nFor pose recognition and classification of HD patients,\nwe use attention mechanisms to focus on local features.\nAlthough traditional ResNet networks can obtain promising\nresults by stacking multilayer network structures, they rarely\nconsider how to determine the subtle differences in body\nmovements between patients and person without Huntington’s\ndisease. Their models are not suitable for image classifica-\ntion of this scene. Therefore, focusing on important feature\ninformation and preventing deep learning from overfitting are\nFig. 4. (a) SE module learns channel importance via global feature\naggregation. (b) STE module incorporates spatial–temporal information with\na channel average operation.\ncritical for HD severity assessment tasks. To achieve a better\nperformance, the spatiotemporal and channel characteristics of\nthe data should be extracted and analyzed comprehensively. In\nthis study, we designed two different ResNet bottlenecks by\ncombining attention to spatial–temporal and channel features\nand stacked them in the layers in rotation, which could\neffectively guarantee the robustness and stability of our model.\n1) SE Block: As shown in Fig. 4(a), as a kind of channel\nexcitation, the SE block can explicitly model the interde-\npendencies between feature channels. It can access global\ninformation and determine the importance of each feature\nchannel in two steps. This allows the network to promote\nuseful features and suppress irrelevant ones, enhancing its\nsensitivity to informative features.\nThe squeeze operation performs feature compression along\nthe spatial dimension, turning each 2-D feature channel into a\nreal number that somehow has a global perceptual field. The\noutput dimension matches the number of input feature chan-\nnels. It characterizes the global distribution of the response\nover the feature channels and makes the"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2560
    },
    {
      "id": "Q11",
      "question": "Is the performance of the predictive models benchmarked or compared to a baseline?",
      "answer": "Yes, compared to baseline and other models.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        14,
        5,
        16
      ],
      "chunks_str": [
        " the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVIDUAL COMPONENTS\nFig. 6 shows that the AUC areas of both 2DCNN and 3DCNN\nmodels are below 0.5, and their poor recall performance makes\nthem unsuitable for clinical assessment. Our model has an\nAUC area of up to 0.76, completely enveloping the baseline,\nindicating that our model not only outperforms other models\nbut also performs well at any judgment threshold.\nC. Ablation Study\nIn this section, we conducted ablation experiments on data\naugmentation, attention module, and the number of motion\nselection boxes, and the results are shown in Table II. Here,\nSE–ST represents SE block and STE block in our model,\nand SpaChan represents multihead attention enhancement in\nour model. Both the SpaChan module and SE module. Null\nrepresents that the attention module is not used. K represents\nthe motion selection box threshold.\n1) Data Enhancement Comparison: To demonstrate the\neffectiveness of the frame difference data augmentation and\ncutting method, we plotted a performance comparison graph\nTABLE III\nCOMPUTATIONAL EFFICIENCY AND COMPLEXITY OF\nEACH COMPONENT OF THE MODEL\nFig. 7. Performance comparison of different models before and after data\naugmentation. The model * represents the performance improvement of the\nmodel after data enhancement, and the part with diagonal lines on the column\nrepresents a decrease in numerical values.\nof 2DCNN, 3DCNN, and our model before and after data\naugmentation, as shown in Fig. 7.\nAccording to the survey results in Table II, the performance\nof our model has been comprehensively improved after imple-\nmenting data augmentation techniques. Moreover, in order to\nexhibit the efficacy of the frame difference data augmentation\nand cutting techniques, we illustrate the performance com-\nparison charts of 2DCNN, 3DCNN, and our model pre and\npostdata augmentation, as demonstrated in Fig. 7. It appears\nthat the 2DCNN model is incapable of effectively deriving\nbenefits from data augmentation, owing to the model’s limited\ncapacity for feature mining. Nevertheless, other models display\na substantial improvement as a result of data augmentation.\nThe implementation of data augmentation was found to sig-\nnificantly decrease the loss of 3DCNN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same",
        " for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel",
        " to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2573
    },
    {
      "id": "Q12",
      "question": "Which type of explainability techniques are used?",
      "answer": "SE and STE attention modules",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        9,
        6,
        17
      ],
      "chunks_str": [
        ", which could\neffectively guarantee the robustness and stability of our model.\n1) SE Block: As shown in Fig. 4(a), as a kind of channel\nexcitation, the SE block can explicitly model the interde-\npendencies between feature channels. It can access global\ninformation and determine the importance of each feature\nchannel in two steps. This allows the network to promote\nuseful features and suppress irrelevant ones, enhancing its\nsensitivity to informative features.\nThe squeeze operation performs feature compression along\nthe spatial dimension, turning each 2-D feature channel into a\nreal number that somehow has a global perceptual field. The\noutput dimension matches the number of input feature chan-\nnels. It characterizes the global distribution of the response\nover the feature channels and makes the global perceptual field\navailable to the layers close to the input. This was accom-\nplished by performing global average pooling (GPooling) on\nthe original feature map C ∗W ∗H and obtaining a feature map\nof size 1∗1∗C with a global perceptual field. The processing of\nour channel attention module can be summarized as follows:\nFt+5 = Sig(FC(Relu(FC(GPooling(Ft+4)))) ∗ Ft+4). (6)\n2) STE Block: As shown in Fig. 4(b), the STE is another\nmodule that extracts spatiotemporal features from videos\nby generating a spatiotemporal attention map. Traditional\nspatial–temporal feature extraction mainly uses 3-D convo-\nlution; however, introducing 3-D convolution directly to the\ninput significantly increases the computational effort of the\nmodel. We first use a channel average on Ft+4 to get a\nglobal channel feature F′\nt+4 ∈ RN∗T ∗1∗H∗W , and then, F′\nt+4\nis reshaped into dimensions that can be manipulated by 3-D\nconvolution, i.e. (N, 1, T, H, W ), where N represents the\nbatch size, T denotes the data time dimension, the channel\ndimension C is compressed to 1, and H ∗ W denotes the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\ndata height and width, respectively. Subsequently, we use a\nconvolutional layer to extract the attention map from F′\nt+4.\nThis attention map is multiplied by Ft+4 to obtain the final\nspatial–temporal features Ft+5.\n3) Learning Strategy: In the final part of the model,\nwe leverage multihead attention to enhance the model’s classi-\nfication ability. Subsequently, we used a fully connected layer\nand a softmax function to complete the disease probability\noutput of the model. This process can be summarized as\nfollows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were ",
        " the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel level. Third, to better\nextract the spatial, temporal, and channel features from the\nfeature maps, we employed attention modules to optimize\nthe 3DResNet module. Specifically, we designed two bottle-\nneck modules: squeeze excitation (SE) attention modules and\nspatial–temporal excitation (STE) attention modules. By rea-\nsonably cross-embedding different bottleneck modules into\nthe 3DRes-Net model, our model can learn the interdepen-\ndencies among different dimensions, promote useful features,\nand suppress redundant features. Finally, all features were\nprocessed using a fully connected layer, and a softmax layer\nwas employed to obtain the probability output for each disease\ntype.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 2. Pipeline of the proposed 3-D RAJNet, which mainly consists of an improved residual network block, the spatial–channel attention model, and a\nfeature classifier.\nA. Upstream Feature Extraction Module\n1) Feature Pre-Extraction: For the few-shot learning task\nof HD disease clinical assessment, we must perform the\ndata enhancement and feature pre-extraction to avoid model\noverfitting and improve the generalization and robustness of\nthe model. In addition, since video information often include\nirrelevant background noise, we used the frame difference\nmethod [45] to further process the data. The frame difference\nmethod can prevent the model from learning information that\nis irrelevant to the target and further enhance the effective\ndata information. A detailed data enhancement algorithm is\nintroduced in Section IV.\nIn this module, the RGB images extracted from the orig-\ninal video are first converted into a grayscale map, and the\nframe-difference method is then used for data enhancement.\nGiven input video IM ∈ RT ∗C∗H∗W , our model first cuts the\nsize of the input image by half and then stacks it in the depth\ndimension, getting a processed video IM ∈ RT ∗4C∗H/2∗W/2, to\nachieve a larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64",
        " Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA, and other models, our model has achieved better perfor-\nmance in the motor assessment of HD diseases. In particular,\nour model can obtain more reliable clinical assessment results\nthan typical video classification models. Based on this fact,\nthe superiority of the model mainly depends on the following\nreasons.\n1) The high-definition motor assessment model we\ndesigned is a small-sample learning model, which effec-\ntively utilizes data processing models such as data\naugmentation and frame difference and enriches and\nexpands effective features.\n2) The spatial–temporal attention module designed in this\narticle enables the model to filter out the redundant\nfeatures in the data well, and effectively capture the\nregions with large patient variation intensity.\n3) The SE and STE modules we use effectively enhance\nthe HD motor signs recognition ability of the base\nResNet module, improving the assessment performance\nand stability of the model.\nThe model’s current inadequacies lie in the insufficiency of\ndata for each stage of HD patients, and the model’s inability\nto determine disease stage the patient is at. In addition, the\nuse of multiple data enhancement algorithms has resulted in a\nlack of generalization in the model. In the future, we plan to\nincorporate patient voices, self-assessment scale, patient video\ndata, and patient genetic data for multimodal diagnosis to\nimprove the model’s interpretability and diagnostic reliability.\nMoreover, future work will examine further aspects of the\nmodel, including testing at different stages of the disease and\nduring the course in single individuals, Furthermore, it will\nallow motor phenotype comparison with other ethnic groups\nin a better way than the instruments used so far. Features\nrecognized in this way will also allow comparison with other\naspects of the phenotype, including other clinical features,\nas well as imaging and laboratory biomarkers. The source code\ncan be found at: https://github.com/JackAILab/RAJNet.\nACKNOWLEDGMENT\nThe authors thank the anonymous reviewers for their\nvaluable suggestions.\nREFERENCES\n[1] G. P. Bates et al., “Huntington disease,” Nature Rev. Disease Primers,\nvol. 1, no. 1, pp. 1–21, Apr. 2015.\n[2] P. Dayalu and R. L. Albin, “Huntington disease: Pathogenesis and\ntreatment,” Neurologic clinics, vol. 33, no. 1, pp. 101–114, 2015.\n[3] S. J. Tabrizi et al., “A biological classification of Huntington’s dis-\nease: The integrated staging system,” Lancet Neurol., vol. 21, no. 7,\npp. 632–644, Jul. 2022.\n[4] C. A. Ross et al., “Movement disorder society task force viewpoint:\nHuntington’s disease diagnostic categories,” Movement Disorders Clin.\nPract., vol. 6, no. 7, pp. 541–546, Sep. 2019.\n[5] H."
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2539
    },
    {
      "id": "Q13",
      "question": "Which evaluation metrics or outcome measures are used to assess the predictive models?",
      "answer": "accuracy, recall, ROC, AUC, loss value",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        13,
        3,
        4
      ],
      "chunks_str": [
        "-end assessment; and c) low cost\nand convenience.\n1) Comparison With Other Models:First, the main meth-\nods for evaluating HD previously included scale analysis,\nfacial muscle group analysis model, and gait analysis for\ncomparison. Their shortcomings are very obvious. The scale\nanalysis uses the UHDRS diagnostic confidence level (DCL),\nwhich is based on the evaluation of motor signs is relatively\nlimited. The facial muscle group analysis uses methods, such\nas coordinate analysis and SVM, to analyze the amplitude\ncharacteristics of facial expressions and the shaking of facial\nsmall muscle groups. It is a non-end-to-end model with weak\ngeneralization ability. The gait analysis measures the severity\nof a patient’s illness based on high-level gait characteristics,\nrequiring patients to wear specialized pressure sensor equip-\nment. This is not only inconvenient for HD patients in remote\nareas but also has a relatively high cost. However, our model is\nan end-to-end 3-D multihead attention residual network, which\neliminates the inconvenience of wearing specialized pressure\nsensor devices to obtain corresponding data in gait analysis\nmodels. We also integrate many effective modules into our\nproposed model, such as data enhancement based on the frame\ndifference method and multihead attention module. Therefore,\nour model can achieve higher accuracy more conveniently and\nefficiently.\nSecond, as shown in Table I, the performance of our\nmodel is far better than classic models, such as 2DCNN\nand 3DCNN in terms of accuracy, recall, accuracy, and loss\nvalue performance. Given that the 2DCNN and 3DCNN model\nnetworks are relatively simple and difficult to extract effective\ninformation, they are lack of specificity analysis in diagnosing\nHD. After adding a specific attention module that captures\npatient features to our model, our model has significantly\nimproved the data utilization.\nThird, compared with the performance of the HD diagnostic\nmodels based on near-infrared spectroscopy data in Table I,\nour model has the highest accuracy, followed by LDA [50],\nQDA [50], Log.Regr. [50], and SVM [50]. Meanwhile, our\nmodel demonstrates significantly higher accuracy compared\nto the second-ranked LDA model. In addition, the acquisition\nof near-infrared spectroscopy data requires complex operations\nwhich may lead to poor repeatability. Moreover, these models\nrequire many training iterations to obtain satisfactory results,\nwhich places high demands on computer performance. Our\nmodel, by contrast, is end-to-end and can be operated by\nsimply obtaining images or videos of HD patients. Most\nimportantly, our model can achieve excellent performance with\na small amount of epoch training.\n2) ROC Curve Comparison:In order to better demonstrate\nthe high recall and accuracy performance of our model,\nwe plotted the receiver operating characteristic (ROC) curves\nof 2DCNN, 3DCNN, and our model in Fig. 6. The ROC\ncurve is of great significance in measuring the performance\nof binary classification models under different thresholds. The\nx-axis represents the false positive rate, the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVID",
        " However,\nshortcomings are to be mentioned. Clinical scoring scales [4],\n[5] require well-trained neurologists to obtain reliable data, and\nthey are not easily adaptable for remote clinical assessment\n1) Although the reliability of biological marker-based [3]\ndiagnostic methods is good, these methods require\nexpensive instruments with inherent issues to be used\non a large scale. Use of wearable devices [7], [8] can\ngenerate data at a low cost. However, this method is a\ncontact method, which is not convenient for long-term\ntracking and analysis of patients, and the probability of\ninhomogeneous assessment is relatively high.\n2) Previously, several machine learning models [13], [17]\nhave been used in HD assessment, including the anal-\nysis models of wearable devices and voice. However,\nsome obvious defects hinder their application in clinical\npractice. For example, voice-based methods need to first\nextract voice features and then use statistics or machine\nlearning models for analysis. This two-stage method\nmay lead to unstable analysis results.\n3) HD is rare disease and the prevalence is low as 0.38 per\n100 000 person-years. Therefore, it is difficult to obtain\na large number of HD video data for traditional machine\nlearning models. Indeed, data scarcity usually results\nin unstable analysis of traditional machine learning\nmodel [22]. How to obtain accurate assessment results\nthrough few-shot data is very important to model devel-\nopment and clinical application.\nBased on the above considerations, we have developed\nan end-to-end few-shot video analysis model. The proposed\nFig. 1. Comparison of six pose examples of HD patients with normal poses,\nincluding: 1—upper bodies, 2—sitting, 3—front-facing standing, 4—side-fac-\ning standing, 5—back-facing standing, and 6—walking. To avoid revealing the\nidentity of the patients, we removed the eye region from their face images.\nmodel utilize facial details and body posture details [9], [23]\nto perform analysis, as shown in Fig. 1. Specifically, we first\ndesign a frame difference method to perform data enhancement\non some key video data. Second, we have embedded two\ntypes of effective attention modules into the 3-D convolutional\nnetwork to enhance the ability of the deep learning model\nto extract HD video features. Finally, we have employed the\nresidual structure to repeatedly stack the extracted features and\nthen use a fully connected layer to obtain the final diagnosis\nresult. Therefore, the main contributions of this article are\nsummarized as follows.\n1) We have developed an effective end-to-end high-\ndefinition diagnosis model based on posture videos.\nCompared with using gait or biomarker data, posture\nvideo-based data are more convenient and can provide\nreliable diagnostic results.\n2) Both spatial–temporal excitation attention and spatial–\nchannel attention modules have been incorporated into\nthe 3-D residual network to enhance its feature extrac-\ntion capabilities. In comparison with other machine\nlearning approaches, superior and more consistent per-\nformance is demonstrated by this architecture.\n3) We have designed a frame difference algorithm to\nimprove the video data through the data enhancement.\nThis new method can overcome issues related to small\nsample size and improve outcomes in this few-shot\nlearning task, thereby providing reliable assessment\noutcomes. Compared with other Huntington evaluation\nmodels, experiments confirm that our model can achieve\nbetter recall and accuracy.\nOverall, HD is a rare genetic disorder with a wide spectrum\nof signs and symptoms. There is an urgent need to develop\nan automatic assessment of HD in clinical practice. Several\nassessments have been developed including the analysis mod-\nels of clinical scoring scales, biological markers and wearable\ndevices, image, and voice. Although the above models can\nobtain promising results, some obvious defects hinder their",
        " demonstrated by this architecture.\n3) We have designed a frame difference algorithm to\nimprove the video data through the data enhancement.\nThis new method can overcome issues related to small\nsample size and improve outcomes in this few-shot\nlearning task, thereby providing reliable assessment\noutcomes. Compared with other Huntington evaluation\nmodels, experiments confirm that our model can achieve\nbetter recall and accuracy.\nOverall, HD is a rare genetic disorder with a wide spectrum\nof signs and symptoms. There is an urgent need to develop\nan automatic assessment of HD in clinical practice. Several\nassessments have been developed including the analysis mod-\nels of clinical scoring scales, biological markers and wearable\ndevices, image, and voice. Although the above models can\nobtain promising results, some obvious defects hinder their\napplication in clinical practice. In the present study, we have\ndeveloped an effective proficient pose video-based end-to-end\ndiagnostic model for HD. This new method can overcome\nissues related to few-shot and improve outcomes in this\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nfew-shot learning task, thereby providing dependable and\noutstanding diagnostic outcomes. The remainder of this article\nis organized as follows. Section II introduces recent work\nrelated to HD diagnosis. Section III elaborates on the proposed\nframework, followed by extensive experimental results and\nanalysis in Section IV. Last, the conclusion is drawn in\nSection V.\nII. R ELATED WORKS\nA. Biological Markers-Based Model\nBiomarkers are very common detection methods in the clin-\nical assessment of HD [24]. Currently, several new biomarkers\nhave been developed [3] to detect symptoms of ND diseases.\nFor example, blood-based assessments have been reported as\npossible biomarkers for the evaluation of HD [25]. Given that\nlipid composition and disturbances in lipoprotein metabolism\nmay be associated with the pathogenesis of HD, the plasma\nlipoprotein profile has been proposed as predictive biomark-\ners for HD progression [3]. In addition, MRI [26] and\nEEG [27], [28] have been explored biomarkers for HD. Based\non EEG features associated with disease progression in HD,\nan automatic classifier has been developed to distinguish\nhealthy controls (HCs) from HD gene carriers. Similarly, MRI-\nbased [26] imaging biomarkers have been used to determine\nthe degree of morphological changes.\nB. Wearable Device-Based Clinical Assessment Model\nUnlike biomarker-based methods, wearable device-based\nmethods are less costly and more straightforward measure-\nments for disease evaluation. Indeed, wearable or portable\nsensors have been shown to be effective in HD signs assess-\nment signs [7], [29]. Based on the data collected by sensors,\nsuch as stride frequency, stride length, and walking speed,\nseveral algorithms have been used for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2566
    },
    {
      "id": "Q14",
      "question": "What considerations were given to selected evaluation metrics or outcome measures in this study?",
      "answer": "Accuracy, recall, precision, loss, AUC.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        10,
        13,
        15
      ],
      "chunks_str": [
        "follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut",
        "-end assessment; and c) low cost\nand convenience.\n1) Comparison With Other Models:First, the main meth-\nods for evaluating HD previously included scale analysis,\nfacial muscle group analysis model, and gait analysis for\ncomparison. Their shortcomings are very obvious. The scale\nanalysis uses the UHDRS diagnostic confidence level (DCL),\nwhich is based on the evaluation of motor signs is relatively\nlimited. The facial muscle group analysis uses methods, such\nas coordinate analysis and SVM, to analyze the amplitude\ncharacteristics of facial expressions and the shaking of facial\nsmall muscle groups. It is a non-end-to-end model with weak\ngeneralization ability. The gait analysis measures the severity\nof a patient’s illness based on high-level gait characteristics,\nrequiring patients to wear specialized pressure sensor equip-\nment. This is not only inconvenient for HD patients in remote\nareas but also has a relatively high cost. However, our model is\nan end-to-end 3-D multihead attention residual network, which\neliminates the inconvenience of wearing specialized pressure\nsensor devices to obtain corresponding data in gait analysis\nmodels. We also integrate many effective modules into our\nproposed model, such as data enhancement based on the frame\ndifference method and multihead attention module. Therefore,\nour model can achieve higher accuracy more conveniently and\nefficiently.\nSecond, as shown in Table I, the performance of our\nmodel is far better than classic models, such as 2DCNN\nand 3DCNN in terms of accuracy, recall, accuracy, and loss\nvalue performance. Given that the 2DCNN and 3DCNN model\nnetworks are relatively simple and difficult to extract effective\ninformation, they are lack of specificity analysis in diagnosing\nHD. After adding a specific attention module that captures\npatient features to our model, our model has significantly\nimproved the data utilization.\nThird, compared with the performance of the HD diagnostic\nmodels based on near-infrared spectroscopy data in Table I,\nour model has the highest accuracy, followed by LDA [50],\nQDA [50], Log.Regr. [50], and SVM [50]. Meanwhile, our\nmodel demonstrates significantly higher accuracy compared\nto the second-ranked LDA model. In addition, the acquisition\nof near-infrared spectroscopy data requires complex operations\nwhich may lead to poor repeatability. Moreover, these models\nrequire many training iterations to obtain satisfactory results,\nwhich places high demands on computer performance. Our\nmodel, by contrast, is end-to-end and can be operated by\nsimply obtaining images or videos of HD patients. Most\nimportantly, our model can achieve excellent performance with\na small amount of epoch training.\n2) ROC Curve Comparison:In order to better demonstrate\nthe high recall and accuracy performance of our model,\nwe plotted the receiver operating characteristic (ROC) curves\nof 2DCNN, 3DCNN, and our model in Fig. 6. The ROC\ncurve is of great significance in measuring the performance\nof binary classification models under different thresholds. The\nx-axis represents the false positive rate, the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVID",
        "NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2573
    },
    {
      "id": "Q15",
      "question": "How were robustness, confidence or statistical significance of the results assessed in this study?",
      "answer": "Confidence intervals and performance metrics comparison.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        16,
        15,
        22
      ],
      "chunks_str": [
        " to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA",
        "NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead",
        " al., “Comparison of walking protocols and gait\nassessment systems for machine learning-based classification of Parkin-\nson’s disease,” Sensors, vol. 19, no. 24, p. 5363, Dec. 2019.\n[37] R. Li et al., “Fine-grained intoxicated gait classification using a bilinear\nCNN,” IEEE Sensors J., vol. 23, no. 23, pp. 29733–29748, Dec. 2023.\n[38] A. Mühlbäck et al., “Establishing normative data for the evaluation of\ncognitive performance in Huntington’s disease considering the impact\nof gender, age, language, and education,” J. Neurol., vol. 270, no. 10,\npp. 4903–4913, Oct. 2023.\n[39] R. Alkhatib, M. O. Diab, C. Corbier, and M. E. Badaoui, “Machine\nlearning algorithm for gait analysis and classification on early detection\nof Parkinson,” IEEE Sensors Lett., vol. 4, no. 6, pp. 1–4, Jun. 2020.\n[40] G. Vignoud et al., “Video-based automated assessment of movement\nparameters consistent with MDS-UPDRS III in Parkinson’s disease,”\nJ. Parkinson’s Disease, vol. 12, no. 7, pp. 2211–2222, 2022.\n[41] B. Jin, Y . Qu, L. Zhang, and Z. Gao, “Diagnosing Parkinson disease\nthrough facial expression recognition: Video analysis,” J. Med. Internet\nRes., vol. 22, no. 7, Jul. 2020, Art. no. e18697.\n[42] I. C. Duta, L. Liu, F. Zhu, and L. Shao, “Improved residual networks for\nimage and video recognition,” in Proc. 25th Int. Conf. Pattern Recognit.\n(ICPR), Jan. 2021, pp. 9415–9422.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[44] D. Tran, H. Wang, M. Feiszli, and L. Torresani, “Video classification\nwith channel-separated convolutional networks,” in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2019, pp. 5551–5560.\n[45] N. Singla, “Motion detection based on frame difference method,” Int.\nJ. Inf. Comput. Technol., vol. 4, no. 15, pp. 1559–1565, 2014.\n[46] A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in\nProc. 18th Int. Conf. Pattern Recognit. (ICPR), 2006, pp. 850–855.\n[47] A. Tharwat, “Classification assessment methods,” Appl. Comput. Infor-\nmat., vol. 17, no. 1, pp. 168–192, Jan. 2021.\n[48] Y . L. Chang, C. S. Chan, and P. Remagnino, “Action recognition on\ncontinuous video,” Neural Comput. Appl., vol. 33, no. 4, pp. 1233–1243,\nFeb. 2021.\n[49] C. Li, Q. Zhong, D. X"
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2569
    },
    {
      "id": "Q16",
      "question": "What limitations of the study were discussed?",
      "answer": "Unknown from this paper.",
      "raw_answer": null,
      "choices_ids": null,
      "answer_label": null,
      "chunks_id": [
        16,
        15,
        19
      ],
      "chunks_str": [
        " to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA",
        "NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead",
        " Eskofier, J. Klucken, and E. Nöth, “Multimodal assessment of\nParkinson’s disease: A deep learning approach,” IEEE J. Biomed. Health\nInformat., vol. 23, no. 4, pp. 1618–1630, Jul. 2019.\n[13] F. D. Acosta-Escalante, E. Beltrán-Naturi, M. C. Boll,\nJ. A. Hernández-Nolasco, and P. Pancardo García, “Meta-classifiers\nin Huntington’s disease patients classification, using iPhone’s\nmovement sensors placed at the ankles,” IEEE Access, vol. 6,\npp. 30942–30957, 2018.\n[14] A. Mannini, D. Trojaniello, A. Cereatti, and A. Sabatini, “A machine\nlearning framework for gait classification using inertial sensors: Applica-\ntion to elderly, post-stroke and Huntington’s disease patients,” Sensors,\nvol. 16, no. 1, p. 134, Jan. 2016.\n[15] Y .-W. Ma, J.-L. Chen, Y .-J. Chen, and Y .-H. Lai, “Explainable deep\nlearning architecture for early diagnosis of Parkinson’s disease,” Soft\nComput., vol. 27, no. 5, pp. 2729–2738, Mar. 2023.\n[16] A. Shcherbak, E. Kovalenko, and A. Somov, “Detection and classifica-\ntion of early stages of Parkinson’s disease through wearable sensors and\nmachine learning,” IEEE Trans. Instrum. Meas., vol. 72, pp. 1–9, 2023.\n[17] A. Talitckii et al., “Comparative study of wearable sensors, video, and\nhandwriting to detect Parkinson’s disease,” IEEE Trans. Instrum. Meas.,\nvol. 71, pp. 1–10, 2022.\n[18] A. Lauraitis, R. Maskeliunas, R. Damaševicius, D. Polap, and\nM. Wozniak, “A smartphone application for automated decision support\nin cognitive task based evaluation of central nervous system motor disor-\nders,” IEEE J. Biomed. Health Informat., vol. 23, no. 5, pp. 1865–1876,\nSep. 2019.\n[19] R. Riad et al., “Predicting clinical scores in Huntington’s disease:\nA lightweight speech test,” J. Neurol., vol. 269, no. 9, pp. 5008–5021,\n2022.\n[20] Z. Chang et al., “Accurate detection of cerebellar smooth pursuit eye\nmovement abnormalities via mobile phone video and machine learning,”\nSci. Rep., vol. 10, no. 1, Oct. 2020, Art. no. 18641.\n[21] W. Huang, W. Xu, R. Wan, P. Zhang, Y . Zha, and M. Pang, “Auto\ndiagnosis of Parkinson’s disease via a deep learning model based on\nmixed emotional facial expressions,” IEEE J. Biomed. Health Informat.,\nvol. 28, no. 5, pp. 2547–2557, May 2024.\n[22] Z. Zha, H. Tang, Y . Sun, and J. Tang, “Boosting few-shot fine-grained\nrecognition with background suppression and foreground alignment,”\nIEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 8, pp. 3947–3961,\nAug."
      ],
      "sent_transformer": "kamalkraj/BioSimCSE-BioLinkBERT-BASE",
      "LLM": "deepseek/deepseek-chat-v3.1:free",
      "finish_reason": "stop",
      "total_len": 2560
    }
  ]
}
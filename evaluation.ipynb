{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385f2920",
   "metadata": {},
   "source": [
    "Build an Excel workbook to simplify manual data extraction (install openpyxl first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ed8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df0db3",
   "metadata": {},
   "source": [
    "Set the path to the documents, random seed, and the number of documents to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492989e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "random.seed(42)\n",
    "num_to_copy = 50\n",
    "\n",
    "# randomly select up to 50 files\n",
    "all_papers = sorted(Path(\"papers\").glob(\"*.pdf\"))\n",
    "\n",
    "selected_papers = random.sample(all_papers, num_to_copy)\n",
    "papers = sorted([str(x).split(\"\\\\\")[-1] for x in selected_papers])\n",
    "\n",
    "dst_dir = Path(\"selected_papers\")\n",
    "dst_dir.mkdir(exist_ok=True)\n",
    "for paper in selected_papers:\n",
    "    shutil.copy2(paper, dst_dir / paper.name)\n",
    "\n",
    "print(f\"Copied {num_to_copy} papers to {dst_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc6b15",
   "metadata": {},
   "source": [
    "Build the Excel workbook to be used for manual data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from manual_extraction.utils import build_workbook\n",
    "import json\n",
    "\n",
    "\n",
    "questions = json.load(open(\"questions.json\"))\n",
    "questions = {q[\"text\"]: q[\"choices\"] for q in questions[\"questions\"]}\n",
    "\n",
    "build_workbook(\n",
    "    papers,\n",
    "    questions,\n",
    "    out_path=\"manual_extraction/dropdown_menus.xlsm\",  # keep .xlsm\n",
    "    start_row=2,\n",
    "    max_rows=200,\n",
    "    template_path=\"manual_extraction/review_dha_macro.xlsm\",  # your VBA template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be04bae",
   "metadata": {},
   "source": [
    "Calculate inter-rater agreement (Cohen's kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28042ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "model_1 = \"qwen72b\"\n",
    "model_2 = \"qwen7b\"\n",
    "\n",
    "answer_dir_1 = f\"answers/{model_1}/\"\n",
    "answer_dir_2 = f\"answers/{model_2}/\"\n",
    "\n",
    "# load data\n",
    "answers_researcher_1 = pd.read_csv(\n",
    "    \"manual_extraction/answers_researcher_1.csv\", sep=\";\"\n",
    ")\n",
    "answers_researcher_2 = pd.read_csv(\n",
    "    \"manual_extraction/answers_researcher_2.csv\", sep=\";\"\n",
    ")\n",
    "\n",
    "\n",
    "def clean_key(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "        .str.replace(\"_\", \"\", regex=False)\n",
    "        .str.replace(\" \", \"\", regex=False)\n",
    "        .str.replace(\"-\", \"\", regex=False)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.lower()\n",
    "    )\n",
    "\n",
    "\n",
    "answers_researcher_1[\"paper_key\"] = clean_key(answers_researcher_1[\"paper\"])\n",
    "answers_researcher_2[\"paper_key\"] = clean_key(answers_researcher_2[\"paper\"])\n",
    "answers_researcher_1 = answers_researcher_1.sort_values(by=\"paper_key\")\n",
    "answers_researcher_2 = answers_researcher_2.sort_values(by=\"paper_key\")\n",
    "\n",
    "# Global boolean labels collected in identical order\n",
    "y_llm1, y_llm2, y_researcher_1, y_researcher_2 = [], [], [], []\n",
    "\n",
    "\n",
    "def norm_list(x):\n",
    "    if isinstance(x, list):\n",
    "        xs = x\n",
    "    else:\n",
    "        xs = [x]\n",
    "    return [str(v).replace(\" \", \"\").lower() for v in xs if str(v).strip() != \"\"]\n",
    "\n",
    "\n",
    "def get_cell(df, mask, col):\n",
    "    val = df.loc[mask, col].iloc[0]\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return \"\"\n",
    "    return str(val)\n",
    "\n",
    "\n",
    "for paper_fname in os.listdir(answer_dir_1):\n",
    "    p1 = os.path.join(answer_dir_1, paper_fname)\n",
    "    p2 = os.path.join(answer_dir_2, paper_fname)\n",
    "    if not os.path.exists(p2):\n",
    "        print(f\"WARNING: missing {p2}; skipping file\")\n",
    "        continue\n",
    "\n",
    "    with open(p1, \"r\") as f:\n",
    "        answers_1 = json.load(f)\n",
    "    with open(p2, \"r\") as f:\n",
    "        answers_2 = json.load(f)\n",
    "\n",
    "    # filename-derived key\n",
    "    paper_clean_name = paper_fname.replace(\"_\", \"\").replace(\" \", \"\").lower()\n",
    "    prefix = paper_clean_name[:35]\n",
    "\n",
    "    mask_researcher_1 = answers_researcher_1[\"paper_key\"].str[:35].eq(prefix)\n",
    "    mask_researcher_2 = answers_researcher_2[\"paper_key\"].str[:35].eq(prefix)\n",
    "\n",
    "    if mask_researcher_1.sum() != 1 or mask_researcher_2.sum() != 1:\n",
    "        print(\"WARNING: No unique match found for paper\", paper_fname)\n",
    "        print(f\"prefix: {prefix}\")\n",
    "        print(f\"researcher 1 paper keys: {answers_researcher_1['paper_key'].str[:35]}\")\n",
    "        print(f\"researcher 2 paper keys: {answers_researcher_2['paper_key'].str[:35]}\")\n",
    "        raise Exception(f\"No unique match found for paper {paper_fname}\")\n",
    "\n",
    "    # Build a quick dict for LLM2 answers by question for robustness\n",
    "    llm2_by_q = {aq[\"question\"]: aq for aq in answers_2[\"answers\"]}\n",
    "\n",
    "    for q1 in answers_1[\"answers\"]:\n",
    "        qname = q1[\"question\"]\n",
    "        if qname not in llm2_by_q:\n",
    "            print(\n",
    "                f\"WARNING: question '{qname}' not in LLM2 for {paper_fname}; skipping\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Normalize LLM answers\n",
    "        llm_answer_1 = norm_list(q1[\"answer\"])\n",
    "        llm_answer_2 = norm_list(llm2_by_q[qname][\"answer\"])\n",
    "\n",
    "        # Fetch human answers and normalize to sets of ids\n",
    "        researcher_1_cell = get_cell(answers_researcher_1, mask_researcher_1, qname)\n",
    "        researcher_2_cell = get_cell(answers_researcher_2, mask_researcher_2, qname)\n",
    "        researcher_1_ans = [\n",
    "            s for s in researcher_1_cell.replace(\" \", \"\").lower().split(\";\") if s\n",
    "        ]\n",
    "        researcher_2_ans = [\n",
    "            s for s in researcher_2_cell.replace(\" \", \"\").lower().split(\";\") if s\n",
    "        ]\n",
    "\n",
    "        # Choices/space of labels — prefer ids from LLM1; sanity-check LLM2's choices if present\n",
    "        choices = q1.get(\"choices_ids\", [])\n",
    "        # If LLM2 has a choices list and it's different, fall back to union to stay aligned\n",
    "        choices2 = llm2_by_q[qname].get(\"choices_ids\", [])\n",
    "        if choices2 and set(map(str, choices2)) != set(map(str, choices)):\n",
    "            # robust union (normalized)\n",
    "            c1 = [str(c).replace(\" \", \"\").lower() for c in choices]\n",
    "            c2 = [str(c).replace(\" \", \"\").lower() for c in choices2]\n",
    "            choices = sorted(set(c1).union(c2))\n",
    "        else:\n",
    "            choices = [str(c).replace(\" \", \"\").lower() for c in choices]\n",
    "\n",
    "        # Append per-choice booleans in a fixed order\n",
    "        for c in choices:\n",
    "            y_llm1.append(c in llm_answer_1)\n",
    "            y_llm2.append(c in llm_answer_2)\n",
    "            y_researcher_1.append(c in researcher_1_ans)\n",
    "            y_researcher_2.append(c in researcher_2_ans)\n",
    "\n",
    "\n",
    "def show_pair(name_a, y_a, name_b, y_b):\n",
    "    k = cohen_kappa_score(y_a, y_b)\n",
    "    acc = (pd.Series(y_a) == pd.Series(y_b)).mean()\n",
    "    print(\n",
    "        f\"{name_a} vs {name_b}: kappa={k:.4f}, agreement={acc*100:.2f}% (n={len(y_a)})\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\n=== Pairwise agreement (Cohen’s κ and raw agreement) ===\")\n",
    "show_pair(f\"{model_1}\", y_llm1, \"researcher 1\", y_researcher_1)\n",
    "show_pair(f\"{model_1}\", y_llm1, \"researcher 2\", y_researcher_2)\n",
    "show_pair(f\"{model_2}\", y_llm2, \"researcher 1\", y_researcher_1)\n",
    "show_pair(f\"{model_2}\", y_llm2, \"researcher 2\", y_researcher_2)\n",
    "show_pair(f\"{model_1}\", y_llm1, f\"{model_2}\", y_llm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9827b5c",
   "metadata": {},
   "source": [
    "Calculate precision and recall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd949206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "model = \"deepseek\"\n",
    "answer_dir = f\"answers/{model}/\"\n",
    "\n",
    "# load data\n",
    "answers_researcher_1 = pd.read_csv(\n",
    "    \"manual_extraction/answers_researcher_1.csv\", sep=\";\"\n",
    ")\n",
    "answers_researcher_2 = pd.read_csv(\n",
    "    \"manual_extraction/answers_researcher_2.csv\", sep=\";\"\n",
    ")\n",
    "\n",
    "\n",
    "# normalize the paper key once\n",
    "def clean_key(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "        .str.replace(\"_\", \"\", regex=False)\n",
    "        .str.replace(\" \", \"\", regex=False)\n",
    "        .str.replace(\"-\", \"\", regex=False)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.lower()\n",
    "    )\n",
    "\n",
    "\n",
    "answers_researcher_1[\"paper_key\"] = clean_key(answers_researcher_1[\"paper\"])\n",
    "answers_researcher_2[\"paper_key\"] = clean_key(answers_researcher_2[\"paper\"])\n",
    "\n",
    "answers_researcher_1 = answers_researcher_1.sort_values(by=\"paper_key\")\n",
    "answers_researcher_2 = answers_researcher_2.sort_values(by=\"paper_key\")\n",
    "\n",
    "tp = fp = fn = 0\n",
    "\n",
    "for i, paper_fname in enumerate(os.listdir(answer_dir)):\n",
    "    with open(os.path.join(answer_dir, paper_fname), \"r\") as f:\n",
    "        answers = json.load(f)\n",
    "\n",
    "    # filename-derived key\n",
    "    paper_clean_name = paper_fname.replace(\"_\", \"\").replace(\" \", \"\").lower()\n",
    "    # use prefix match on first 35 chars (your original intent)\n",
    "    prefix = paper_clean_name[:35]\n",
    "\n",
    "    # boolean masks — NOTE: .loc with boolean mask (NOT .iloc)\n",
    "    mask_researcher_1 = answers_researcher_1[\"paper_key\"].str[:35].eq(prefix)\n",
    "    mask_researcher_2 = answers_researcher_2[\"paper_key\"].str[:35].eq(prefix)\n",
    "\n",
    "    # optionally enforce uniqueness (skip if no or multi matches)\n",
    "    if mask_researcher_1.sum() != 1 or mask_researcher_2.sum() != 1:\n",
    "        print(f\"prefix: {prefix}\")\n",
    "        print(f\"Researcher 1 paper keys: {answers_researcher_1['paper_key'].str[:35]}\")\n",
    "        print(f\"Researcher 2 paper keys: {answers_researcher_2['paper_key'].str[:35]}\")\n",
    "        raise Exception(f\"No unique match found for paper {paper_fname}\")\n",
    "\n",
    "    for q in answers[\"answers\"]:\n",
    "\n",
    "        qname = q[\"question\"]\n",
    "\n",
    "        # normalize LLM answer to list[str]\n",
    "        llm_answer = q[\"answer\"]\n",
    "        if not isinstance(llm_answer, list):\n",
    "            llm_answer = [llm_answer]\n",
    "        llm_answer = [str(x).replace(\" \", \"\").lower() for x in llm_answer]\n",
    "\n",
    "        # fetch human answers as scalars and normalize\n",
    "        def get_cell(df, mask, col):\n",
    "            # returns a single normalized string ('' if NaN)\n",
    "            val = df.loc[mask, col].iloc[0]  # Series -> scalar\n",
    "            if isinstance(val, float) and math.isnan(val):\n",
    "                return \"\"\n",
    "            return str(val)\n",
    "\n",
    "        researcher_1_cell = get_cell(answers_researcher_1, mask_researcher_1, qname)\n",
    "        researcher_2_cell = get_cell(answers_researcher_2, mask_researcher_2, qname)\n",
    "\n",
    "        researcher_1_answer = (\n",
    "            researcher_1_cell.replace(\" \", \"\").lower().split(\";\")\n",
    "            if researcher_1_cell\n",
    "            else []\n",
    "        )\n",
    "        researcher_2_answer = (\n",
    "            researcher_2_cell.replace(\" \", \"\").lower().split(\";\")\n",
    "            if researcher_2_cell\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        # scoring\n",
    "        choices = q[\"choices_ids\"]\n",
    "        for choice in choices:\n",
    "            c = str(choice).replace(\" \", \"\").lower()\n",
    "            if c in llm_answer:\n",
    "                if (c in researcher_2_answer) or (c in researcher_1_answer):\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if (c in researcher_2_answer) and (c in researcher_1_answer):\n",
    "                    fn += 1\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

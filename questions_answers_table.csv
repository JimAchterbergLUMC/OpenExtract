paper_id,question_id,question,answer,raw_answer,answer_label,choices_ids,chunks_id,chunks_str,sent_transformer,LLM,finish_reason,total_len,source_type
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q1,Which data type is used in this study?,audio,audio,audio,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[23, 5, 22]","[' Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.MenonandA.M.Kelly,“Howaccurateisweightestimationin\nthe emergency department?”Emergency Medicine Australasia,\nvol.17,no.2,pp.113–116,2005.\n[57] R. J. Moran, R. B. Reilly, P. De Chazal, and P. D. Lacy,\n“Telephony-basedvoicepathologyassessmentusingautomated\nspeech analysis,”IEEE Transactions on Biomedical Engineering,\nvol.5 3,no .3,p p .468–477 ,2006.\n[ 5 8 ]D .D .P h a m ,J .H .D o ,B .K u ,H .J .L e e ,H .K i m ,a n dJ .Y .\nKim, “Body mass index and facial cues in Sasang typology\nforyoungandelderlypersons,”\nEvidence-BasedComplementary\nandAlternativeMedicine ,vol.2011,ArticleID749209,2011.\n[ 5 9 ]H .C h a e ,I .K .L y o o ,S .J .L e ee ta l . ,“ A na l t e r n a t i v ew a yt o\nindividualized medicine: psychological and physical traits of\nSasang typology,”Journal of Alternative and Complementary\nMedicine,vol.9 ,no .4,p p .519–5 28,2003.\n[60] B.J.Lee,B.Ku,K.Park,K.H.Kim,andJ.Y .Kim,“ Anewmethod\nof diagnosing constitutional types based on vocal and facial\nfeaturesforpersonalizedmedicine,” JournalofBiomedicineand\nBiotechnology,vol.2012,ArticleID818607 ,2012.\n[61] S. W. Lee, E. S. Jang, J. Lee, and J. Y. Kim, “Current researches\nonthemethodsofdiagnosingsasangconstitution:anoverview,”\nEvidence-based Complementary and Alternative Medicine,v o l .\n6,no .1,p p .43–49 ,2009 .\n[ 6 2 ]J .H .D o ,E .S .J a n g ,B .K u ,J .S .J a n g ,H .K i m ,a n dJ .Y .K i m ,\n“Development of an integrated Sasang constitution diagnosis\nmethodusingface,bodyshape,voice,andquestionnaireinfor-\nmation,”BMCComplementaryandAlternativeMedicine ,vol.12,\narticle9,2012.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n', ' 23\nor over were labeled as overweight. Underweight patients\nwere passed over due the lack of a minimum number of\nsubjects. Finally, we divided the data set into 6 groups for\nage-andgender-specificclassification:female:20–30(females\naged20–39years),female:40–50(femalesaged40–59years),\nfemale: 60 (females aged 60 years and over), male: 20–30\n(males aged 20–39 years), male: 40–50 (males aged 40–59\nyears),andmale:60(malesaged60yearsandover).\nThe overall mean ages of the female and male subjects\nwere41.79and40.51,respectively.Themeanageandstandard\ndeviationoffemalesaged20–39yearswere28.22and ±6.326,\nand the mean BMI and standard deviation were 21.76 and\n±2.489. The rest of the groups are described inTable 2.Th e\nnumberofnormalandoverweightsubjectsinthe6groupsis\ndescribedin Table 4.\n      \nVoltage \n0.8\n0.2\n0.6\n0.4\n0\n−0.2\n−0.4\n−0.6\n−0.8\n5 1 01 52 0 25 30 35\n(s)\n(a)\nVoltage\n0.8\n0.2\n0.6\n0.4\n0\n−0.2\n−0.4\n−0.6\n−0.8\n(s)\n1.8 2 2.21.6 2.4 2.6\n(b)\nFigure 1: Sample of speech signal recording of 5 vowels and one\nsentence((a):signalsof5vowelsandonesentenceand(b):detailed\nsignalofonevoweltodemonstratethedifferencebetweennoiseand\nsignal).\n2.2. Feature Selection and Experiment Configurations.For\nfeature subset selection, we applied normalization(scale 0∼1\nv a l u e )t oa l ld a t as e t s .Th eW r a p p e r - b a s e df e a t u r es e l e c t i o n\napproach [43, 44] using machine learning of logistic regres-\nsion [30, 45] with genetic search was used to maximize the\narea under ROC curve (AUC). The selected features in each\ngroupareshownin Table 3.Allexperimentswereperformed\nusing logistic regression in Weka [46], and a 10-fold cross\nvalidation was performed [47]. We used the accuracy, true\npositiverate(sensitivity,TPR),falsepositiverate(1specificity,\nFPR), precision, and F measure as performance evaluation\ncriteria [47, 48]. A large proportion of classification algo-\nrithms may not solve the class-size imbalance problem [49].\nThus, the accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the', ' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2567.0,labeled
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q2,Which type of digital health application is considered in this study?,Telehealth,Telehealth,Telehealth,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[21, 3, 22]","['en,“Theadoptionofmobileweightmanagementservices\nin a virtual community: the perspective of college students,”\nTelemedicine and e-Health,vol.16,no .4,p p .490–497 ,2010.\n[ 3 5 ]J .R .W a r r e n ,K .J .D a y ,C .P a t o ne ta l . ,“ I m p l e m e n t a t i o n s\nof health information technologies with consumers as users:\nfindingsfromasystematicreview,” HealthCareandInformatics\nReviewOnline,vol.1 4,no .3,p p .2–17 ,2010.\n[36] M. J. Mor´on, A. G´omez-Jaime, J. R. Luque, and E. Casilari,\n“DevelopmentandevaluationofaPythontelecaresystembased\nonaBluetoothBodyAreaNetwork,” EurasipJournalonWireless\nCommunications and Networking, vol. 2011, Article ID 629526,\n2011.\n[37] A. C. Norris, “Scope, Benefits and limitations of telemedicine,”\ninEssentialsofTelemedicineandTelecare ,pp.30–35,JohnWiley\n&Sons,Chichester,UK,2002.\n[38] K. H. Kim, B. Ku, S. Kang, Y. S. Kim, J. S. Jang, and J. Y. Kim,\n“Studyofavocalfeatureselectionmethodandvocalproperties\nfor discriminating four constitution types,” Evidence-Based\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n10 Evidence-BasedComplementaryandAlternativeMedicine\nComplementary and Alternative Medicine,v o l .2 0 1 2 ,A r t i c l eI D\n831543, 10pages,2012.\n[39] WHO Expert Consultation, “Appropriate body-mass index for\nAsianpopulationsanditsimplicationsforpolicyandinterven-\ntionstrategies,” TheLancet,vol.363,p p .157 –163,2004.\n[40] T .Haas,S.Svacina,J.Pav ,R.Hovorka,P .Sucharda,andJ.Sonka,\n“Risk calculation of type 2 diabetes,”Computer Methods and\nPrograms in Biomedicine,vol.41,no .3-4,p p .297 –303,1994.\n[41] K. K. Khokhar, G. Kaur, and S. Sidhu, “Prevalence of obesity\nin working premenopausal and postmenopausal women of\nJalandhar District, Punjab,”Journal of Human Ecology,v o l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S', ' minimal network or\ntelephone time and communication equipment. Because a\ngreat deal of medical information is needed for patient care\nand prognosis prediction [30, 31], telemedicine or remote\nhealthcare system facilitates the quality and quantity of data\ncollection and integration, communication between patients\nand healthcare systems, preprocessing to optimize medical\ntreatment,anddecisionsupportandmodificationofmedical\ntreatment primarily using telephones, computers, fax, and\nWCU VC (virtual community program) [32–34]. Also, the\ntechnologies have the advantages of health improvement,\npatient convenience, cost effectiveness, economy of time,\ndata accuracy and permanence, and continuous real-time\nmonitoringofchronicdisease[ 35–37].\nOur contributions in this study are as follows: we first\npropose a method for classifying the normal weight or\nthe high weight using speech signals in age-and gender-\nspecific groups. Our method may apply to the development\nof advanced and automatic methods for individual BMI\nd i a g n o s i si nt e l e m e d i c i n ea n du - h e a l t h c a r ea n da s s i s ti nt h e\ndevelopmentofasimplersystemforBMImeasurement.Also,\nour suggestion that is possible to support context awareness\nmayprovidecluestoimprovetheoverallqualityofemergency\nservice via automatic support of patient BMI information\nin remote healthcare systems with limited resources. We\nfind discriminatory and meaningful features for normal and\noverweight diagnoses via a statistical analysis between BMI\nandspeechfeaturesandidentifyacompactandusefulfeature\nsubset in accordance with the age-and gender-specific anal-\nysis. The results will serve to create a better discriminatory\nfeaturesetandaccurateclassificationmodelsinthisfield.\n2. Materials and Methods\n2.1.DataPreparation\n2.1.1. Data Collection. A total of 1830 people participated\nin this study. Data was collected from subjects in several\nhos p i talsa ndtheK o r eaI n s ti t u t eo fOrien talM edicineinthe\nRepublic of Korea. Subjects with any voice-related diseases\nwere excluded from this study. Speech recording configura-\ntionswereasfollows:noresonance;roomtemperature,20\n∘C\n(±5∘C); noise intensity,<40dB; and humidity, 40%(±5%).\nPersonalcomputersandanexternalsoundcard(BlasterLive\n24-bit) to avoid noise from the personal computers were\nused for initial voice acquisition. GoldWave v5.58 was used\nto record audio data, and the voice files were saved in the\nwav format. The distance from the subjects’ mouth to the\nmicrophone(Sennheisere-835smicrophone)was4–6cm.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 3\nThe recording of the speakers’ speech was strictly con-\ntrolled by a standard operating procedure (SOP). The SOP\nwas established to capture the natural characteristics of the\nspeakers in short recordings. The speakers rested for 1 hour\nbefore actual recording to reduce suspense. An operator\ninstructedthespeakersregardingtherecordingcontent,and\nthespeakerswereaskedtopronouncewordsintheirnormal\ntonewithouttension.Theoperatorconstantlymonitoredthe\nspeakers’ speech and their distance from the', ' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2588.0,labeled
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q3,To which ICD-10 code group does the digital health application in this study pertain?,Unknown from this paper,Unknown from this paper.,,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[5, 4, 0]","[' 23\nor over were labeled as overweight. Underweight patients\nwere passed over due the lack of a minimum number of\nsubjects. Finally, we divided the data set into 6 groups for\nage-andgender-specificclassification:female:20–30(females\naged20–39years),female:40–50(femalesaged40–59years),\nfemale: 60 (females aged 60 years and over), male: 20–30\n(males aged 20–39 years), male: 40–50 (males aged 40–59\nyears),andmale:60(malesaged60yearsandover).\nThe overall mean ages of the female and male subjects\nwere41.79and40.51,respectively.Themeanageandstandard\ndeviationoffemalesaged20–39yearswere28.22and ±6.326,\nand the mean BMI and standard deviation were 21.76 and\n±2.489. The rest of the groups are described inTable 2.Th e\nnumberofnormalandoverweightsubjectsinthe6groupsis\ndescribedin Table 4.\n      \nVoltage \n0.8\n0.2\n0.6\n0.4\n0\n−0.2\n−0.4\n−0.6\n−0.8\n5 1 01 52 0 25 30 35\n(s)\n(a)\nVoltage\n0.8\n0.2\n0.6\n0.4\n0\n−0.2\n−0.4\n−0.6\n−0.8\n(s)\n1.8 2 2.21.6 2.4 2.6\n(b)\nFigure 1: Sample of speech signal recording of 5 vowels and one\nsentence((a):signalsof5vowelsandonesentenceand(b):detailed\nsignalofonevoweltodemonstratethedifferencebetweennoiseand\nsignal).\n2.2. Feature Selection and Experiment Configurations.For\nfeature subset selection, we applied normalization(scale 0∼1\nv a l u e )t oa l ld a t as e t s .Th eW r a p p e r - b a s e df e a t u r es e l e c t i o n\napproach [43, 44] using machine learning of logistic regres-\nsion [30, 45] with genetic search was used to maximize the\narea under ROC curve (AUC). The selected features in each\ngroupareshownin Table 3.Allexperimentswereperformed\nusing logistic regression in Weka [46], and a 10-fold cross\nvalidation was performed [47]. We used the accuracy, true\npositiverate(sensitivity,TPR),falsepositiverate(1specificity,\nFPR), precision, and F measure as performance evaluation\ncriteria [47, 48]. A large proportion of classification algo-\nrithms may not solve the class-size imbalance problem [49].\nThus, the accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the', ' See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 3\nThe recording of the speakers’ speech was strictly con-\ntrolled by a standard operating procedure (SOP). The SOP\nwas established to capture the natural characteristics of the\nspeakers in short recordings. The speakers rested for 1 hour\nbefore actual recording to reduce suspense. An operator\ninstructedthespeakersregardingtherecordingcontent,and\nthespeakerswereaskedtopronouncewordsintheirnormal\ntonewithouttension.Theoperatorconstantlymonitoredthe\nspeakers’ speech and their distance from the microphone\nw h i l er e c o r d i n g .W h e nt h es p e a k e r sc o u l dn o tp r o d u c ea\nuniform tone for 5 vowels, their speech was rerecorded\nuntil they achieved a certain level of tone uniformity. Each\nsentence was recorded twice, and the value of each feature\nwas obtained by averaging the values of the 2 recordings for\nmorestablefeatures.\nAll features were extracted using 5 vowels (A, E, I,\nO, U) and 1 sentence [38]. For speech feature extraction,\nwe extracted 65 features from the collected data set. The\nextracted features consisted of pitch, average ratio of pitch\nperiod, correlation coefficient between F0 and intensity\n(CORR), absolute Jitter (Jita), and Mel frequency cepstral\ncoefficients (MFCC), among others [18, 23, 27]. The specific\ncontent of the extracted features is described in Table 1,\nand sample of speech signal recording of 5 vowels and one\nsentenceisshowedin Figure1.\n2.1.2. Class Label Decision for Normal and Overweight Sta-\ntuses. Obesity and BMI research is difficult due to different\nethnic groups and different national economic statuses [7].\nAlso, BMI values differ according to physiological factors\nand environmental factors, such as residing in a city or a\nrural area. For instance, BMI values of a population in an\nAsian region tend to be lower than those of a population\nin a Western region, whereas Asians have risk factors for\ncardiovascular disease and diabetes related to obesity at\nrelatively low BMI values [9, 39]. The BMI cutoff values for\noverweight and obesity depend on several factors including\nethnicity,rural/urbanresidence,andeconomicstatus[ 7,40].\nTherefore, we decided that this study’s overweight cutoff\npoint of BMI value was≥23kg/m\n2, according to suggestions\nby the World Health Organization and references [39, 41,\n42]. We refer here to only 2 classes: the “normal” and the\n“overweight.” Subjects in the BMI who range from 18.5 to\n22.9 were labeled normal, and subjects with a BMI of 23\nor over were labeled as overweight. Underweight patients\nwere passed over due the lack of a minimum number of\nsubjects. Finally, we divided the data set into 6 groups for\nage-andgender-specificclassification:female:20–30(females\naged20–39years),female:40–50(femalesaged40–59years),\nfemale: 60 (females aged 60 years and over), male: 20–30\n(males aged 20–39 years), male: 40–50 (males aged 40–59\nyears),andmale:60(malesaged60yearsandover).\nThe overall mean ages of the female and male subjects\nwere41.79and40.51,respectively.Themean', 'Hindawi Publishing Corporation\nEvidence-BasedComplementaryandAlternativeMedicine\nVolume2013,ArticleID150265, 10pages\nhttp://dx.doi.org/10.1155/2013/150265\nResearch Article\nA Novel Method for Classifying Body Mass Index on the Basis of\nSpeech Signals for Future Clinical Applications: A Pilot Study\nBum Ju Lee, Boncho Ku, Jun-Su Jang, and Jong Yeol Kim\nMedical Research Division, Korea Institute of Oriental Medicine, 1672 Yuseongdae-ro, Yuseong-gu,\nDeajeon305-811,RepublicofKorea\nCorrespondenceshouldbeaddressedtoJongYeolKim;ssmed@kiom.re.kr\nReceived5November2012;Revised11January2013;Accepted13January2013\nAcademicEditor:HyunsuBae\nCopyright © 2013 BumJuLeeetal. This is an open access article distributed under the Creative Commons Attribution License,\nwhichpermitsunrestricteduse,distribution,andreproductio ninanymedium,providedtheoriginalworkisproperlycited.\nObesity is a serious public health problem because of the risk factors for diseases and psychological problems. The focus of this\nstudyistodiagnosethepatientBMI(bodymassindex)statuswithoutweightandheightmeasurementsfortheuseinfutureclinical\napplications.Inthispaper,wefirstproposeamethodforclassifyingthenormalandtheoverweightusingonlyspeechsignals.Also,\nweperformastatisticalanalysisofthefeaturesfromspeechsig nals.Basedon1830subjects,theaccuracyandAUC(areaunderthe\nROCcurve)ofage-andgender-specificclassificationsrangedfrom60.4to73.8%andfrom0.628to0.738,respectively.Weidentified\nseveralfeaturesthatweresignificantlydifferentbetweennormalandoverweight subjects( 𝑃<0.05). Also,we foundcompact and\ndiscriminatoryfeaturesubsetsforbuildingmodelsfordiagnosing normaloroverweightindividualsthroughwrapper-basedfeature\nsubset selection. Our results showed that predicting BMI status is possible using a combination of speech features, even though\nsignificantfeaturesarerareandweakinage-andgender-specificgroupsandthattheclassificationaccuracywithfeatureselection\nwas higher than that without feature selection. Our method has the potential to be used in future clinical applications such as\nautomaticBMIdiagnosisintelemedicineorremotehealthcare.\n1. Introduction\nWorldwide, increasing numbers of people are becoming\nobese, including adults, adolescents, and children and both\nmenandwoman[ 1,2].Obesityreferstoexcessadiposetissue\ncausedbygeneticdeterminants,excessiveeating,insufficient\nphysical movement, and an inappropriate lifestyle [1, 3, 4].\nObesity and being overweight are serious public health\nproblems; obesity has a direct relationship with physical\nhealth and psychological health and is a potential risk\nfactor for many diseases, including cardiovascular diseases,\nstroke, ischemic heart disease, diabetes, and cancer [2, 5–\n8]. Therefore, it is important to recognize when patients are\noverweightorobese,andmanystudieshavebeenperformed\nabout the relationship of obesity, as determined by body\nmassindex(BMI),anddisease[ 4,6,7,9–11].BMI,proposed\nby Lambert Adolphe Jacques Quetelet, is a measurement\ncriterion presenting the relationship between body weight\nand height [3]a n dac o m m o n l yu s e dp u b l i ch e a l t hm e t h o d\nfor classifying underweight, normal, overweight, and obese\npatients.\nOn the other hand, research on the association of body\nshape (weight, height),']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2707.0,labeled
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q4,Does the predictive model perform regression or classification?,Unknown from this paper,Unknown from this paper,,"['Regression', 'Classification']","[6, 17, 22]","[' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', ' of broad range, rich\ndata,andamoreaccurateclassificationmodel.\nAcknowledgment\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) Grant funded by the Korea government\n(MEST)(20120009001,2006-2005173).\nReferences\n[1] T. J. Parsons, O. Manor, and C. Power, “Physical activity\nand change in body mass index from adolescence to mid-\nadulthood in the 1958 British cohort,”International Journal of\nEpidemiology,vol.35,no .1,p p .197 –204,2006.\n[2] H. Hirose, T. Takayama, S. Hozawa, T. Hibi, and I. Saito, “Pre-\ndiction of metabolic syndrome using artificial neural network\nsystembasedonclinicaldataincludinginsulinresistanceindex\nand serum adiponectin,”Computers in Biology and Medicine,\nvol.41,pp.1051–1056,2011.\n[3] D. Gallagher, M. Visser, D. Sep´u l v e d a ,R.N .P i e r so n ,T .H a rri s ,\nand S. B. Heymsfieid, “How useful is body mass index for\ncomparisonofbodyfatnessacrossage,sex,andethnicgroups?”\nAmerican Journal of Epidemiology,v o l .1 4 3 ,n o .3 ,p p .2 2 8 – 2 3 9 ,\n1996.\n[ 4 ]E .A n u u r a d ,K .S h i w a k u ,A .N o g ie ta l . ,“Th en e wB M Ic r i t e r i a\nf o rA s i a n sb yt h er e g i o n a lo ffi c ef o rt h ew e s t e r np a c i fi cr e g i o n\nof WHO are suitable for screening of overweight to prevent\nmetabolic syndrome in elder Japanese workers,”Journal of\nOccupational Health,vol.45,no.6,pp.335–343,2003.\n[5]L.L.Y a n,M.L.Da vigl us,K.Li uetal.,“ BMIa ndheal th-r ela ted\nqualityoflifeinadults65yearsandolder,” ObesityResearch,vol.\n12,no .1,p p .69–7 6,2004.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 9\n[6] Asia Pacific Cohort Studies Collaboration, “Body mass index\nand cardiovascular disease in the Asia-Pacific Region: an\noverviewof33cohortsinvolving310000participants,” Interna-\ntional Journal of Epidemiology,vol.3 3,p p .7 51 –7 58,2004.\n[ 7 ]C .M .L e e ,S .C o l a g i u r i ,M .E z z a t i ,a n dM .W o o d w a r d ,“ Th e\nburden of cardiovascular disease associated with high body\nmass index in the Asia-Pacific region,”Obesity Reviews,v o l .12 ,\npp.e454–e459,2011.\n[8] L. Li, A. P. De Moira, and C. Power', ' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,,,,labeled
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q1,Which data type is used in this study?,images,images,images,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[6, 2, 7]","['.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', ' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', '3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2577.0,labeled
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q2,Which type of digital health application is considered in this study?,Unknown from this paper,Unknown from this paper.,,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[2, 12, 5]","[' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', 'iﬁcant impact on the anal-\nysis of artiﬁcial intelligence. Therefore, the future work will\ngive more priority to the improvement of the hardware design\nand the efﬁciency of image acquisition device, and further work\nwill focus on improving algorithm efﬁciency and recognition\nrate, reducing false alarm rate, and setting up a larger image\ndata set for dental diseases.\nV. C ONCLUSION\nThis paper proposes an iHome smart dental Health-IoT sys-\ntem based on intelligent hardware, deep learning and mobile\nterminal, aiming to regulate as well as optimize the accessibil-\nity of dental treatment and provide home-based dental health\ncare service more efciently. The trained model was used to\nrealize the detection and classication of dental diseases, and\napplication software (Apps) on mobile terminal was designed\nfor client-side and dentist-side. The software platform with the\nfunctions including pre-examination of dental disease, consulta-\ntion, appointment, and evaluation, etc, made the service docking\nbetween the patient and the dentist resources a reality. The AI\nalgorithm achieved more than 90% recognition rate for seven\ndental diseases., which has greatly improved the patient rate and\nthe resource utilization rate of the dental clinic through a one-\nmonth systematic testing in 10 private dental clinics, showing\nhigh reliability in practical application.\nREFERENCES\n[1] Z. Qian, “Opportunities abound for dental care in China,” China\nBrieﬁng, Feb. 2015. [Online]. Available: http://www.china-brieﬁng.com/\nnews/2015/02/27/opportunities-abound-dental-care-china.html\n[2] P . J. Pussinen, P . Jousilahti, G. Alfthan, T. Palosuo, S. Asikainen, and V .\nSalomaa, “Antibodies to periodontal pathogens are associated with coro-\nnary heart disease,” Arteriosclerosis Thrombosis V ascular Biol. , vol. 23,\nno. 7, 2003, pp. 1250–1254.\n[3] H. Jansson et al. , “Type 2 diabetes and risk for periodontal disease: A\nrole for dental health awareness,” J. Clinical Periodontol. , vol. 33, no. 6,\npp. 408–414, 2006.\n[4] N. W. Johnson, “The mouth in HIV/AIDS: Markers of disease status and\nmanagement challenges for the dental profession,” Australian Dental J. ,\nvol. 55, no. s1, pp. 85–102, 2010.\n[5] L. Atzori, A. Iera, and G. Morabito, The Internet of Things: A Survey .\nAmsterdam, The Netherlands: Elsevier, 2010.\n[6] D. Metcalf, S. T. Milliard, M. Gomez, and M. Schwartz, “Wearables and\nthe internet of things for health: Wearable, interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L.', 'namely, client, service platform and doctor at the application\nlogic level. Users capture pictures using intelligent dental im-\nage data acquisition device, and then upload the pictures to the\nservice platform through the mobile terminal, and these den-\ntal image data will be analyzed using AI methods. The system\nwill advise the user to seek for medical care if problems have\nbeen found in teeth after the conﬁrmation of the service by the\nuser. Nearby dental clinics or doctors based on their geographic\nlocation will be recommended at the service platform, thereby\nmaking it possible for users to consult with doctors to make\nappointments and complete ofﬂine medical procedures if the\nappointment is successful.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 901\nFig. 4. The intelligent dental image acquisition device hardware\ndiagram.\nC. Intelligent Hardware\nTraditional dental image collection methods can bring about\nthe inconvenience as follows:\nr Dentists are required to acquire knowledge on more shoot-\ning skills;\nr It is necessary to expand the auxiliary equipment such as\nthe oral expander, along with high cost of the SLR camera.\nr Only partial dental images can be obtained\nDue to the peculiarity of oral structure, the general image\nacquisition equipment (e.g. smart cell-phone) cant meet the im-\nage data collection requirement of 6 surfaces of teeth, thereby\nhelping explain that an intelligent dental image acquisition de-\nvice is designed to photograph dental images. As shown in the\nFig. 4 , the intelligent dental image acquisition device is mainly\ncomposed of an image module and the control board, and the\nmicro exible packaging technology is applied to manufacture\nthe image module. On the exible printed circuit board, it is in-\ntegrated with a 1-megapixel CMOS Sensor, the supplementary\nlighting LEDs, and a macro lens to form a module.\nIncluding processing chip, Wi-Fi module, gyroscope, mem-\nory, keys, LEDs, power management and wireless charger, the\nmain board has functions such as video encoding, interface con-\ntrol, light control and task processing. Additionally, the device\ncan be communicated with the mobile terminal through the Wi-\nFi module. When photographing the teeth, the left-side images\nand right-side images are ﬂipped, with the use of a gyroscope\nsensor in the hardware design [16]. Moreover, In the process\nof shooting, the processor can sense the change signal, and then\ncorrect the image position with the software automatically.\nIII. I NTELLIGENT DENT AL DIAGNOSIS\nA. System Flow and Data Acquisition\nAI detection of dental diseases is the most important part of\nthe intelligent dental Health-IoT platform, and the main steps\nFig. 5. Dental image analysis system ﬂow.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2583.0,labeled
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q3,To which ICD-10 code group does the digital health application in this study pertain?,Unknown from this paper,Unknown from this paper.,,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[2, 4, 10]","[' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', ' seek\nfor low-price as well as time-saving medical treatment.\nr Group purchase (E-MALL) service: It can classify the cus-\ntomer needs as well as suitable product services and pro-\nmote different group purchase & product services based\non AI diagnosis results.\nr Large data customization service: It can classify the cus-\ntomer needs and provide the customized or personalized\nproducts as well as the services according to AI diagnosis\nresults.\nMainly consisting of dental image data acquisition devices\nand the mobile terminal, local computing, and processing units,\nwireless transmitting modules and display terminal, the dental\nimage data acquisition layer serves as the basis of the entire\nplatform. The dental image data is uploaded to the smart dental\nservice layer via the network (Wi-Fi, 3G/4G). A display terminal\nis needed in this layer to meet the daily needs of the users in\ndental health self-inspection service, thereby making it possible\nfor mobile terminal to satisfy this requirement very well. With\nthis three-layer iHome dental Health-IoT system, the interaction\nbetween dentists and home-stay patients can easily take place\non demand or on a regular basis.\nB. Apps on Mobile Terminal\nThe intelligent dental Health-IoT platform facilitates the in-\nteraction between the patients and the dentists mainly through\nthe mobile terminal. As shown in Fig. 2 , the application soft-\nware system is designed into four layers, namely, the front-end\nlayer, the cloud-end application programming interface (API)\nlayer, the server & service layer and the database layer.\nComposed of Android, iOS, H5 and web programs, The front-\nend directly interacts with users and the server through the cloud\nservice interface. Additionally, it processes data interaction with\nthe cloud-end APIs.\nFig. 3. The entire service ﬂow in the intelligent dental Health-IoT\nplatform.\nThe cloud-end APIs include the basic interface of logic\nsupport, location service interface, message service interface\nand payment service interface, etc., the logic modules of which\ncan satisfy all of the users requests. The API, instead of di-\nrectly calling and accessing to algorithm layer, interacts with\nthe algorithm servo middleware.\nThe server & service layer consists of the algorithm servo\nmiddleware and the Apache server. The latter, undertaking\nthe basic HTTP service, completes the HTTP service resource\nscheduling and allocation through the load balancing strategy,\nand supports performance extensions and third-party access,\nsuch as payment (e.g., Alipay/ WeChat, SMS, login authen-\ntication, notiﬁcation push), while the former, implementing\nthe connection between the cloud-end API layer and the algo-\nrithms, is responsible for resource scheduling and allocation of\nalgorithms.\nThe database layer, mainly composed of MySQL database\nand ﬁle storage services, is mainly applied to store basic data,\nimage ﬁles, and dental image diagnostic algorithm.\nFig. 3 shows the entire service ﬂow, indicating that the in-\ntelligent dental Health-IoT platform is divided into three parts,\nnamely, client, service platform and doctor at the application\nlogic level. Users capture pictures using intelligent dental im-\nage data acquisition device, and then upload the pictures to the\nservice platform through the mobile terminal, and these den-\ntal image data will be analyzed using AI methods. The system\nwill advise the user to seek for medical care if problems have\nbeen found in teeth after the conﬁrmation of the service by the\nuser. Nearby dental clinics or doctors based on their geographic\nlocation will be recommended at the service platform, thereby\nmaking it possible for users to consult with doctors to make\nappointments and complete ofﬂine medical procedures if the\nappointment is successful.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at', ' as dental caries\nbetween the teeth, smoke scale.\nC. System Integration Testing\nVia the client-side APP , the users, if logging in for the ﬁrst\ntime, have to register information, and set up the network\nconnection parameters for the device, then enter the dental\ndisease detection process normally, with the dental photo\nbeing uploaded to the algorithm server for artiﬁcial intelligence\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n904 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nSENSITIVITY AND SPECIFICITY OF 7T YPES OF DENT AL DISEASES\nFig. 9. The APPs running interface.\nanalysis, and the patient will enter the doctor’s appointment pro-\ncess if the dental disease is found in the analysis results. It is also\npossible for users to learn about dental care knowledge, the lat-\nest treatment technologies and products through the client-side\nAPP .\nVia the doctor-side APP , the dentist can enter the normal work\nmode after ﬁnishing the registration, and the dentist, if receiv-\ning the patient diagnosis request, will respond to the request,\ncoordinate treatment time with the patient. Meanwhile, dentists\ncan manage their own patients and track the effects, with Fig. 9\nshowing parts of the APPs running interface.\nD. The Diagnostic Efﬁciency\nWe carried on a systematic testing in 10 private dental clinics,\nwith a total of 25 dentists being used the dentist-side APP , and\nFig. 10. (a) Comparison chart of patient numbers. (b) The mean diag-\nnosis time comparison chart.\nthen counted the working hours of each dentist, the number of\npatients received and the mean time of diagnosis after a month.\nFig. 10 shows the statistical results, suggesting that compared\nwith the traditional way, the number of treated patients with\nthe help of smart dental service increases by 18.4%, while the\nmean diagnosis time reduces by at least 37.5%. Investigation\nand analysis show that patients capable of using the intelligent\ndental image acquisition device and AI analysis to perform pre-\nscreening at home are responsible for it. In short, compared to\nthe traditional way of checking in the dental clinic, this method\ncan save 25–30 minutes of the diagnosis time. Another interest-\ning factor is that dentists often take some time to explain and\ncommunicate with the patients after diagnosis in private dental\nclinics, while patients can acquire the relevant knowledge on\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 905\nT ABLE IV\nTHE RECOGNITION RESUL TS\ndental diseases at the iHome smart dental Health-IoT platform\nafter self-examination.\nIn this test, the recognition results of the algorithm in the\nreal environment have bene counted, with the sample distribu-\ntion and recognition results being shown in Table IV . Statis-\ntically, we found that the reason for the signiﬁcant inﬂuence\non the recognition rate was still owing to the fact that the pa-\ntients were not particularly skillful at using the intelligent den-\ntal image acquisition equipment. Compared with the results in\nTable II above, the results in Table IV show no signiﬁcant ﬂuctu-\nation, indicating that the algorithm is highly reliable']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2687.0,labeled
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[7, 8, 4]","['3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN', ' image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN ANCHOR SCALES are set. The sample\ndata is transferred from the 16-bit grayscale to 8-bit grayscale,\nthen copied to the training directory to execute the training code.\nThe data sets for each dental disease were basically balanced,\nand the method of limiting training time is adopted to avoid\noverﬁtting, with the detection results using MASK R-CNN on\ndental images being shown in the right of Fig. 7 .\nD. Inference\nRecognition rate in this study refers to the rate at which dental\ndiseases can be correctly identiﬁed in these testing image data.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 903\nT ABLE II\nTHE RECOGNITION ACCURACY OF 7T YPES OF DENT AL DISEASES\nFig. 8. A board level prototype of the intelligent dental image acquisition\ndevicet.\nThe algorithm above is followed to train the model following the\n4-step training of Faster R-CNN [22], with the Table II showing\nthe recognition accuracy of each dental disease. The table shows\nthat we can ﬁnd good results achieved by MASK R-CNN even\nunder challenging conditions, such as the reection of saliva,\ntooth gap, etc. The recognition rate of the algorithm is over 90%\nfor these seven major dental diseases, with the recognition rate\nof dental plaque as 100%, and the recognition rate of decayed\ntooth as 90.1%. It is analyzed that relatively low recognition rate\nof decayed tooth is attributed to the device using a xed focus\nlens, which will be affected by the shooting, such as smear\nand defocusing blurring. In addition, coding errors and missing\nframes can also be responsible for it. Compared with the related\nwork in [14], the accuracy of the presented transfer learning\nmethod for the classication of dental caries and periodontitis\nare all 87.5 %. This work achieves an accuracy of 90.1% and\n94.3% respectively, indicating that the classication accuracy of\ncorresponding dental diseases has been greatly improved in our\nwork.\nIV . S YSTEM INTEGRA TION AND PROTOTYPE IMPLEMENT A TION\nA. System Implementation\nThe PCB hardware of the intelligent dental image acquisition\ndevice is designed and completed, as well as the APP soft-\nware for android and apple system, with the motherboard and\nthe phone communicating via Wi-Fi. As shown in Fig. 8 ,t h e\nhardware motherboard is a circular plate with 6cm of diameter,\nwhich is suitable for being held in hands, and can be inserted\ninto the mouth exibly to collect images of the teeth. A small\npercentage of the collected images, due to the lack of', ' seek\nfor low-price as well as time-saving medical treatment.\nr Group purchase (E-MALL) service: It can classify the cus-\ntomer needs as well as suitable product services and pro-\nmote different group purchase & product services based\non AI diagnosis results.\nr Large data customization service: It can classify the cus-\ntomer needs and provide the customized or personalized\nproducts as well as the services according to AI diagnosis\nresults.\nMainly consisting of dental image data acquisition devices\nand the mobile terminal, local computing, and processing units,\nwireless transmitting modules and display terminal, the dental\nimage data acquisition layer serves as the basis of the entire\nplatform. The dental image data is uploaded to the smart dental\nservice layer via the network (Wi-Fi, 3G/4G). A display terminal\nis needed in this layer to meet the daily needs of the users in\ndental health self-inspection service, thereby making it possible\nfor mobile terminal to satisfy this requirement very well. With\nthis three-layer iHome dental Health-IoT system, the interaction\nbetween dentists and home-stay patients can easily take place\non demand or on a regular basis.\nB. Apps on Mobile Terminal\nThe intelligent dental Health-IoT platform facilitates the in-\nteraction between the patients and the dentists mainly through\nthe mobile terminal. As shown in Fig. 2 , the application soft-\nware system is designed into four layers, namely, the front-end\nlayer, the cloud-end application programming interface (API)\nlayer, the server & service layer and the database layer.\nComposed of Android, iOS, H5 and web programs, The front-\nend directly interacts with users and the server through the cloud\nservice interface. Additionally, it processes data interaction with\nthe cloud-end APIs.\nFig. 3. The entire service ﬂow in the intelligent dental Health-IoT\nplatform.\nThe cloud-end APIs include the basic interface of logic\nsupport, location service interface, message service interface\nand payment service interface, etc., the logic modules of which\ncan satisfy all of the users requests. The API, instead of di-\nrectly calling and accessing to algorithm layer, interacts with\nthe algorithm servo middleware.\nThe server & service layer consists of the algorithm servo\nmiddleware and the Apache server. The latter, undertaking\nthe basic HTTP service, completes the HTTP service resource\nscheduling and allocation through the load balancing strategy,\nand supports performance extensions and third-party access,\nsuch as payment (e.g., Alipay/ WeChat, SMS, login authen-\ntication, notiﬁcation push), while the former, implementing\nthe connection between the cloud-end API layer and the algo-\nrithms, is responsible for resource scheduling and allocation of\nalgorithms.\nThe database layer, mainly composed of MySQL database\nand ﬁle storage services, is mainly applied to store basic data,\nimage ﬁles, and dental image diagnostic algorithm.\nFig. 3 shows the entire service ﬂow, indicating that the in-\ntelligent dental Health-IoT platform is divided into three parts,\nnamely, client, service platform and doctor at the application\nlogic level. Users capture pictures using intelligent dental im-\nage data acquisition device, and then upload the pictures to the\nservice platform through the mobile terminal, and these den-\ntal image data will be analyzed using AI methods. The system\nwill advise the user to seek for medical care if problems have\nbeen found in teeth after the conﬁrmation of the service by the\nuser. Nearby dental clinics or doctors based on their geographic\nlocation will be recommended at the service platform, thereby\nmaking it possible for users to consult with doctors to make\nappointments and complete ofﬂine medical procedures if the\nappointment is successful.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2572.0,labeled
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q1,Which data type is used in this study?,time-series,time-series,time-series,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[2, 6, 3]","[' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN', ' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2572.0,labeled
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q2,Which type of digital health application is considered in this study?,Unknown from this paper,Unknown from this paper.,,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[4, 7, 2]","['Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our', ' detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI conference on human factors in computing systems. 2016.\n[6] Mishra, Varun, et al. ”Continuous detection of physiological stress with\ncommodity hardware.” ACM transactions on computing for healthcare\n1.2 (2020): 1-30.\n[7] Lee, Joong Hoon, et al. ”Stress monitoring using multimodal bio-sensing\nheadset.” Extended Abstracts of the 2020 CHI Conference on Human\nFactors in Computing Systems. 2020.\n[8] Song, J., Li, D., Ma, X., Teng, G. and Wei, J., 2019. PQR signal quality\nindexes: A method for real-time photoplethysmogram signal quality\nestimation based on noise interferences. Biomedical Signal Processing\nand Control, 47, pp.88-95.\n[9] Lea, C., Vidal, R., Reiter, A., & Hager, G. D. (2016). Temporal convolu-\ntional networks: A unified approach to action segmentation. In Computer\nVision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October\n8-10 and 15-16, 2016, Proceedings, Part III 14 (pp. 47-54). Springer\nInternational Publishing.\n[10] Van Gent, P., Farah, H., Van Nes, N., & Van Arem, B. (2019).\nHeartPy: A novel heart rate algorithm for the analysis of noisy signals.\nTransportation research part F: traffic psychology and behaviour, 66,\n368-378.\n[11] van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2019). Analysing\nnoisy driver physiology real-time using off-the-shelf sensors: Heart rate\nanalysis software from the taking the fast lane project. Journal of Open\nResearch Software, 7(1).\n[12] Jing, L., & Tian, Y . (2020). Self-supervised visual feature learning with\ndeep neural networks: A survey. IEEE transactions on pattern analysis\nand machine intelligence, 43(11), 4037-4058\n[13] Ziegler, A., & Asano, Y . M. (2022). Self-supervised learning of object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[', ' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2573.0,labeled
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q3,To which ICD-10 code group does the digital health application in this study pertain?,Unknown from this paper,Unknown from this paper.,,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[4, 7, 3]","['Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our', ' detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI conference on human factors in computing systems. 2016.\n[6] Mishra, Varun, et al. ”Continuous detection of physiological stress with\ncommodity hardware.” ACM transactions on computing for healthcare\n1.2 (2020): 1-30.\n[7] Lee, Joong Hoon, et al. ”Stress monitoring using multimodal bio-sensing\nheadset.” Extended Abstracts of the 2020 CHI Conference on Human\nFactors in Computing Systems. 2020.\n[8] Song, J., Li, D., Ma, X., Teng, G. and Wei, J., 2019. PQR signal quality\nindexes: A method for real-time photoplethysmogram signal quality\nestimation based on noise interferences. Biomedical Signal Processing\nand Control, 47, pp.88-95.\n[9] Lea, C., Vidal, R., Reiter, A., & Hager, G. D. (2016). Temporal convolu-\ntional networks: A unified approach to action segmentation. In Computer\nVision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October\n8-10 and 15-16, 2016, Proceedings, Part III 14 (pp. 47-54). Springer\nInternational Publishing.\n[10] Van Gent, P., Farah, H., Van Nes, N., & Van Arem, B. (2019).\nHeartPy: A novel heart rate algorithm for the analysis of noisy signals.\nTransportation research part F: traffic psychology and behaviour, 66,\n368-378.\n[11] van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2019). Analysing\nnoisy driver physiology real-time using off-the-shelf sensors: Heart rate\nanalysis software from the taking the fast lane project. Journal of Open\nResearch Software, 7(1).\n[12] Jing, L., & Tian, Y . (2020). Self-supervised visual feature learning with\ndeep neural networks: A survey. IEEE transactions on pattern analysis\nand machine intelligence, 43(11), 4037-4058\n[13] Ziegler, A., & Asano, Y . M. (2022). Self-supervised learning of object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2693.0,labeled
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[6, 5, 3]","[' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI', '28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our knowledge,\nHeartPy is the most prominent and trusted public library to\nprovide HRV features from PPG signals. For the prediction\nof all the HRV features, we use a 4-layer TCN to get the\ninput representation from the PPG signal that, in turn is\npassed through 2 Fully Connected (FC) layers with hidden\nsizes of 64 to get the output prediction. We train the model\nusing a learning rate of 5e-05 and a dropout value of 0.3.\nWe evaluate the performance of our model against HeartPy\nusing mean absolute error (MAE) with the estimates from the\nBiopac sensor data considered the ground truth. We report\nthese results in Table I. Among all the parameters predicted,\nwe observe that for heart rate alone, HeartPy outperforms our\nproposed framework. For the remaining HRV parameters, our\nmodel performs better than HeartPy prediction. On average,\nthe improvement in HRV parameter estimation compared to\nHeartPy is around 26%.\nTo highlight the significance of fine-tuning, we predicted\nHRV features from the earbud PPG without any pre-training\nstep. The results of this experiment are shown in Table I.\nWith the exception of HR, we observe that transfer learning\nimproves performance for all the HRV features.\nThroughout all our experiments, we consistently observed\nrelatively poorer performance in average HR estimation using\nour proposed framework. This may be related to the fact that\nheart rate estimation is fundamentally different from HRV\nparameter estimation. While HRV estimation depends on the\nspecific sequence of individual peaks, HR is more of an\naverage of the distances between peaks. The framework was\nnot optimized for this averaging task and the removal of\noutliers that would adversely affect the average. Nevertheless,\nheart rate is an important feature and integrating its estimation\nin a unified framework can be the focus of future work.\nB. Stress detection\nIn this section, we report the performance of stress clas-\nsification using HRV parameters obtained from HeartPy and\nour model, respectively. The PPG window from a subject is\nlabeled as stress if it is sampled from the task that induces\nstress, which in our case are Dot-tracking, Speech-Preparation,\nSpeech, and Exciting Music tasks (see Figure 1 for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2552.0,labeled
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q1,Which data type is used in this study?,images,images,images,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[11, 16, 14]","[' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15', '/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14', ' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,labeled
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q2,Which type of digital health application is considered in this study?,Unknown from this paper,Unknown from this paper,,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[11, 2, 9]","[' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15', ' validation studies of AI-\nbased algorithms related to dermatology in clinical settings.\nMaterial and methods\nThe study aimed to develop a convolutional neural network\n(CNN)-based algorithm trained with clinical images of 40 differ-\nent skin diseases. A user-friendly, smartphone app was also gen-\nerated, and a clinical validation study on 5014 patients was done\nby physicians in urban, rural primary care and tertiary care set-\ntings. The app’s analysis of a single image of the lesion was com-\npared to the consensus diagnosis made by two board-certiﬁed\ndermatologists. Only patients, for whom the treating board-cer-\ntiﬁed dermatologist was conﬁdent of the diagnosis and the diag-\nnosis was cross-veriﬁed by another board-certiﬁed\ndermatologist, were included. If there was no consensus then the\npatient was excluded from the study.\nGeneration of the CNN-based algorithm\nThe CNN architecture used was a modiﬁed version of Densenet-\n161 with optimized augmentation and inference pipelines.\n16\nThese optimizations were derived through rigorous experiments\nand were attuned to clinical skin images. All training and testing\nimages were annotated and segmented so a bounding box sur-\nrounded lesion of interest and irrelevant features such as rulers\nand surgical markings were discarded. The 17 718 raw images\nwere sourced from public databases (http://www.hellenicderma\ntlas.com/en and http://www.danderm.dk/atlas), after obtaining\npermission from them, as well as images from dermatologists in\nIndia. Of these, 310 images were discarded during the\npreprocessing stage due to poor resolution or multiple lesions.\nOut of the remaining 17 408 images, 1990 images belonged to\nthe non-speciﬁc category and 15 418 images were within the 40\nselected disease categories (Table 1). These images were split\ninto ﬁve equal parts (folds) and ﬁve iterations of training and\nvalidation were performed in a manner so that within each itera-\ntion, a different fold of the data was held-out for validation while\nthe remaining fourfolds were used for learning. The training\nimages were 12 350 and testing images were 3068. Since the\ndataset was imbalanced, a custom modiﬁed version of the\nweighted focal loss function was used to train the network. The\nweights for each class were calculated using the inverse sample\nsize method. Our test images were either in-distribution images\ni.e. images within 40 training classes or out-of-distribution sets\nthat we named as non-speciﬁc set.\nWe initially used Inception-v4, which was one of the most\npopular CNN models at the time. For 10 diseases, a sensitivity of\naround 85% was achieved, but for the 20-disease class model,\nthis fell to around 65%. We experimented with many parameters\nlike learning rate and choice of optimizer etc. with no signiﬁcant\nimprovements. It was felt that as the architectures became dee-\nper, there could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants', ' to calculate the true positive rate (same as\nsensitivity) and the false-positive rate (1 – speciﬁcity). ROC\ncurve was the plot of the true positive rate versus the false-\npositive rate.\nClinical validation study\nWe tested our mobile app against dermatologists’ diagnosis in\nthree different clinical settings, namely, at a tertiary care centre\nData \ncollection\nAlgorithm \nfinetuning & \nvalidation\nApp \ndevelopment\nClinical \nvalidation of \nApp\nPublic database of annotated images for \n40 skin diseases + normal skin + nonspecific\nN = 9,203\nPrivate data classified by board certified \ndermatologists\nN = 8,205\nCNN training images N =12,350\nCNN testing images N =3,068\n(1990 nonspecific images not included)\nComparison of model diagnosis against \npredetermined classification\n15,418 images used for training of model to \ngenerate smartphone app\nApp tested by physicians and compared with \ndermatologists’ diagnosis. N = 5,014 \nDetermination of app’s overall sensitivity and \ndisease-specific sensitivity, specificity, PPV, \nNPV and AUC values\nApp testing on rural primary-care patients\nN = 932\nApp testing on urban, private clinic patients\nN = 383\nApp testing on urban tertiary hospital patients\nN = 3,699\nFigure 1 This ﬂowchart depicts the different steps in data collection, algorithm generation, algorithm testing, app generation and app\nvalidation in clinical studies.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 539\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosaceaMelasma\nAcne\nVitiligo/Leucoderma\nTinea pedis\nAnogenital wartsKeratoacanthoma\nMolluscum contagiosumHidradenitis suppurativa\nHerpes zosterTinea capitis\nImpetigo and Pyodermas\nAlopecia\nMilia\nBasal cell carcinoma\nMelanoma\nMelanocytic nevi/Mole\nIchthyosis\nPityriasis r\nosea\nActin\nic keratosis\nTinea cruris  corporis or faciei\nKeloid/Hype\nrtrophic scar\nSeborrheic\n keratosis\nFixed d\nrug eruption\nLichen sclerosus et atrophicus\nCh\nicken pox\nTinea ma\nnuum\nLichen planus\nUrticaria\nPityriasis ve\nrsicolorPsoriasis\nPe\nmphigus\nCandid\niasis\nBowens disease\nViral warts\nSquamous cell c\narcino\nma\nBullous pe\nmphigoidEcze\nma\nDis\ncoid lupus erythematosus\nPercentage\nSensitivity\nSpecificity\nPPV\nNPV\nSensitivity and specificity of five fold validation data(a)\n(b)\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosacea\nKeloid/Hypertrophic scar\nFixed drug eruption\nHerpes zoster\nVitiligo/Leucoderma\nIchthyosis\nHidradenitis suppurativa\nMelasma\nKeratoacanthoma\n']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2579.0,labeled
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q3,To which ICD-10 code group does the digital health application in this study pertain?,L00-L99,L00-L99,L00-L99,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[16, 11, 2]","['/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14', ' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15', ' validation studies of AI-\nbased algorithms related to dermatology in clinical settings.\nMaterial and methods\nThe study aimed to develop a convolutional neural network\n(CNN)-based algorithm trained with clinical images of 40 differ-\nent skin diseases. A user-friendly, smartphone app was also gen-\nerated, and a clinical validation study on 5014 patients was done\nby physicians in urban, rural primary care and tertiary care set-\ntings. The app’s analysis of a single image of the lesion was com-\npared to the consensus diagnosis made by two board-certiﬁed\ndermatologists. Only patients, for whom the treating board-cer-\ntiﬁed dermatologist was conﬁdent of the diagnosis and the diag-\nnosis was cross-veriﬁed by another board-certiﬁed\ndermatologist, were included. If there was no consensus then the\npatient was excluded from the study.\nGeneration of the CNN-based algorithm\nThe CNN architecture used was a modiﬁed version of Densenet-\n161 with optimized augmentation and inference pipelines.\n16\nThese optimizations were derived through rigorous experiments\nand were attuned to clinical skin images. All training and testing\nimages were annotated and segmented so a bounding box sur-\nrounded lesion of interest and irrelevant features such as rulers\nand surgical markings were discarded. The 17 718 raw images\nwere sourced from public databases (http://www.hellenicderma\ntlas.com/en and http://www.danderm.dk/atlas), after obtaining\npermission from them, as well as images from dermatologists in\nIndia. Of these, 310 images were discarded during the\npreprocessing stage due to poor resolution or multiple lesions.\nOut of the remaining 17 408 images, 1990 images belonged to\nthe non-speciﬁc category and 15 418 images were within the 40\nselected disease categories (Table 1). These images were split\ninto ﬁve equal parts (folds) and ﬁve iterations of training and\nvalidation were performed in a manner so that within each itera-\ntion, a different fold of the data was held-out for validation while\nthe remaining fourfolds were used for learning. The training\nimages were 12 350 and testing images were 3068. Since the\ndataset was imbalanced, a custom modiﬁed version of the\nweighted focal loss function was used to train the network. The\nweights for each class were calculated using the inverse sample\nsize method. Our test images were either in-distribution images\ni.e. images within 40 training classes or out-of-distribution sets\nthat we named as non-speciﬁc set.\nWe initially used Inception-v4, which was one of the most\npopular CNN models at the time. For 10 diseases, a sensitivity of\naround 85% was achieved, but for the 20-disease class model,\nthis fell to around 65%. We experimented with many parameters\nlike learning rate and choice of optimizer etc. with no signiﬁcant\nimprovements. It was felt that as the architectures became dee-\nper, there could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2699.0,labeled
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[14, 9, 0]","[' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' to calculate the true positive rate (same as\nsensitivity) and the false-positive rate (1 – speciﬁcity). ROC\ncurve was the plot of the true positive rate versus the false-\npositive rate.\nClinical validation study\nWe tested our mobile app against dermatologists’ diagnosis in\nthree different clinical settings, namely, at a tertiary care centre\nData \ncollection\nAlgorithm \nfinetuning & \nvalidation\nApp \ndevelopment\nClinical \nvalidation of \nApp\nPublic database of annotated images for \n40 skin diseases + normal skin + nonspecific\nN = 9,203\nPrivate data classified by board certified \ndermatologists\nN = 8,205\nCNN training images N =12,350\nCNN testing images N =3,068\n(1990 nonspecific images not included)\nComparison of model diagnosis against \npredetermined classification\n15,418 images used for training of model to \ngenerate smartphone app\nApp tested by physicians and compared with \ndermatologists’ diagnosis. N = 5,014 \nDetermination of app’s overall sensitivity and \ndisease-specific sensitivity, specificity, PPV, \nNPV and AUC values\nApp testing on rural primary-care patients\nN = 932\nApp testing on urban, private clinic patients\nN = 383\nApp testing on urban tertiary hospital patients\nN = 3,699\nFigure 1 This ﬂowchart depicts the different steps in data collection, algorithm generation, algorithm testing, app generation and app\nvalidation in clinical studies.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 539\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosaceaMelasma\nAcne\nVitiligo/Leucoderma\nTinea pedis\nAnogenital wartsKeratoacanthoma\nMolluscum contagiosumHidradenitis suppurativa\nHerpes zosterTinea capitis\nImpetigo and Pyodermas\nAlopecia\nMilia\nBasal cell carcinoma\nMelanoma\nMelanocytic nevi/Mole\nIchthyosis\nPityriasis r\nosea\nActin\nic keratosis\nTinea cruris  corporis or faciei\nKeloid/Hype\nrtrophic scar\nSeborrheic\n keratosis\nFixed d\nrug eruption\nLichen sclerosus et atrophicus\nCh\nicken pox\nTinea ma\nnuum\nLichen planus\nUrticaria\nPityriasis ve\nrsicolorPsoriasis\nPe\nmphigus\nCandid\niasis\nBowens disease\nViral warts\nSquamous cell c\narcino\nma\nBullous pe\nmphigoidEcze\nma\nDis\ncoid lupus erythematosus\nPercentage\nSensitivity\nSpecificity\nPPV\nNPV\nSensitivity and specificity of five fold validation data(a)\n(b)\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosacea\nKeloid/Hypertrophic scar\nFixed drug eruption\nHerpes zoster\nVitiligo/Leucoderma\nIchthyosis\nHidradenitis suppurativa\nMelasma\nKeratoacanthoma\n', 'ORIGINAL ARTICLE\nA machine learning-based, decision support, mobile phone\napplication for diagnosis of common dermatological\ndiseases\nR. Pangti,1 J. Mathur,2 V. Chouhan,2 S. Kumar,2 L. Rajput,1 S. Shah,1 A. Gupta,3 A. Dixit,1\nD. Dholakia,4,5 S. Gupta,6 S. Gupta,1 M. George,7 V.K. Sharma,1 S. Gupta1,*\n1Department of Dermatology and Venereology, All India Institute of Medical Science, New Delhi, India\n2Nurithm Labs Private Limited, Noida, India\n3Skin Aid Clinic, Cross Point Mall, Gurugram, India\n4Genomics and Molecular Medicine Unit, Academy of Scientiﬁc and Innovative Research, New Delhi, India\n5Academy of Scientiﬁc and Innovative Research, Ghaziabad, Uttar Pradesh, India\n6Maharishi Markandeshwar Institute of Medical Sciences and Research, Mullana, Ambala, India\n7Sahrudya Hospital, Alappuzha, India\n*Correspondence: S. Gupta. E-mail: someshgupta@hotmail.com\nAbstract\nBackground The integration of machine learning algorithms in decision support tools for physicians is gaining popular-\nity. These tools can tackle the disparities in healthcare access as the technology can be implemented on smartphones.\nWe present theﬁrst, large-scale study on patients with skin of colour, in which the feasibility of a novel mobile health\napplication (mHealth app) was investigated in actual clinical workﬂows.\nObjective To develop a mHealth app to diagnose 40 common skin diseases and test it in clinical settings.\nMethods A convolutional neural network-based algorithm was trained with clinical images of 40 skin diseases. A\nsmartphone app was generated and validated on 5014 patients, attending rural and urban outpatient dermatology\ndepartments in India. The results of this mHealth app were compared against the dermatologists’ diagnoses.\nResults The machine–learning model, in an in silico validation study, demonstrated an overall top-1 accuracy of\n76.93 /C6 0.88% and mean area-under-curve of 0.95/C6 0.02 on a set of clinical images. In the clinical study, on patients\nwith skin of colour, the app achieved an overall top-1 accuracy of 75.07% (95% CI= 73.75–76.36), top-3 accuracy of\n89.62% (95% CI= 88.67–90.52) and mean area-under-curve of 0.90/C6 0.07.\nConclusion This study underscores the utility of artiﬁcial intelligence-driven smartphone applications as a point-of-\ncare, clinical decision support tool for dermatological diagnosis for a wide spectrum of skin diseases in patients of the\nskin of colour.\nReceived: 11 April 2020; revised: 22 June 2020; Accepted: 5 August 2020\nConﬂict of interests\nJyoti Mathur and Sharad Kumar are direct beneﬁciaries of any proﬁts acquired by the smartphone application developed\nat Nurithm Labs Private Limited and Vikas Chouhan is a salaried employee at Nurithm Labs Private Limited.\nFunding sources\nNone.\nIntroduction\nSkin diseases are the fourth leading cause of non-fatal disease\nburden across 188 countries in both low and high-income coun-\ntries.\n1,2 There is less than one dermatologist per 100 000 people\nin India and less than three dermatologists per 100 000 of the\npopulation in the United States and the ratios are expected to\ndecline further.\n3,4 In a survey by the American Academy of Der-\nmatology, 33%']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2538.0,labeled
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q1,Which data type is used in this study?,time-series,time-series,time-series,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[2, 10, 3]","[' experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices', 'iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3', ' such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices, including steps,\nheart rate, calories burned, and distance traveled. At the end\nof each week, they were asked to ﬁll out eight PROMIS short\nforms as a self-report assessment of their health status [4].\nA. Activity Data\nThe Fitbit Charge 2 (Fitbit Inc., San Francisco, CA, USA)\nis a popular commercially available activity tracker that can\nrecord a person’s daily activities and health indices like heart\nrate, steps, and sleep ( Table I ). Previous work has validated\nthe accuracy of heart rate monitoring speciﬁcally in the Fitbit\nCharge 2 [29]. The Fitbit hardware and its computational algo-\nrithms for calculating step counts and physical activity have been\nT ABLE I\nSUMMARY OF 17 T YPES OF FEA TURECOLLECTED FROM FITBIT PER DAY\n∗means that feature was eliminated for model input because it was highly sparse or redun-\ndant.\nvalidated using other Fitbit devices [30], [31]. The Fitbit Charge\n2 estimates activity using metabolic equivalents (METs), which\nare calculated based on heart rate and distance traveled [32].\nHeart rate during activity is also provided, however it has been\nshown to be inaccurate during activity [33]. Data quality was\nassured by verifying that there were no extreme outliers based\non subject-speciﬁc inter-quartile range [34]. We aggregated the\ndata for each day to compensate for noise and redundancy. Af-\nter data preprocessing, tracker distance was eliminated because\nit was identical to total distance, and logged activity distance\nand sedentary active distance were also deleted because of high\nsparsity. As a result, there were 14 features per day for each\npatient in our model.\nB. Patient-Reported Outcome Measures\nPatient-Reported Outcomes Measurement Information Sys-\ntems (PROMIS) questionnaires are a library of instruments\ndeveloped and validated to measure many domains of phys-\nical and mental health [15]. This analysis uses data from\neight PROMIS instruments: Global Physical Health and Global\nMental Health, which are two composite scores from the Global-\n10 short form [35]; Fatigue-Short Form 4a; Physical Function-\nShort Form 10a; Emotional Distress-Anxiety-Short Form 6a;\nDepression-Short Form 4a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2575.0,labeled
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q2,Which type of digital health application is considered in this study?,Unknown from this paper,Unknown from this paper.,,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[11, 6, 14]","[' future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart', ' transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENCE ALGORITHMS\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement over GBRT.\n†Signiﬁcant improvement over both GBRT and AdaBoost.\nclassiﬁed state,\nS (yt |yt−1 ,...,y 1 ,x t ,...,x 1 )= P (xt |yt )\n∗\n∑\nP( yt |yt−1 ) ∗S( yt−1 |yt−2 ,..., xt−1 ,... ) (1)\nwhere the weekly PRO score was treated as state yt , with obser-\nvation of features xt . The emission probability, P (xt |yt ), com-\nputed the probability of the observed feature vector xt given\nstate yt , computed from the random forest classiﬁer and P (yt ):\nP (xt |yt ) ∝ P (yt |xt )\nP (yt ) (2)\nAt the ﬁrst-time step, the transition probability distribution is\nundeﬁned, so the state probability was:\nS (y1 |x1 ) ∝ P (x1 |y1 ) P (y1 ) (3)\nFor analysis, states were binarized according to the criteria\ndeﬁned above. Because dichotomizing PRO score values loses\nsome information and precision, a regression analysis was con-\nducted between the median value of HMM stages and actual\nscores for the HMM. This method of predicting PRO scores\nwas compared against multinomial logistic regression to evalu-\nate the accuracy of predicting PRO scores over time.\nIV . R ESULTS\nTable II shows the mean AUC for binary classiﬁcation of\nPRO scores for the seven PROMIS measures using GBRT, Ad-\naBoost and RF. The highest mean AUC was 0.75 using RF for\nclassifying Physical Function, while the lowest was 0.47 using\nAdaBoost for Depression. The results indicated that RF signif-\nicantly outperformed other models in classiﬁcation of Anxiety\nand Depression (p < 0.05), and it was also signiﬁcantly better\nthan GBRT for Global Physical Health and Mental Health (p =\n0.01 and p = 0.01, respectively). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps,', 'L. Morey, “Physical activity intervention for women,” Am. J. Preventive\nMed., vol. 49, no. 3, pp. 414–418, 2015.\n[28] J. B. Wang et al. , “Wearable sensor/device (ﬁtbit one) and SMS text-\nmessaging prompts to increase physical activity in overweight and obese\nadults: A randomized controlled trial,” T elemedicine e-Health, vol. 21, no.\n10, pp. 18–23, 2014.\n[29] S. Benedetto, C. Caldato, E. Bazzan, D. C. Greenwood, V . Pensabene,\nand P . Actis, “Assessment of the ﬁtbit charge 2 for monitoring heart rate,”\nPLoS One , vol. 13, no. 2, 2018, Art. no. e0192691.\n[30] M. A. Tully, C. McBride, L. Heron, and R. F. Hunter, “The validation of\nFibit ZipTM physical activity monitor as a measure of free-living physical\nactivity,” BMC Res Notes , vol. 7, 2014, Art. no. 952.\n[31] K. M. Diaz et al. , “Fitbit: An accurate and reliable device for wireless\nphysical activity tracking,” Int. J. Cardiol. , vol. 185, pp. 138–140, 2015.\n[32] R. K. Reddy et al. , “Accuracy of wrist-worn activity monitors during com-\nmon daily physical activities and types of structured exercise: Evaluation\nstudy,” JMIR Mhealth Uhealth , vol. 6, 2018, Art. no. e10338.\n[33] E. Jo, K. Lewis, D. Directo, M. J. Kim, and B. A. Dolezal, “V alidation of\nbiofeedback wearables for photoplethysmographic heart rate tracking,” J.\nSports Sci. Med. , vol. 15, pp. 540–547, 2016.\n[34] A. Ghasemi and S. Zahediasl, “Normality tests for statistical analysis:\nA guide for non-statisticians,” Int. J. Endocrinol. Metab. , vol. 10, no. 2,\npp. 486–489, 2012.\n[35] R. D. Hays, J. B. Bjorner, D. A. Revicki, K. L. Spritzer, and D. Cella,\n“Development of physical and mental health summary scores from the\npatient-reported outcomes measurement information system (PROMIS)\nglobal items,” Qual. Life Res. , vol. 18, no. 7, pp. 873–880, 2009.\n[36] B. M. R. Spiegel et al. , “Development of the NIH patient-reported\noutcomes measurement information system (PROMIS) gastrointestinal\nsymptom scales,” Am. J. Gastroenterol. , vol. 109, no. 11, pp. 1804–1814,\n2014.\n[37] J. O. Ogutu, H. P . Piepho, and T. Schulz-Streeck, “A comparison of random\nforests, boosting and support vector machines for genomic selection,”\nBMC Proc. , vol. 5, no. Suppl. 3, pp. 3–7, 2011.\n[38] R. J. Petrella, J. J. Koval, and D. A. Cunningham, “A self-paced step test\nto predict aerobic ﬁtness in older adults in the primary']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2583.0,labeled
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q3,To which ICD-10 code group does the digital health application in this study pertain?,I00-I99,I00-I99,I00-I99,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[14, 4, 11]","['L. Morey, “Physical activity intervention for women,” Am. J. Preventive\nMed., vol. 49, no. 3, pp. 414–418, 2015.\n[28] J. B. Wang et al. , “Wearable sensor/device (ﬁtbit one) and SMS text-\nmessaging prompts to increase physical activity in overweight and obese\nadults: A randomized controlled trial,” T elemedicine e-Health, vol. 21, no.\n10, pp. 18–23, 2014.\n[29] S. Benedetto, C. Caldato, E. Bazzan, D. C. Greenwood, V . Pensabene,\nand P . Actis, “Assessment of the ﬁtbit charge 2 for monitoring heart rate,”\nPLoS One , vol. 13, no. 2, 2018, Art. no. e0192691.\n[30] M. A. Tully, C. McBride, L. Heron, and R. F. Hunter, “The validation of\nFibit ZipTM physical activity monitor as a measure of free-living physical\nactivity,” BMC Res Notes , vol. 7, 2014, Art. no. 952.\n[31] K. M. Diaz et al. , “Fitbit: An accurate and reliable device for wireless\nphysical activity tracking,” Int. J. Cardiol. , vol. 185, pp. 138–140, 2015.\n[32] R. K. Reddy et al. , “Accuracy of wrist-worn activity monitors during com-\nmon daily physical activities and types of structured exercise: Evaluation\nstudy,” JMIR Mhealth Uhealth , vol. 6, 2018, Art. no. e10338.\n[33] E. Jo, K. Lewis, D. Directo, M. J. Kim, and B. A. Dolezal, “V alidation of\nbiofeedback wearables for photoplethysmographic heart rate tracking,” J.\nSports Sci. Med. , vol. 15, pp. 540–547, 2016.\n[34] A. Ghasemi and S. Zahediasl, “Normality tests for statistical analysis:\nA guide for non-statisticians,” Int. J. Endocrinol. Metab. , vol. 10, no. 2,\npp. 486–489, 2012.\n[35] R. D. Hays, J. B. Bjorner, D. A. Revicki, K. L. Spritzer, and D. Cella,\n“Development of physical and mental health summary scores from the\npatient-reported outcomes measurement information system (PROMIS)\nglobal items,” Qual. Life Res. , vol. 18, no. 7, pp. 873–880, 2009.\n[36] B. M. R. Spiegel et al. , “Development of the NIH patient-reported\noutcomes measurement information system (PROMIS) gastrointestinal\nsymptom scales,” Am. J. Gastroenterol. , vol. 109, no. 11, pp. 1804–1814,\n2014.\n[37] J. O. Ogutu, H. P . Piepho, and T. Schulz-Streeck, “A comparison of random\nforests, boosting and support vector machines for genomic selection,”\nBMC Proc. , vol. 5, no. Suppl. 3, pp. 3–7, 2011.\n[38] R. J. Petrella, J. J. Koval, and D. A. Cunningham, “A self-paced step test\nto predict aerobic ﬁtness in older adults in the primary', 'a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n880 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nFig. 1. Distribution of normal and abnormal (moderate to severe) class\nfor each PRO measure.\nFor function (i.e., Global Physical Health, Global Mental\nHealth, and Physical Function), scores less than 40 are classi-\nﬁed as moderate to severe, meaning less functional ability than\nnormal. For this study, PRO scores were predicted in two ways:\nregression was used to predict PRO scores from patient activity\ntracker data, and classiﬁcation was used to determine whether\nsubjects’ PRO scores were above the threshold for at least mod-\nerate severity. The distributions of PRO scores are shown in\nFig. 1 . Because of a lack of moderate or severe cases for social\nisolation (<2%), this variable was eliminated for analysis in our\nmodel.\nIII. M ETHODS\nMissing data is a common concern when dealing with activity\ntracker data and can result from subjects either forgetting to wear\ntheir devices or removing them for charging. Patients were asked\nto ﬁll out eight PROMIS questionnaires at the end of each week\nfor a 12-week monitoring period. In total, 19.1 percent of weeks\nhad missing PRO data and 16.6 percent of weeks had missing\nvalues from the activity tracker in four or more days. If data was\navailable for at least four days in a week, missing values were\npermuted by using the average value of the rest of the week for\nsteps or resting heart rate. Weeks with missing survey scores, as\nwell as those without step and resting heart rate data for more\nthan three days, were removed from the analysis.\nA correlation analysis between subjects’ missing Fitbit data\nand their average Global Physical Health and Global Mental\nHealth scores shows a slight negative relationship ( −0.11 and\n−0.09, respectively) that was not statistically signiﬁcant (p =\n0.13 and p = 0.23, respectively). The correlation coefﬁcient be-\ntween number of missing PROs and the average global health\nscores are −0.17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig', ' future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2699.0,labeled
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[7, 9, 10]","['). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly', ' used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach', 'iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2527.0,labeled
1_s2.0_S2666693623000737_main,Q1,Which data type is used in this study?,time-series,time-series,time-series,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[5, 2, 1]","[' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', ' learning is used, where each observation of the data has\na class label. This must be done with ECGs, since photople-\nthysmography or derived signals are difﬁcult to interpret by a\nclinician. This so-called labeling or annotating of signals by\nphysicians is infeasible for the large amounts of (continuous)\ndata required, and therefore semi-automated\n4,5 and fully\nautomated2,3 ECG labeling systems6 have been developed.\nHowever, these still require a lot of manual labor from contin-\nuously monitored users.\nTherefore, the objective of the ME-TIME is (early) detec-\ntion and prevention of heart disease by leveraging time series\ndata from smartwatches, a cloud-based infrastructure, and\nmachine learning algorithms speciﬁcally designed to func-\ntion effectively with minimal labeling efforts.\nMethods\nStudy design and data collection\nME-TIME (registered at ClinicalTrials.gov; ID:\nNCT05802563) is designed as an observational cohort study\nconsisting of 3 data subject groups, as depicted inFigure 1.\nThe ﬁrst group consists of patients with systolic heart failure\n(HF group); the second group consists of patients with docu-\nmented atrial ﬁbrillation (AF group); and the third group,\nserving as a reference, consists of healthy volunteers. The\nrationale for creating distinct AF and HF groups comes\nfrom their unique pathophysiological characteristics. Conse-\nquently, heart rate patterns that are indicative of these dis-\neases might also be different. The HF group consists of 50\nstudy participants with systolic heart failure, deﬁned as a\nleft ventricular ejection fraction,35% without documented\natrial ﬁbrillation. The AF group consists of 50 patients with\ndocumented atrialﬁbrillation (paroxysmal, persistent, or per-\nmanent) without systolic heart failure. Ejection fractions will\nbe assessed from echocardiograms that are made within 1\nyear of inclusion, and if this is not available an echocardio-\ngram will be performed. The reference group consists of\n100 participants without any prior medical history and\nwithout medication use. Potential study subjects that meet\n2 31\nRef AF HF\n4 5\nFigure 1 Data analysis pipeline for the ME-TIME study. Included participants (image 1) are given a smartwatch (image 2), which is connected to our data\nacquisition and storage platform (image 3). The resulting data are then preprocessed (image 4) and put into the data-efﬁcient machine learning model (image\n5). AF5 atrial ﬁbrillation group; HF5 heart failure group; Ref5 reference group.\nAB C\nFigure 2 Pipeline for the study’s proposed approach.a: Segmentation of the time series of each subject (1 healthy and 2 atrialﬁbrillation) using a sliding win-\ndow. Only the label of the entire subject is available, instead of each individual window.b: The windows are inputs to an autoencoder and are compressed to a\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\n', ' Digital Health Journal 2023;4:165–172)\n© 2023\nHeart Rhythm Society. This is an open access article under the CC\nBY license (http://creativecommons.org/licenses/by/4.0/).\nIntroduction\nCardiovascular disease is one of the leading causes of mortal-\nity globally1 and cardiovascular healthcare accounts for a\nlarge portion of global healthcare costs and expenses. Early\ndetection and prevention will decrease the burden of cardio-\nvascular disease and will therefore decrease mortality,\nmorbidity, and costs. Cardiovascular monitoring using big\ndata from wearables and machine learning can drastically in-\ncrease the availability and ef ﬁciency of cardiovascular\nhealthcare globally, at the fraction of the costs of conven-\ntional medical-grade devices.\nElectrocardiogram (ECG) monitoring, such as Holters or\nimplantable loop recorders, are the gold standard for moni-\ntoring of outpatients with known or suspected arrhythmias.\nHowever, they are burdensome, can only be used for a\nlimited period of time, and are expensive. Implantable loop\nrecorders are invasive and have to be manually activated\nand analyzed in the hospital. This severely limits the use of\nthese devices for long-term home monitoring of patients,\nand they have suboptimal patient comfort. For patients with\nchronic cardiovascular diseases, such as atrial ﬁbrillation\nand heart failure, this implies frequent hospital visits and\nAddress reprint requests and correspondence:Mr Arman Naseri, Haga\nHospital, The Hague, Netherlands. E-mail address: a.naserijahfari@\nhagaziekenhuis.nl.\n2666-6936/© 2023 Heart Rhythm Society. This is an open access article under the CC BY license\n(http://creativecommons.org/licenses/by/4.0/).\nhttps://doi.org/10.1016/j.cvdhj.2023.09.001\n\nsometimes even hospital admissions (associated with higher\nmortality) that can be prevented by continuous and adequate\nhome monitoring.\nWith the widespread availability of reliable, consumer-\ngrade wearables such as smartwatches, continuous moni-\ntoring of, for example, heart rate with photoplethysmography\nand step counting with accelerometers is possible. This moni-\ntoring is easy, patient friendly, and cost effective. Combining\nthe power of large amounts of data (big data) and novel ma-\nchine learning techniques, these time series can be used to\ndetect and perhaps even predict cardiovascular disease, there-\nfore improving patient care. There are some caveats, howev-\ner, as not all wearables have the same characteristics and\nquality. Consequently, they have been used with moderate\nsuccess.\n2,3 They also provide less informative diagnostic sig-\nnals as compared to, for example, electrocardiography or\nother commonly used cardiologic diagnostic modalities.\nThe challenge but also the strength of machine learning\nmodels is that they learn by example and therefore large\namounts of data are needed for which the cardiovascular\noutcome (class label) has been determined. Typically, super-\nvised learning is used, where each observation of the data has\na class label. This must be done with ECGs, since photople-\nthysmography or derived signals are difﬁcult to interpret by a\nclinician. This so-called labeling or annotating of signals by\nphysicians is infeasible for the large amounts of (continuous)\ndata required, and therefore semi-automated\n4,5 and fully\nautomated2,3 ECG labeling systems6 have been developed.\nHowever, these still require a lot of manual labor from contin-\nuously monitored users.\nTherefore, the objective of the ME-TIME is (early) detec-\ntion and prevention of heart disease by leveraging time series\ndata from smartwatches, a cloud-based infrastructure, and\nmachine learning algorithms']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2552.0,labeled
1_s2.0_S2666693623000737_main,Q2,Which type of digital health application is considered in this study?,Digital Medicine,Digital Medicine,Digital Medicine,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[1, 3, 4]","[' Digital Health Journal 2023;4:165–172)\n© 2023\nHeart Rhythm Society. This is an open access article under the CC\nBY license (http://creativecommons.org/licenses/by/4.0/).\nIntroduction\nCardiovascular disease is one of the leading causes of mortal-\nity globally1 and cardiovascular healthcare accounts for a\nlarge portion of global healthcare costs and expenses. Early\ndetection and prevention will decrease the burden of cardio-\nvascular disease and will therefore decrease mortality,\nmorbidity, and costs. Cardiovascular monitoring using big\ndata from wearables and machine learning can drastically in-\ncrease the availability and ef ﬁciency of cardiovascular\nhealthcare globally, at the fraction of the costs of conven-\ntional medical-grade devices.\nElectrocardiogram (ECG) monitoring, such as Holters or\nimplantable loop recorders, are the gold standard for moni-\ntoring of outpatients with known or suspected arrhythmias.\nHowever, they are burdensome, can only be used for a\nlimited period of time, and are expensive. Implantable loop\nrecorders are invasive and have to be manually activated\nand analyzed in the hospital. This severely limits the use of\nthese devices for long-term home monitoring of patients,\nand they have suboptimal patient comfort. For patients with\nchronic cardiovascular diseases, such as atrial ﬁbrillation\nand heart failure, this implies frequent hospital visits and\nAddress reprint requests and correspondence:Mr Arman Naseri, Haga\nHospital, The Hague, Netherlands. E-mail address: a.naserijahfari@\nhagaziekenhuis.nl.\n2666-6936/© 2023 Heart Rhythm Society. This is an open access article under the CC BY license\n(http://creativecommons.org/licenses/by/4.0/).\nhttps://doi.org/10.1016/j.cvdhj.2023.09.001\n\nsometimes even hospital admissions (associated with higher\nmortality) that can be prevented by continuous and adequate\nhome monitoring.\nWith the widespread availability of reliable, consumer-\ngrade wearables such as smartwatches, continuous moni-\ntoring of, for example, heart rate with photoplethysmography\nand step counting with accelerometers is possible. This moni-\ntoring is easy, patient friendly, and cost effective. Combining\nthe power of large amounts of data (big data) and novel ma-\nchine learning techniques, these time series can be used to\ndetect and perhaps even predict cardiovascular disease, there-\nfore improving patient care. There are some caveats, howev-\ner, as not all wearables have the same characteristics and\nquality. Consequently, they have been used with moderate\nsuccess.\n2,3 They also provide less informative diagnostic sig-\nnals as compared to, for example, electrocardiography or\nother commonly used cardiologic diagnostic modalities.\nThe challenge but also the strength of machine learning\nmodels is that they learn by example and therefore large\namounts of data are needed for which the cardiovascular\noutcome (class label) has been determined. Typically, super-\nvised learning is used, where each observation of the data has\na class label. This must be done with ECGs, since photople-\nthysmography or derived signals are difﬁcult to interpret by a\nclinician. This so-called labeling or annotating of signals by\nphysicians is infeasible for the large amounts of (continuous)\ndata required, and therefore semi-automated\n4,5 and fully\nautomated2,3 ECG labeling systems6 have been developed.\nHowever, these still require a lot of manual labor from contin-\nuously monitored users.\nTherefore, the objective of the ME-TIME is (early) detec-\ntion and prevention of heart disease by leveraging time series\ndata from smartwatches, a cloud-based infrastructure, and\nmachine learning algorithms', '\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are', ' of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2583.0,labeled
1_s2.0_S2666693623000737_main,Q3,To which ICD-10 code group does the digital health application in this study pertain?,I00-I99,I00-I99,I00-I99,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[10, 4, 9]","[' commercial, or not-for-proﬁt sectors.\nDisclosures\nThe authors declare no conﬂict of interest.\nAuthorship\nAll authors attest they meet the current ICMJE criteria for\nauthorship.\nPatient Consent\nInformed consent was obtained from all subjects involved in\nthe study.\nInstitutional Review Board Statement\nThe study was conducted in accordance with the Declaration\nof Helsinki, and approved by the Institutional Review Board\n(or Ethics Committee) of METC-LDD (protocol code\nNL73708.058.20) for studies involving humans.\nReferences\n1. Roth GA, Mensah GA, Johnson CO, et al. Global burden of cardiovascular dis-\neases and risk factors, 1990–2019: update from the GBD 2019 study. J Am\nColl Cardiol 2020;76:2982–3021.\n2. Tison GH, Sanchez JM, Ballinger B, et al. Passive detection of atrialﬁbrillation\nusing a commercially available smartwatch. JAMA Cardiol 2018;3:409–416.\n3. Wasserlauf J, You C, Patel R, Valys A, Albert D, Passman R. Smartwatch perfor-\nmance for the detection and quantiﬁcation of atrialﬁbrillation. Circ Arrhythm\nElectrophysiol 2019;12:e006834.\n4. Zhu L, Nathan V, Kuang J, et al. Atrialﬁbrillation detection and atrialﬁbrillation\nburden estimation via wearables. IEEE J Biomed Health Inform 2021;\n26:2063–2074.\n5. Lubitz SA, Faranesh AZ, Selvaggi C, et al. Detection of atrialﬁbrillation in a large\npopulation using wearable devices: the Fitbit heart study. Circulation 2022;\n146:1415–1424.\n6. Hall A, Mitchell ARJ, Wood L, Holland C. Effectiveness of a single lead Alive-\nCor electrocardiogram application for the screening of atrialﬁbrillation: a system-\natic review. Medicine (Baltimore) 2020;99:e21388.\n7. Torres-Soto J, Ashley EA. Multi-task deep learning for cardiac rhythm detection\nin wearable devices. NPJ Digit Med 2020;3:116.\n8. Mienye ID, Sun Y, Wang Z. Improved sparse autoencoder based artiﬁcial neural\nnetwork approach for prediction of heart disease. Informatics in Medicine Un-\nlocked 2020;18:100307.\n9. Spathis D, Perez-Pozuelo I, Brage S, Wareham NJ, Mascolo C. Self-supervised\ntransfer learning of physiological representations from free-living wearable\ndata. In: Proceedings of the Conference on Health. Inference, and Learning;\n2021. p. 69–78.\n10. Quer G, Gouda P, Galarnyk M, Topol EJ, Steinhubl SR. Inter-and intraindividual\nvariability in daily resting heart rate and its associations with age, sex, sleep, BMI,\nand time of year: retrospective, longitudinal cohort study of 92,457 adults. PLoS\nOne 2020;15:e0227709.\n11. McInnes L, Healy J, Melville J. Umap: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint arXiv:1802.03426 2018.\n12. Coote JH. Recovery of heart rate following intense dynamic exercise. Exp Physiol\n2010;95:431–440.\n13. Gyawali PK, Horacek BM, Sapp JL, Wang L. Sequential factorized autoencoder\nfor localizing the origin of ventricular activation from 12-lead electrocardiograms.\nIEEE Trans Biomed Eng', ' of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains', '-correlation function\nbetween the heart rate window and its corresponding step\ncounter window is indicative of heart disease. To calculate\nthe correlation, we consider varying window sizes and time\ndifferences (lags) between heart rate and steps. The computed\ncross-correlation matrix for the healthy group, along with the\nAF and HF patient groups, as shown inFigure 7, shows that\nthe heart rate is correlated with the step counter with 1-minute\ndelay.\nDiscussion\nBy building a suitable infrastructure with Cloud technology,\nbig data acquired in the study is used to develop data-efﬁcient\nmodels using methods from multiple instance and self-\nsupervised learning.\nWe aim to examine the inﬂuence of inter-subject vari-\nability on predicting cardiovascular disease and will explore\npotential methods to mitigate these variabilities.\n13,14 We\nexpect that patterns indicative of cardiovascular disease\nbecome apparent within a timeframe of minutes, hours, or\nmore, considering that consumer-grade wearables have a\nslower sampling rate compared to the gold standard. We\nhave shown 1 example of such a pattern: the acceleration-\ndeceleration curve. Preselecting windows based on such\nTable 2 Confusion matrix of per-week healthy vs atrialﬁbrillation\nclassiﬁcation of the MILES model with peak aligned curves\nconcatenated with step counter data, with true and predicted labels\nshown vertically and horizontally, respectively\nTrue/Predicted label AF Healthy\nAF 11 14\nHealthy 7 33\nAF 5 atrial ﬁbrillation.\nFigure 7 Cross-correlation matrices between windowed heart rate data and\nnumber of steps for healthy and the persistent atrialﬁbrillation (AF) group.\nRows are window sizes and columns lag between heart rate and step counter.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 171\n\npatterns furthermore mitigates searching through substantial\namounts of data that may not provide much information\nabout cardiovascular disease. Inspecting the cross-\ncorrelation for several combinations of window size and\nlag show a different proﬁle for healthy and AF group individ-\nuals, showing that it is meaningful to analyze step counter\nand heart rate together. Big data for heart disease detection\nrequires substantial labeling efforts from physicians.\nUsing self-supervised learning and MIL, a model can be\ntrained with much fewer labels. Ourﬁndings demonstrate\nthis by employing MILES to achieve high speciﬁcity, which\ncan aid in ruling out heart disease in individuals experiencing\nsymptoms similar to heart disease but without the condition\n(ie, false-positives).\nConclusion\nThe ongoing ME-TIME study is a longitudinal observational\nstudy that uses machine learning with time series data from\nconsumer-grade smartwatches to detect atrial ﬁbrillation\nand heart failure. This will contribute to cost-effective cardio-\nvascular monitoring of outpatients, thereby reducing exacer-\nbation of cardiovascular disease and effectively increasing\ncapacity of global cardiovascular healthcare.\nFunding Sources\nThis research did not receive any speciﬁc grant from funding\nagencies in the public, commercial, or not-for-proﬁt sectors.\nDisclosures\nThe authors declare no conﬂict of interest.\nAuthorship\nAll authors attest they meet the current ICMJE criteria for\nauthorship.\nPatient Consent\nInformed consent was obtained from all subjects involved in\nthe study.\nInstitutional Review Board Statement\nThe study was conducted in accordance with the Declaration\nof Helsinki, and approved by the Institutional Review Board\n(or Ethics Committee) of METC-LDD (protocol code\nNL73708.058.20) for studies involving humans.\nReferences\n1. Roth GA, Mensah GA, Johnson CO, et al. Global burden of cardiovascular dis-\neases and risk factors, 1990–2019: update from the GBD 2019 study. J']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2658.0,labeled
1_s2.0_S2666693623000737_main,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[5, 3, 2]","[' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', '\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are', ' learning is used, where each observation of the data has\na class label. This must be done with ECGs, since photople-\nthysmography or derived signals are difﬁcult to interpret by a\nclinician. This so-called labeling or annotating of signals by\nphysicians is infeasible for the large amounts of (continuous)\ndata required, and therefore semi-automated\n4,5 and fully\nautomated2,3 ECG labeling systems6 have been developed.\nHowever, these still require a lot of manual labor from contin-\nuously monitored users.\nTherefore, the objective of the ME-TIME is (early) detec-\ntion and prevention of heart disease by leveraging time series\ndata from smartwatches, a cloud-based infrastructure, and\nmachine learning algorithms speciﬁcally designed to func-\ntion effectively with minimal labeling efforts.\nMethods\nStudy design and data collection\nME-TIME (registered at ClinicalTrials.gov; ID:\nNCT05802563) is designed as an observational cohort study\nconsisting of 3 data subject groups, as depicted inFigure 1.\nThe ﬁrst group consists of patients with systolic heart failure\n(HF group); the second group consists of patients with docu-\nmented atrial ﬁbrillation (AF group); and the third group,\nserving as a reference, consists of healthy volunteers. The\nrationale for creating distinct AF and HF groups comes\nfrom their unique pathophysiological characteristics. Conse-\nquently, heart rate patterns that are indicative of these dis-\neases might also be different. The HF group consists of 50\nstudy participants with systolic heart failure, deﬁned as a\nleft ventricular ejection fraction,35% without documented\natrial ﬁbrillation. The AF group consists of 50 patients with\ndocumented atrialﬁbrillation (paroxysmal, persistent, or per-\nmanent) without systolic heart failure. Ejection fractions will\nbe assessed from echocardiograms that are made within 1\nyear of inclusion, and if this is not available an echocardio-\ngram will be performed. The reference group consists of\n100 participants without any prior medical history and\nwithout medication use. Potential study subjects that meet\n2 31\nRef AF HF\n4 5\nFigure 1 Data analysis pipeline for the ME-TIME study. Included participants (image 1) are given a smartwatch (image 2), which is connected to our data\nacquisition and storage platform (image 3). The resulting data are then preprocessed (image 4) and put into the data-efﬁcient machine learning model (image\n5). AF5 atrial ﬁbrillation group; HF5 heart failure group; Ref5 reference group.\nAB C\nFigure 2 Pipeline for the study’s proposed approach.a: Segmentation of the time series of each subject (1 healthy and 2 atrialﬁbrillation) using a sliding win-\ndow. Only the label of the entire subject is available, instead of each individual window.b: The windows are inputs to an autoencoder and are compressed to a\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\n']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2542.0,labeled
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q1,Which data type is used in this study?,images,images,images,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[2, 6, 4]","[' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C', 'ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2548.0,labeled
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q2,Which type of digital health application is considered in this study?,Health IT,Health IT,Health IT,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[2, 0, 1]","[' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be', 'A Novel Web Application Framework for\nUbiquitous Classiﬁcation of Fatty Liver Using\nUltrasound Images\nD. Santhosh Reddy and P. Rajalakshmi\nWiNet Research Lab, Department of Electrical Engineering\nIndian Institute of Technology Hyderabad, India\nEmail: ee15resch11005, raji@iith.ac.in\nAbstract—Medical imaging techniques are being profoundly\nused for diagnosis of many diseases. In many of the remote areas\nin the developing nations, patients who live in rural areas are\nfacing vital health disparities compared to the general population.\nIn such scenarios, eHealth can offer promising solutions. The\neHealth especially aims at developing digital applications for\noffering high accuracy diagnosis even in remote areas. Also,\nthe integration of eHealth with advanced technologies such as\ndeep learning and artiﬁcial intelligence will further improve\nthe diagnostic accuracy and also reduces the duration. In this\npaper, we develop a novel low-cost and easily scalable eHealth\narchitecture comprising of a web application which enables\nclinicians in fatty liver classiﬁcation using ultrasound images.\nThe developed web application framework uses a deep learning\nmodel (using CNN) for accurate classiﬁcation of fatty liver\nusing ultrasound images. The clinician in a remote location\nwith a moderate internet connectivity can upload the scanned\nultrasound image to the developed web application and the\napplication identiﬁes if any abnormality is present. From the\nperformance analysis, it is observed that the developed model\nachieves an accuracy of 91.37%. Also, regarding latency the\ndeveloped classiﬁcation model predicts the abnormality presence\nin less than 20 ms. However, including the network latency, it is\nobserved that the developed eHealth architecture predicts with a\nlatency of less than 150 ms using moderate network connectivity.\nIndex Terms—eHealth, Artiﬁcial Intelligence, Fatty liver clas-\nsiﬁcation, Ultrasound, CAD, IoT Architecture\nI. I NTRODUCTION\nThe Internet of Things (IoT) is rapidly proliferating into\nmany sectors including healthcare. Today most of the health-\ncare services starting from in-hospital patient care to remote\nhealthcare are utilizing IoT technologies to improve the af-\nfordability and reachability [1]. Especially in the developing\ncountries where healthcare providers are inadequate compared\nto the total population, this can be a promising solution [2].\nIn the recent past many technologies such as wearables for\nautomated electrocardiogram monitoring, glucose and blood\npressure monitoring, and epilepsy detection have been devel-\noped at low cost using IoT technologies [3], [4]. However,\nthere is a long way to go before the healthcare becomes truly\npervasive and ubiquitous, especially in the developing nations.\nImportantly, in the case of medical imaging technologies, the\nhigh cost of the infrastructure is hindering the adoption of\nIoT technologies which realizes the telemedicine or remote\nhealthcare delivery [5]. Due to the lack of skilled sonographers\nor physicians in the rural areas, the majority of people are\nsuffering from inaccurate diagnosis being offered by the semi-\nskilled clinicians. Especially, the ultrasound imaging which\nis a widely used modality for diagnosing various diseases\npertaining to multiple organs requires skilled clinicians for\noffering accurate diagnosis [6]. Hence, we strongly feel that\nthe usage of IoT and artiﬁcial intelligence based technologies\nwill signiﬁcantly improve the current state of the healthcare.\nIn this paper, our primary contributions include:\n1) Developed a novel low-cost and scalable eHealth archi-\ntecture for classifying the fatty liver using ultrasound\nimages.\n2) Developed a web application that enables sonographers\nin the remote areas for classifying the fatty liver by\nuploading the scanned ultrasound images.\n3) The web application developed using a convolution', ' are\nsuffering from inaccurate diagnosis being offered by the semi-\nskilled clinicians. Especially, the ultrasound imaging which\nis a widely used modality for diagnosing various diseases\npertaining to multiple organs requires skilled clinicians for\noffering accurate diagnosis [6]. Hence, we strongly feel that\nthe usage of IoT and artiﬁcial intelligence based technologies\nwill signiﬁcantly improve the current state of the healthcare.\nIn this paper, our primary contributions include:\n1) Developed a novel low-cost and scalable eHealth archi-\ntecture for classifying the fatty liver using ultrasound\nimages.\n2) Developed a web application that enables sonographers\nin the remote areas for classifying the fatty liver by\nuploading the scanned ultrasound images.\n3) The web application developed using a convolutional\nneural network (CNN) along with ﬁne-tuning and trans-\nfer learning based framework for the classiﬁcation of the\nfatty liver using ultrasound images.\n4) Development of dataset comprising of 58 liver ultra-\nsound images (of which 22 correspond to fatty liver)\nfor analyzing the performance of the proposed eHealth\nframework.\n5) Performance analysis considering latency and classiﬁca-\ntion accuracy as the key performance metrics.\nThe usage of information and communication technology\n(ICT) or more broadly IoT technologies in healthcare delivery\nis generally referred to as eHealth [7]. In promoting universal\nhealth coverage, eHealth plays an important role by provid-\ning low cost healthcare services to remote and under-served\npopulations. It improves the accuracy of diagnosis by uti-\nlizing IoT and artiﬁcial intelligence technologies. Ultrasound\nscanners are usually located in well-established hospitals due\nto their huge form factor, cost and need for a person who\nis well trained in sonography. With the advancements in\ncomputing technology, conventional ultrasound scanner are\nreduced to portable ultrasound scanners for improving the\nease of scanning in rural and emergency locations [8], [9].\nAlso, they support IoT technologies wherein a doctor situated978-1-5386-4980-0/19/$31.00 ©2019 IEEE\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n978-1-5386-4980-0/19/$31.00 ©2019 IEEE 502\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nin a different location can administer the scanning in real-\ntime. However, many hospitals located in both urban and rural\nareas still use the traditional ultrasound scanners which do not\nsupport remote administration due to multiple factors such as\nlack of investment [10], [11].\nAuthors in [12] developed a low-cost portable system to\nperform obstetric ultrasonography to monitor baby in the\nwomb using an eHealth architecture. However, this requires\nthe conventional ultrasound scanner to be replaced with the\ndeveloped portable ultrasound sound scanner. Authors in [13]\ndeveloped a multimedia framework to support eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2524.0,labeled
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q3,To which ICD-10 code group does the digital health application in this study pertain?,Unknown from this paper,Unknown from this paper.,,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[2, 5, 9]","[' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be', ' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2251.0,labeled
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[4, 5, 6]","['ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.', ' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2542.0,labeled
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q1,Which data type is used in this study?,video,video,video,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[11, 10, 3]","[' s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cutting\nmethods.\na) Frame difference method: the frame difference\nmethod was used [45] for data augmentation, by adding images\nwith significant differences between consecutive frames to the\ndataset. Therefore, high-quality images could be more effec-\ntively used by the model. Specifically, the frame difference\nmethod is a method of obtaining the contour of a mov-\ning object by performing differential operations on adjacent\nframes in a video image sequence. When there is abnor-\nmal object movement in the video, there will be significant\ndifferences between frames. Subtract two frames [I (x, y, t)\nand I (x, y, t − 1)] to obtain the absolute value of the pixel\ndifference [D(x, y, t)] between the two images and determine\nwhether it is greater than the threshold to determine whether\nthere is object motion in the image sequence, as shown in the\nfollowing formula:\nD(x, y, t) = |I (x, y, t) − I (x, y, t − 1)|. (8)\nFirst, we performed median filtering, binarization, and dila-\ntion corrosion on the grayscale image to extract the contour of\nthe object. Second, a motion selection frame was performed\nby comparing the difference D(x, y, t) between the front\nand back frames, the area with a motion difference value\nis greater than the set motion threshold was selected. As\nshown in Fig. 5, using the frame difference method allows\nnot only to distinguish the characters from the background\nbut also to extract the parts of the human body that have\nundergone obvious changes. It should be noted that the motion\nselection boxes will have a large number of overlapping areas.\nIn this study, the representative motion selection boxes were\nselected according to the nonmaximum-suppression (NMS)\nalgorithm [46]. The principle of the NMS algorithm is to retain\nthe detection box with the highest confidence score for the\nsame object among multiple detection boxes while suppressing\nother boxes with lower confidence scores. Finally, according to\nthe relationship between the number of motion-selected boxes\nin the image and the set threshold k, it is determined whether\nthe frame image needs to be copied. According to the ablation\nexperimental results, we determine the value of k to be 1,\nwhich can maximize the gain of model performance.\nb) Image cutting method: After processing by using the\nframe difference method, we could obtain a large amount\nof effective image data, and then, these data were subjected\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 5. Pixel intensity changes in a screenshot of an HD patient’s video with\na 1-s interval between the front and back. The', 'follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut', ' However,\nshortcomings are to be mentioned. Clinical scoring scales [4],\n[5] require well-trained neurologists to obtain reliable data, and\nthey are not easily adaptable for remote clinical assessment\n1) Although the reliability of biological marker-based [3]\ndiagnostic methods is good, these methods require\nexpensive instruments with inherent issues to be used\non a large scale. Use of wearable devices [7], [8] can\ngenerate data at a low cost. However, this method is a\ncontact method, which is not convenient for long-term\ntracking and analysis of patients, and the probability of\ninhomogeneous assessment is relatively high.\n2) Previously, several machine learning models [13], [17]\nhave been used in HD assessment, including the anal-\nysis models of wearable devices and voice. However,\nsome obvious defects hinder their application in clinical\npractice. For example, voice-based methods need to first\nextract voice features and then use statistics or machine\nlearning models for analysis. This two-stage method\nmay lead to unstable analysis results.\n3) HD is rare disease and the prevalence is low as 0.38 per\n100 000 person-years. Therefore, it is difficult to obtain\na large number of HD video data for traditional machine\nlearning models. Indeed, data scarcity usually results\nin unstable analysis of traditional machine learning\nmodel [22]. How to obtain accurate assessment results\nthrough few-shot data is very important to model devel-\nopment and clinical application.\nBased on the above considerations, we have developed\nan end-to-end few-shot video analysis model. The proposed\nFig. 1. Comparison of six pose examples of HD patients with normal poses,\nincluding: 1—upper bodies, 2—sitting, 3—front-facing standing, 4—side-fac-\ning standing, 5—back-facing standing, and 6—walking. To avoid revealing the\nidentity of the patients, we removed the eye region from their face images.\nmodel utilize facial details and body posture details [9], [23]\nto perform analysis, as shown in Fig. 1. Specifically, we first\ndesign a frame difference method to perform data enhancement\non some key video data. Second, we have embedded two\ntypes of effective attention modules into the 3-D convolutional\nnetwork to enhance the ability of the deep learning model\nto extract HD video features. Finally, we have employed the\nresidual structure to repeatedly stack the extracted features and\nthen use a fully connected layer to obtain the final diagnosis\nresult. Therefore, the main contributions of this article are\nsummarized as follows.\n1) We have developed an effective end-to-end high-\ndefinition diagnosis model based on posture videos.\nCompared with using gait or biomarker data, posture\nvideo-based data are more convenient and can provide\nreliable diagnostic results.\n2) Both spatial–temporal excitation attention and spatial–\nchannel attention modules have been incorporated into\nthe 3-D residual network to enhance its feature extrac-\ntion capabilities. In comparison with other machine\nlearning approaches, superior and more consistent per-\nformance is demonstrated by this architecture.\n3) We have designed a frame difference algorithm to\nimprove the video data through the data enhancement.\nThis new method can overcome issues related to small\nsample size and improve outcomes in this few-shot\nlearning task, thereby providing reliable assessment\noutcomes. Compared with other Huntington evaluation\nmodels, experiments confirm that our model can achieve\nbetter recall and accuracy.\nOverall, HD is a rare genetic disorder with a wide spectrum\nof signs and symptoms. There is an urgent need to develop\nan automatic assessment of HD in clinical practice. Several\nassessments have been developed including the analysis mod-\nels of clinical scoring scales, biological markers and wearable\ndevices, image, and voice. Although the above models can\nobtain promising results, some obvious defects hinder their']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2580.0,labeled
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q2,Which type of digital health application is considered in this study?,Unknown from this paper,Unknown from this paper,,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[6, 4, 7]","[' the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel level. Third, to better\nextract the spatial, temporal, and channel features from the\nfeature maps, we employed attention modules to optimize\nthe 3DResNet module. Specifically, we designed two bottle-\nneck modules: squeeze excitation (SE) attention modules and\nspatial–temporal excitation (STE) attention modules. By rea-\nsonably cross-embedding different bottleneck modules into\nthe 3DRes-Net model, our model can learn the interdepen-\ndencies among different dimensions, promote useful features,\nand suppress redundant features. Finally, all features were\nprocessed using a fully connected layer, and a softmax layer\nwas employed to obtain the probability output for each disease\ntype.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 2. Pipeline of the proposed 3-D RAJNet, which mainly consists of an improved residual network block, the spatial–channel attention model, and a\nfeature classifier.\nA. Upstream Feature Extraction Module\n1) Feature Pre-Extraction: For the few-shot learning task\nof HD disease clinical assessment, we must perform the\ndata enhancement and feature pre-extraction to avoid model\noverfitting and improve the generalization and robustness of\nthe model. In addition, since video information often include\nirrelevant background noise, we used the frame difference\nmethod [45] to further process the data. The frame difference\nmethod can prevent the model from learning information that\nis irrelevant to the target and further enhance the effective\ndata information. A detailed data enhancement algorithm is\nintroduced in Section IV.\nIn this module, the RGB images extracted from the orig-\ninal video are first converted into a grayscale map, and the\nframe-difference method is then used for data enhancement.\nGiven input video IM ∈ RT ∗C∗H∗W , our model first cuts the\nsize of the input image by half and then stacks it in the depth\ndimension, getting a processed video IM ∈ RT ∗4C∗H/2∗W/2, to\nachieve a larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64', ' demonstrated by this architecture.\n3) We have designed a frame difference algorithm to\nimprove the video data through the data enhancement.\nThis new method can overcome issues related to small\nsample size and improve outcomes in this few-shot\nlearning task, thereby providing reliable assessment\noutcomes. Compared with other Huntington evaluation\nmodels, experiments confirm that our model can achieve\nbetter recall and accuracy.\nOverall, HD is a rare genetic disorder with a wide spectrum\nof signs and symptoms. There is an urgent need to develop\nan automatic assessment of HD in clinical practice. Several\nassessments have been developed including the analysis mod-\nels of clinical scoring scales, biological markers and wearable\ndevices, image, and voice. Although the above models can\nobtain promising results, some obvious defects hinder their\napplication in clinical practice. In the present study, we have\ndeveloped an effective proficient pose video-based end-to-end\ndiagnostic model for HD. This new method can overcome\nissues related to few-shot and improve outcomes in this\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nfew-shot learning task, thereby providing dependable and\noutstanding diagnostic outcomes. The remainder of this article\nis organized as follows. Section II introduces recent work\nrelated to HD diagnosis. Section III elaborates on the proposed\nframework, followed by extensive experimental results and\nanalysis in Section IV. Last, the conclusion is drawn in\nSection V.\nII. R ELATED WORKS\nA. Biological Markers-Based Model\nBiomarkers are very common detection methods in the clin-\nical assessment of HD [24]. Currently, several new biomarkers\nhave been developed [3] to detect symptoms of ND diseases.\nFor example, blood-based assessments have been reported as\npossible biomarkers for the evaluation of HD [25]. Given that\nlipid composition and disturbances in lipoprotein metabolism\nmay be associated with the pathogenesis of HD, the plasma\nlipoprotein profile has been proposed as predictive biomark-\ners for HD progression [3]. In addition, MRI [26] and\nEEG [27], [28] have been explored biomarkers for HD. Based\non EEG features associated with disease progression in HD,\nan automatic classifier has been developed to distinguish\nhealthy controls (HCs) from HD gene carriers. Similarly, MRI-\nbased [26] imaging biomarkers have been used to determine\nthe degree of morphological changes.\nB. Wearable Device-Based Clinical Assessment Model\nUnlike biomarker-based methods, wearable device-based\nmethods are less costly and more straightforward measure-\nments for disease evaluation. Indeed, wearable or portable\nsensors have been shown to be effective in HD signs assess-\nment signs [7], [29]. Based on the data collected by sensors,\nsuch as stride frequency, stride length, and walking speed,\nseveral algorithms have been used for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through', ' larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2575.0,labeled
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q3,To which ICD-10 code group does the digital health application in this study pertain?,Unknown from this paper,Unknown from this paper.,,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[7, 12, 9]","[' larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res', ' the value of k to be 1,\nwhich can maximize the gain of model performance.\nb) Image cutting method: After processing by using the\nframe difference method, we could obtain a large amount\nof effective image data, and then, these data were subjected\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 5. Pixel intensity changes in a screenshot of an HD patient’s video with\na 1-s interval between the front and back. The right image occurs 1 s after\nthe left image.\nTABLE I\nCOMPARATIVE EXPERIMENTAL RESULTS WITH STATE\nOF THE ART AND OTHER TRADITIONAL MODELS\nto additional processing as follows. In particular: 1) crop\nthe original image into four small images, so the shape of\nthe image changes from (1, 512, 512) to (4, 256, 256) and\n2) the images obtained by slicing are reorganized, and every\n20 images were stacked into a 3-D matrix.\nSpecifically, we use indicators, such as recall rate and accu-\nracy, to evaluate the model [47]. Among them, the recall rate\nis of great significance for medical disease clinical assessment\nbecause it can reflect the situation of missed assessment in a\nmodel when diagnosing diseases. In medical disease clinical\nassessment, if a model fails to diagnose a disease, the patient\nmay miss the optimal treatment opportunity, leading to the\ndeterioration of the disease. Specifically, HD, as a relatively\nrare ND disease, has a high probability of missed assessment\nin clinical practice. However, our model has a high recall\nrate and high accuracy, which can reduce the risk of missed\nassessment to a certain extent, and accurately target clinical\npain points.\nB. Model Evaluation\nIn this section, we compare our model with the current\nmain methods for diagnosing HD. The comparison was con-\nducted on the results from different models (2DCNN [48] and\n3DCNN [49]) utilizing the same dataset. The other compared\nmethods can be found in [50], and the resulting outcomes are\ndisplayed in Table I.\nHere, the results marked with * in the 3DCNN and 2DCNN\nmodels are taken from the enhanced data: 1) shortcom-\nings of traditional models: the 2DCNN and 3DCNN obtain\npoor model performance. The SVM, Log. Regr, LDA, and\nQDA: a) invasive data acquisition; b) need to be trained\nfor enough epochs to obtain satisfactory results [50]; and\nc) low model performance and low cost-effectiveness and\n2) our contribution: a) better performance and low-missed\nassessment rate; b) end-to-end assessment; and c) low cost\nand convenience.\n1) Comparison With Other Models:First, the main meth-\nods for evaluating HD previously included scale analysis,\nfacial muscle group analysis model, and gait analysis for\ncomparison. Their shortcomings are very obvious. The scale\nanalysis uses the UHDRS diagnostic confidence level (DCL),\nwhich is based on the evaluation of motor signs is relatively\nlimited. The facial muscle group analysis uses methods, such\nas coordinate analysis and SVM, to analyze the amplitude\ncharacteristics of facial expressions and the shaking of facial\nsmall muscle groups. It is a non-end-to-end model with weak\ngeneralization ability. The gait analysis measures the severity\nof a patient’s illness based on high-level gait characteristics,\nrequiring patients to wear', ', which could\neffectively guarantee the robustness and stability of our model.\n1) SE Block: As shown in Fig. 4(a), as a kind of channel\nexcitation, the SE block can explicitly model the interde-\npendencies between feature channels. It can access global\ninformation and determine the importance of each feature\nchannel in two steps. This allows the network to promote\nuseful features and suppress irrelevant ones, enhancing its\nsensitivity to informative features.\nThe squeeze operation performs feature compression along\nthe spatial dimension, turning each 2-D feature channel into a\nreal number that somehow has a global perceptual field. The\noutput dimension matches the number of input feature chan-\nnels. It characterizes the global distribution of the response\nover the feature channels and makes the global perceptual field\navailable to the layers close to the input. This was accom-\nplished by performing global average pooling (GPooling) on\nthe original feature map C ∗W ∗H and obtaining a feature map\nof size 1∗1∗C with a global perceptual field. The processing of\nour channel attention module can be summarized as follows:\nFt+5 = Sig(FC(Relu(FC(GPooling(Ft+4)))) ∗ Ft+4). (6)\n2) STE Block: As shown in Fig. 4(b), the STE is another\nmodule that extracts spatiotemporal features from videos\nby generating a spatiotemporal attention map. Traditional\nspatial–temporal feature extraction mainly uses 3-D convo-\nlution; however, introducing 3-D convolution directly to the\ninput significantly increases the computational effort of the\nmodel. We first use a channel average on Ft+4 to get a\nglobal channel feature F′\nt+4 ∈ RN∗T ∗1∗H∗W , and then, F′\nt+4\nis reshaped into dimensions that can be manipulated by 3-D\nconvolution, i.e. (N, 1, T, H, W ), where N represents the\nbatch size, T denotes the data time dimension, the channel\ndimension C is compressed to 1, and H ∗ W denotes the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\ndata height and width, respectively. Subsequently, we use a\nconvolutional layer to extract the attention map from F′\nt+4.\nThis attention map is multiplied by Ft+4 to obtain the final\nspatial–temporal features Ft+5.\n3) Learning Strategy: In the final part of the model,\nwe leverage multihead attention to enhance the model’s classi-\nfication ability. Subsequently, we used a fully connected layer\nand a softmax function to complete the disease probability\noutput of the model. This process can be summarized as\nfollows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2687.0,labeled
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[5, 14, 15]","[' for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel', ' the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVIDUAL COMPONENTS\nFig. 6 shows that the AUC areas of both 2DCNN and 3DCNN\nmodels are below 0.5, and their poor recall performance makes\nthem unsuitable for clinical assessment. Our model has an\nAUC area of up to 0.76, completely enveloping the baseline,\nindicating that our model not only outperforms other models\nbut also performs well at any judgment threshold.\nC. Ablation Study\nIn this section, we conducted ablation experiments on data\naugmentation, attention module, and the number of motion\nselection boxes, and the results are shown in Table II. Here,\nSE–ST represents SE block and STE block in our model,\nand SpaChan represents multihead attention enhancement in\nour model. Both the SpaChan module and SE module. Null\nrepresents that the attention module is not used. K represents\nthe motion selection box threshold.\n1) Data Enhancement Comparison: To demonstrate the\neffectiveness of the frame difference data augmentation and\ncutting method, we plotted a performance comparison graph\nTABLE III\nCOMPUTATIONAL EFFICIENCY AND COMPLEXITY OF\nEACH COMPONENT OF THE MODEL\nFig. 7. Performance comparison of different models before and after data\naugmentation. The model * represents the performance improvement of the\nmodel after data enhancement, and the part with diagonal lines on the column\nrepresents a decrease in numerical values.\nof 2DCNN, 3DCNN, and our model before and after data\naugmentation, as shown in Fig. 7.\nAccording to the survey results in Table II, the performance\nof our model has been comprehensively improved after imple-\nmenting data augmentation techniques. Moreover, in order to\nexhibit the efficacy of the frame difference data augmentation\nand cutting techniques, we illustrate the performance com-\nparison charts of 2DCNN, 3DCNN, and our model pre and\npostdata augmentation, as demonstrated in Fig. 7. It appears\nthat the 2DCNN model is incapable of effectively deriving\nbenefits from data augmentation, owing to the model’s limited\ncapacity for feature mining. Nevertheless, other models display\na substantial improvement as a result of data augmentation.\nThe implementation of data augmentation was found to sig-\nnificantly decrease the loss of 3DCNN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same', 'NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2574.0,labeled
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q1,Which data type is used in this study?,tabular,tabular,tabular,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[5, 7, 2]","['olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis', ' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates', ' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2540.0,labeled
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q2,Which type of digital health application is considered in this study?,Unknown from this paper,Unknown from this paper.,,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[6, 10, 2]","[' classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Discriminative Performance: The primary metric that\nwe use to judge the discriminative power of a model is\nthe Area Under the Curve (AUC) of the receiver operating\ncharacteristic (ROC) curve. Models with ideal discrimination\npower have an AUC of 1.0 and the ones with no discrimi-\nnatory ability have AUC equal 0.5. Along with AUC we also\nreport balanced accuracy (or average class accuracy), precision\n(or positive predictive value) and recall (or true positive\nrate). Balanced accuracy is calculated as the average of the\nproportion of correct predictions for each class individually.\nBalanced accuracy is chosen ahead of regular accuracy as it\nprovides a better overview of the model performance than the\nclassiﬁcation accuracy when the classes are not balanced [25].\nPrecision is the proportion of positive identiﬁcations by the\nmodel that were actually correct. Recall is the proportion of\nactual positives that were identiﬁed correctly.\nIt should be noted, that AUC is the strongest indicator of the\nmodel performance among all the discriminative performance\nmetrics that we report here. AUC is statistically consistent and\nmore discriminating than accuracy [26]. Balanced accuracy,\nprecision and recall are all dependent on the threshold we\nchoose to apply on the probability estimations of the class\nprediction to get the class labels. Choosing an appropriate\ndecision threshold is highly dependent on the desired outcome\nof the predictive model (high speciﬁcity or high sensitivity).\nSince our goal is to give a probability score for MetS, AUC\nis more relevant for us. For computing balanced accuracy,\nprecision and recall, a decision threshold of 0.5 is assumed.\nA 5-fold cross-validation was performed to get a more robust\nestimate of the validation metrics. The hyper-parameters for\nthe different classiﬁers were optimized empirically.\n2) Model Calibration: Model calibration refers to the\nagreement between subgroups of predicted probabilities and\ntheir observed frequencies [27]. For predictive models in\nmedicine, usually the predicted probabilities of the different\nclass labels are important as they show how conﬁdent the\nmodel is about the predictions. Models like LR are usually\nwell calibrated as it directly optimizes the log-loss. But many\nother machine learning models like RF are thought to be\npoorly calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning', '-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.', ' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2550.0,labeled
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q3,To which ICD-10 code group does the digital health application in this study pertain?,E00-E90,E00-E90,E00-E90,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[6, 5, 15]","[' classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Discriminative Performance: The primary metric that\nwe use to judge the discriminative power of a model is\nthe Area Under the Curve (AUC) of the receiver operating\ncharacteristic (ROC) curve. Models with ideal discrimination\npower have an AUC of 1.0 and the ones with no discrimi-\nnatory ability have AUC equal 0.5. Along with AUC we also\nreport balanced accuracy (or average class accuracy), precision\n(or positive predictive value) and recall (or true positive\nrate). Balanced accuracy is calculated as the average of the\nproportion of correct predictions for each class individually.\nBalanced accuracy is chosen ahead of regular accuracy as it\nprovides a better overview of the model performance than the\nclassiﬁcation accuracy when the classes are not balanced [25].\nPrecision is the proportion of positive identiﬁcations by the\nmodel that were actually correct. Recall is the proportion of\nactual positives that were identiﬁed correctly.\nIt should be noted, that AUC is the strongest indicator of the\nmodel performance among all the discriminative performance\nmetrics that we report here. AUC is statistically consistent and\nmore discriminating than accuracy [26]. Balanced accuracy,\nprecision and recall are all dependent on the threshold we\nchoose to apply on the probability estimations of the class\nprediction to get the class labels. Choosing an appropriate\ndecision threshold is highly dependent on the desired outcome\nof the predictive model (high speciﬁcity or high sensitivity).\nSince our goal is to give a probability score for MetS, AUC\nis more relevant for us. For computing balanced accuracy,\nprecision and recall, a decision threshold of 0.5 is assumed.\nA 5-fold cross-validation was performed to get a more robust\nestimate of the validation metrics. The hyper-parameters for\nthe different classiﬁers were optimized empirically.\n2) Model Calibration: Model calibration refers to the\nagreement between subgroups of predicted probabilities and\ntheir observed frequencies [27]. For predictive models in\nmedicine, usually the predicted probabilities of the different\nclass labels are important as they show how conﬁdent the\nmodel is about the predictions. Models like LR are usually\nwell calibrated as it directly optimizes the log-loss. But many\nother machine learning models like RF are thought to be\npoorly calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning', 'olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis', ' a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D. G. Altman and P. Royston, “The cost of dichotomising continuous\nvariables,”Bmj, vol. 332, no. 7549, p. 1080, 2006.\n[22] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.\n5–32, 2001.\n[23] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, “Classiﬁca-\ntion and regression trees. belmont, ca: Wadsworth,”International Group,\nvol. 432, pp. 151–166, 1984.\n939\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\n[24] J. H. Friedman, “Greedy function approximation: a gradient boosting\nmachine,”Annals of statistics, pp. 1189–1232, 2001.\n[25] J. D. Kelleher, B. Mac Namee, and A. D’arcy,Fundamentals of machine\nlearning for predictive data analytics: algorithms, worked examples, and\ncase studies. MIT Press, 2015.\n[26] C. X. Ling, J. Huang, H. Zhang et al., “Auc: a statistically consistent\nand more discriminating measure than accuracy,” inIjcai, vol. 3, 2003,\npp. 519–524.\n[27] F. J. Dankers, A. Traverso, L. Wee, and S. M. van Kuijk, “Prediction\nmodeling methodology,” in Fundamentals of Clinical Data Science.\nSpringer, 2019, pp. 101–120.\n[28] A. Niculescu-Mizil and R. Caruana, “Predicting good probabilities\nwith supervised learning,” in Proceedings of the 22nd international\nconference on Machine learning. ACM, 2005, pp. 625–632.\n[29] M. H. DeGroot and S. E. Fienberg, “The comparison and evaluation\nof forecasters,”Journal of the Royal Statistical Society: Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncompar']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2667.0,labeled
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[7, 2, 10]","[' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates', ' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', '-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2525.0,labeled
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q1,Which data type is used in this study?,time-series,time-series,time-series,"['tabular', 'time-series', 'images', 'text', 'video', 'audio']","[4, 2, 12]","[',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', ' principal component analysis (PCA) [9], fast\n2168-2291 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 415\ncorrelation-based ﬁlter [10], and sequential forward selection\n(wrapper) [11] methods. By comparing the classiﬁcation rate of\nidentifying ﬁve daily activities carrying a smartphone on ﬁve\nbody positions, Khanet al.[12] proved that Kernel discriminant\nanalysis is better than linear discriminant analysis and signal\nmagnitude area. Shoaibet al. [13] demonstrated putting three\ntypes of sensors (i.e., accelerometer, gyroscope, and linear ac-\ncelerometer) on the wrist and pocket to identify less-repetitive\nactivities, such as smoking, eating, and drinking coffee. For\nsolving the problem of online time series segmentation, Ignatov\net al. [14] constructed the phase trajectory matrix and applied\nthe PCA technique to extract features of the ﬁrst period. Sousa\net al.[15] investigated the classiﬁcation capability with different\nfeature sets, such as time-domain (mathematical and statis-\ntical parameters), frequency-domain (wavelet transform), and\ndiscrete-domain (symbolic representations). Although the above\nworks achieved signiﬁcant progress for HAR, the limitations\nstill existed. Most of them ignored the affection of shift changes\nin position and orientation of the smartphone due to motion\nartifacts.\nDuring the past decades, many researchers have explored\nvarious ML and DL algorithms to identify more complex ac-\ntivities with higher recognition rates. Koseet al.[1] proved that\nk-nearest neighbor (k-NN) could obtain a higher classiﬁcation\naccuracy than Na¨ive Bayes (NB) by recognizing four kinds\nof activities, i.e., walking, running, sitting, and standing. Lee\nand Cho [16] designed a mixture-of-experts model for dealing\nwith uncertain and incomplete data. Reyes-Ortiz et al. [17]\nproposed a transition-aware HAR system based on the support\nvector machine (SVM) algorithm to classify a broad spectrum\n(up to 33 types) of activity with smartphone in real-time. Lee\nand Cho [18] presented a hierarchical hidden Markov model\n(HHMM) method to classify motions (standing, walking, go\nstairs, and running) and daily activities (shopping, taking a\nbus, and moving arms). A two-stage continuous hidden Markov\nmodel (HMM) algorithm was proposed to reduce the number of\nfeature subsets because it decomposed the complex activities\ninto several simpler ones [19]. Reiss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions', ' (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the average testing time and training\ntime computed on the compressed dataset are signiﬁcantly less\nthan those for the uncompressed dataset. Hence, compressing the\ntraining datasets saves more time for building a new classiﬁer.\nEven if the accuracy obtained on the uncompressed dataset is\ngreater than that on the compressed dataset, it is time-consuming\nto build the classiﬁer and predict an activity. Moreover, the trust\ndegree η can be adjusted to increase the accuracy using (7).\nB. Online Learning Examination\nThis experiment aims to prove the updating ability of Ada-\nHAR system for recognizing a new activity. Two new activities\n(squats and twisting hips) were collected from one subject\ncarrying the smartphone on the waist or put in the left pocket,\nrespectively. To conﬁrm that the proposed signal processing al-\ngorithms could overcome the effect of changing the direction of\nmobile phone, we combined the collected data of two directions\nwith the same testing protocol (shown in each top graph of\nFig. 6). The DL-based HC algorithm was not adopted because\nthe training procedure is costly. The experiments adopted an\nonline batch learning mechanism. The SVM method used the\nGaussian kernel. The built ML-based classiﬁers for identifying\nthe 12 original activities were used to predict the new activity\non the four datasets. The following four experiments were per-\nformed to evaluate the adaptive performance of both compressed\nand uncompressed datasets.\nFig. 6(a) displays a comparison results of the online accuracy\nand computational time for predicting squats by carrying the\nmobile phone on the waist. The top graph is the collected 3-axis\nacceleration with two directions. The middle graph is the online\nclassiﬁcation accuracy computed by (1) and (2) at time (epoch)t.\nBoth k-NN and SVM methods obtain a higher accuracy than the\nother methods. The bottom graph is the online predicting time.\nThe amplitudes ofk-NN, DT, and SVM are 0.05 s, while the other\napproaches require more than 1 s to predict activity. Hence, the\nk-NN-based HC classiﬁer is the best method for recognizing\nthe new activity. Fig. 6(b) portrays the results of predicting\nsquats by putting the mobile phone in the left pant pocket. The\nHC classiﬁers based onk-NN, SVM, and DT obtain a higher\naccuracy for identifying squats, andk-NN spends less time to\nretrain the classiﬁer. Fig. 6(c) and (d) shows the validation\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 421\nTABLE III\nTHE COMPARATIVERESULTS AMONG HC CLASSIFIER BASED ON K-NN, DT']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2517.0,labeled
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q2,Which type of digital health application is considered in this study?,Telehealth,Telehealth,Telehealth,"['Precision Medicine', 'Health IT', 'Digital Medicine', 'Telehealth', 'Wellness']","[0, 13, 2]","['414 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nA Smartphone-Based Adaptive Recognition and\nReal-Time Monitoring System for Human Activities\nWen Qi , Student Member , IEEE,H a n gS u, Member , IEEE, and Andrea Aliverti\nAbstract—Human activity recognition (HAR) using smart-\nphones provides signiﬁcant healthcare guidance for telemedicine\nand long-term treatment. Machine learning and deep learning\n(DL) techniques are widely utilized for the scientiﬁc study of the\nstatistical models of human behaviors. However, the performance\nof existing HAR platforms is limited by complex physical activity.\nIn this article, we proposed an adaptive recognition and real-time\nmonitoring system for human activities (Ada-HAR), which is ex-\npected to identify more human motions in dynamic situations. The\nAda-HAR framework introduces an unsupervised online learning\nalgorithm that is independent of the number of class constraints.\nFurthermore, the adopted hierarchical clustering and classiﬁca-\ntion algorithms label and classify 12 activities (ﬁve dynamics, six\nstatics, and a series of transitions) autonomously. Finally, practical\nexperiments have been performed to validate the effectiveness and\nrobustness of the proposed algorithms. Compared with the methods\nmentioned in the literature, the results show that the DL-based\nclassiﬁer obtains a higher recognition rate (95.15%, waist, and\n92.20%, pocket). The decision-tree-based classiﬁer is the fastest\nmethod for modal evolution. Finally, the Ada-HAR system can\nmonitor human activity in real time, regardless of the direction\nof the smartphone.\nIndex Terms —Data compression, deep learning (DL),\nhierarchical classiﬁcation (HC), human activity recognition\n(HAR).\nI. INTRODUCTION\nR\nECOGNIZING human activities using inertial sensors in\na smartphone has attracted increasing research interests\nduring the past decades in various domains, ranging from home\nhealthcare to sports monitoring, rehabilitation, personalized\nmedicine, and mental disorders [1]. Recently, with the develop-\nment of the Internet of Things, machine learning (ML), and deep\nlearning (DL) techniques, human activity recognition (HAR)\ncan be achieved by transferring data in the body area networks\nand wireless Ethernet, allowing the assessment of the human\nphysical and physiological status [2]. However, many studies\ndevelop appropriate tasks for a given HAR system by resorting\nto extensive heuristic knowledge [3]. They are applicable in a\nlaboratory or well-controlled situation via body-ﬁxed mobile\nManuscript received May 24, 2019; revised September 2, 2019 and January\n1, 2020; accepted February 23, 2020. Date of publication April 24, 2020; date of\ncurrent version September 15, 2020. This article was recommended by Associate\nEditor Bin Guo.(Corresponding author: Wen Qi.)\nThe authors are with the Dipartimento di Elettronica, Informazione\ne Bioingegneria, Politecnico di Milano, 20133 Milano, Italy (e-mail:\nwen.qi@polimi.it; hang.su@polimi.it; andrea.aliverti@polimi.it).\nColor versions of one or more of the ﬁgures in this article are available online\nat http://ieeexplore.ieee.org.\nDigital Object Identiﬁer 10.1109/THMS.2020.2984181\ndevices [4]. The variability of the disturbances of the mobile\ndevices affects the recognition rate of the HAR system, such as\nmovement artifacts, baseline noise, the happening of new activ-\nity, and the differences among users. For example, identifying\nthe same walking status of the elderly and', ' putting the mobile phone in the left pant pocket. The\nHC classiﬁers based onk-NN, SVM, and DT obtain a higher\naccuracy for identifying squats, andk-NN spends less time to\nretrain the classiﬁer. Fig. 6(c) and (d) shows the validation\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 421\nTABLE III\nTHE COMPARATIVERESULTS AMONG HC CLASSIFIER BASED ON K-NN, DT, NB, ANN, SVM,AND LSTM\nFig. 6. Comparison of the results of predicting new activities. The three graphs are the 3-axis acceleration with two directions, the online accuracycomputed\nby the ﬁve ML-based Ada-HAR systems and the online predicting time. The solid line and dotted line are the compressed and uncompressed datasets. The pulse\npoints in the bottom graph are the locations at which the HC classiﬁer was retrained. (a) Squats with the smartphone on the waist. (b) Squats with the smartphone\nin the pocket. (c) Twisting hips with the smartphone on the waist. (d) Twisting hips with the smartphone in the pocket.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n422 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nTABLE IV\nCOMPARISON OF THEFINAL ACCURACY ANDTOTAL TESTING TIME AMONG THE FIVE ML ALGORITHMS OF THEADA-HAR SYSTEM\nresults of predicting twisting hips by carrying the smartphone\non the waist and in the pocket. Table IV reports a comparison of\nthe total classiﬁcation accuracy and average testing time among\nthe Ada-HAR system based on the ﬁve ML algorithms. The\nresults prove that the compressed training dataset can save the\nupdating time for updating the HC classiﬁer, whilek-NN and\nDT algorithms are the best two algorithms.\nC. Demonstration\nThe video experiment of Ada-HAR demonstrates the recog-\nnition process in real time by changing the direction of the\nsmartphone with two positions. The mean transmission time\nof local network is shorter than that of remote connection. For\nexample, it only cost 0.18 s to receive ten new samples in the\nlocal network, while it needed 0.25 s for the remote transmission.\nEach result was obtained as the average of 100 repetitions.\nVI. CONCLUSION\nIn this article, we proposed a smartphone-based adaptive\nrecognition and real-time monitoring system (Ada-HAR) for\nhuman activities. Experiments were performed with 25 subjects\nto validate the performance of the developed system. The most\ninnovative part was the online learning algorithm. It was capable\nof updating the classiﬁer in a dynamic environment, i.e., which\nmeans if any new activity is found, the HC classiﬁer will be\nupdated automatically to include the new class. It is an unsu-\npervised online learning algorithm that does not need to get the\ntrue labels. In addition to the adaptive algorithm, an automatic\nlabeling method using Hk-mC algorithm was another original\nachievement that improves the efﬁciency of labeling the raw\nsignals. The experiments proved its robustness by placing the\nsmartphone on different positions with various directions. The\nintroduced signal preprocessing strategy provides stable inputs\nfor the classiﬁer regardless of the', ' principal component analysis (PCA) [9], fast\n2168-2291 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 415\ncorrelation-based ﬁlter [10], and sequential forward selection\n(wrapper) [11] methods. By comparing the classiﬁcation rate of\nidentifying ﬁve daily activities carrying a smartphone on ﬁve\nbody positions, Khanet al.[12] proved that Kernel discriminant\nanalysis is better than linear discriminant analysis and signal\nmagnitude area. Shoaibet al. [13] demonstrated putting three\ntypes of sensors (i.e., accelerometer, gyroscope, and linear ac-\ncelerometer) on the wrist and pocket to identify less-repetitive\nactivities, such as smoking, eating, and drinking coffee. For\nsolving the problem of online time series segmentation, Ignatov\net al. [14] constructed the phase trajectory matrix and applied\nthe PCA technique to extract features of the ﬁrst period. Sousa\net al.[15] investigated the classiﬁcation capability with different\nfeature sets, such as time-domain (mathematical and statis-\ntical parameters), frequency-domain (wavelet transform), and\ndiscrete-domain (symbolic representations). Although the above\nworks achieved signiﬁcant progress for HAR, the limitations\nstill existed. Most of them ignored the affection of shift changes\nin position and orientation of the smartphone due to motion\nartifacts.\nDuring the past decades, many researchers have explored\nvarious ML and DL algorithms to identify more complex ac-\ntivities with higher recognition rates. Koseet al.[1] proved that\nk-nearest neighbor (k-NN) could obtain a higher classiﬁcation\naccuracy than Na¨ive Bayes (NB) by recognizing four kinds\nof activities, i.e., walking, running, sitting, and standing. Lee\nand Cho [16] designed a mixture-of-experts model for dealing\nwith uncertain and incomplete data. Reyes-Ortiz et al. [17]\nproposed a transition-aware HAR system based on the support\nvector machine (SVM) algorithm to classify a broad spectrum\n(up to 33 types) of activity with smartphone in real-time. Lee\nand Cho [18] presented a hierarchical hidden Markov model\n(HHMM) method to classify motions (standing, walking, go\nstairs, and running) and daily activities (shopping, taking a\nbus, and moving arms). A two-stage continuous hidden Markov\nmodel (HMM) algorithm was proposed to reduce the number of\nfeature subsets because it decomposed the complex activities\ninto several simpler ones [19]. Reiss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2567.0,labeled
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q3,To which ICD-10 code group does the digital health application in this study pertain?,Unknown from this paper,Unknown from this paper.,,"['A00-B99', 'C00-D48', 'D50-D89', 'E00-E90', 'F00-F99', 'G00-G99', 'H00-H59', 'I00-I99', 'J00-J99', 'K00-K93', 'L00-L99', 'M00-M99', 'N00-N99', 'O00-O99', 'P00-P96', 'Q00-Q99', 'R00-R99', 'S00-S99', 'T00-T98', 'U00-U99', 'V01-Y98', 'Z00-Z99']","[2, 13, 7]","[' principal component analysis (PCA) [9], fast\n2168-2291 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 415\ncorrelation-based ﬁlter [10], and sequential forward selection\n(wrapper) [11] methods. By comparing the classiﬁcation rate of\nidentifying ﬁve daily activities carrying a smartphone on ﬁve\nbody positions, Khanet al.[12] proved that Kernel discriminant\nanalysis is better than linear discriminant analysis and signal\nmagnitude area. Shoaibet al. [13] demonstrated putting three\ntypes of sensors (i.e., accelerometer, gyroscope, and linear ac-\ncelerometer) on the wrist and pocket to identify less-repetitive\nactivities, such as smoking, eating, and drinking coffee. For\nsolving the problem of online time series segmentation, Ignatov\net al. [14] constructed the phase trajectory matrix and applied\nthe PCA technique to extract features of the ﬁrst period. Sousa\net al.[15] investigated the classiﬁcation capability with different\nfeature sets, such as time-domain (mathematical and statis-\ntical parameters), frequency-domain (wavelet transform), and\ndiscrete-domain (symbolic representations). Although the above\nworks achieved signiﬁcant progress for HAR, the limitations\nstill existed. Most of them ignored the affection of shift changes\nin position and orientation of the smartphone due to motion\nartifacts.\nDuring the past decades, many researchers have explored\nvarious ML and DL algorithms to identify more complex ac-\ntivities with higher recognition rates. Koseet al.[1] proved that\nk-nearest neighbor (k-NN) could obtain a higher classiﬁcation\naccuracy than Na¨ive Bayes (NB) by recognizing four kinds\nof activities, i.e., walking, running, sitting, and standing. Lee\nand Cho [16] designed a mixture-of-experts model for dealing\nwith uncertain and incomplete data. Reyes-Ortiz et al. [17]\nproposed a transition-aware HAR system based on the support\nvector machine (SVM) algorithm to classify a broad spectrum\n(up to 33 types) of activity with smartphone in real-time. Lee\nand Cho [18] presented a hierarchical hidden Markov model\n(HHMM) method to classify motions (standing, walking, go\nstairs, and running) and daily activities (shopping, taking a\nbus, and moving arms). A two-stage continuous hidden Markov\nmodel (HMM) algorithm was proposed to reduce the number of\nfeature subsets because it decomposed the complex activities\ninto several simpler ones [19]. Reiss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions', ' putting the mobile phone in the left pant pocket. The\nHC classiﬁers based onk-NN, SVM, and DT obtain a higher\naccuracy for identifying squats, andk-NN spends less time to\nretrain the classiﬁer. Fig. 6(c) and (d) shows the validation\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 421\nTABLE III\nTHE COMPARATIVERESULTS AMONG HC CLASSIFIER BASED ON K-NN, DT, NB, ANN, SVM,AND LSTM\nFig. 6. Comparison of the results of predicting new activities. The three graphs are the 3-axis acceleration with two directions, the online accuracycomputed\nby the ﬁve ML-based Ada-HAR systems and the online predicting time. The solid line and dotted line are the compressed and uncompressed datasets. The pulse\npoints in the bottom graph are the locations at which the HC classiﬁer was retrained. (a) Squats with the smartphone on the waist. (b) Squats with the smartphone\nin the pocket. (c) Twisting hips with the smartphone on the waist. (d) Twisting hips with the smartphone in the pocket.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n422 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nTABLE IV\nCOMPARISON OF THEFINAL ACCURACY ANDTOTAL TESTING TIME AMONG THE FIVE ML ALGORITHMS OF THEADA-HAR SYSTEM\nresults of predicting twisting hips by carrying the smartphone\non the waist and in the pocket. Table IV reports a comparison of\nthe total classiﬁcation accuracy and average testing time among\nthe Ada-HAR system based on the ﬁve ML algorithms. The\nresults prove that the compressed training dataset can save the\nupdating time for updating the HC classiﬁer, whilek-NN and\nDT algorithms are the best two algorithms.\nC. Demonstration\nThe video experiment of Ada-HAR demonstrates the recog-\nnition process in real time by changing the direction of the\nsmartphone with two positions. The mean transmission time\nof local network is shorter than that of remote connection. For\nexample, it only cost 0.18 s to receive ten new samples in the\nlocal network, while it needed 0.25 s for the remote transmission.\nEach result was obtained as the average of 100 repetitions.\nVI. CONCLUSION\nIn this article, we proposed a smartphone-based adaptive\nrecognition and real-time monitoring system (Ada-HAR) for\nhuman activities. Experiments were performed with 25 subjects\nto validate the performance of the developed system. The most\ninnovative part was the online learning algorithm. It was capable\nof updating the classiﬁer in a dynamic environment, i.e., which\nmeans if any new activity is found, the HC classiﬁer will be\nupdated automatically to include the new class. It is an unsu-\npervised online learning algorithm that does not need to get the\ntrue labels. In addition to the adaptive algorithm, an automatic\nlabeling method using Hk-mC algorithm was another original\nachievement that improves the efﬁciency of labeling the raw\nsignals. The experiments proved its robustness by placing the\nsmartphone on different positions with various directions. The\nintroduced signal preprocessing strategy provides stable inputs\nfor the classiﬁer regardless of the', 'Sα|\nare selected to compute the static features. The meanings of\nthe symbols are as follows: Standard deviationσ, average (e.g.,\n¯∑ Sβ), maximummax, minimummin, difference (recurrence\nrelation) ▽ , and the whole numbers (N).\nB. Creation Module\nThe creation module includes two steps: One aims to achieve\nautomatic labeling using the Hk-mC algorithm, and the other\none is to build the HC classiﬁer for HAR. The Hk-mC algo-\nrithm 1 is implemented in an unsupervised manner with the\nﬁxed structure. It labels the extracted features setFmjj\ns∗p\n,jj =\n1,2,...,Ly m computed on the processed segments s∗\np,p =\n1,2,...,M (lines 2 and 3). For each layer, it calculates a parti-\ntion of a dissimilarity object consisting ofkk ≤ nc medoids.\nGiven a set of M data objects S = sp, the objective is to\npartition the dataset intonc classes, with each cluster having one\nrepresentative object known as a medoidsm\njj [33]. The objective\nfunction of the k-medoids clustering is\nCostkk =\nLym∑\njj=1\n∑\nsp∈M\nDkk(sp,sm\njj) (6)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n418 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 4. Labeled tri-axis acceleration with ﬁxed activity programming.\nTABLE II\nTHE LAYERS ANDFEATURES FORCLUSTERING\nAlgorithm 1: Hierarchical k-medoids Clustering (Hk-mC).\nInput:\nthe raw IMU signalsS, the number of layersLy;\nOutput: labeled signal segments{sp,yp};\n1: divide S into M segments {sp};\n2: acquire the processed signalss∗\np by the algorithms in\nsection IV-A;\n3: extract features Fmjj\ns∗p\nbased on Table II;\n4: label the features and corresponding segments as\n{sp,yp};\nwhere Dkk is the dissimilarity between data object sp and\nmedoid sm\njj associated with cluster jj. In this article,Dkk is\nthe city block distance Dkk(sp,sm\njj)= |sp −sm\njj|. Then, the\nalgorithm clusters the features intokk classes and labels the\nsegments as{sp,yp} (line 4).\nFor clustering accuracy enhancement, the subjects were asked\nto do the 12 activities with a ﬁxed order (shown in Fig. 4). In\naddition, the number of clustering layers and order were ﬁxed\nand are reported in Table II. The mislabeled activities using the\nHk-mC algorithm will be removed or corrected manually.\nHowever, similar segments increase the computing burden for\nbuilding and updating a classiﬁer. To solve this problem for fast\ncomputation, a correlation-based data compression algorithm\nis designed to remove partially similar segments based on the\nsimilarity measurement theory [34]. The data compression algo-\nrithm aims to compare similarity by computing the covariance\ncoefﬁcient ρ (7) between every two matricessa and sb in the\nreconstructed datasetspk,k =1 ,2,...,n c with the same label,\nwhere a,b ∈ R+ and a ̸= b. For saving the compressing time, we\nadopt the seeker optimization algorithm [35] to control the speed\nby setting a trust degreeη.I fρ ≤ η, one']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2680.0,labeled
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q4,Does the predictive model perform regression or classification?,Classification,Classification,Classification,"['Regression', 'Classification']","[11, 10, 5]","[' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the', 'Ld +Lo)=1 ,\n2: identify dynamic or static activity byˆyt = f1(X,θ1)\nbased on featureσ(∑ ||Sα|l|2));\n3: if ˆyt ∈ ΛD, identify a dynamical activity on featuresD\nor segments∗\nt;\n4: else ˆyt ∈ ΛS, identify a static activity on featuresS or\nsegment s∗\nt;\n5: end if\n6: end while\nFor accuracy enhancement, the SVM model commonly utilizes\nthe radial basis function kernel and linear kernel commonly.\nAs a standard RNN method, the LSTM algorithm learns the\nlong-term dependencies based on the three gates, namely an\ninput gate, a forget gate to allow the LSTM unit to unlearn the\nprevious memory, and an output gate to decide the quantity of\nmemory transferring to the next hidden layers. However, it will\ncost ample time to optimize the parameters of the LSTM network\nand predict a result.\nC. Recognition Module\nFig. 5 shows the details of the communication network for\nmonitoring daily human activities in real time with a local LAN\nand a remote LAN. Algorithm 3 describes the procedure for\nidentifying human activities. When a new inputst is divided as a\ndynamical or static activity by the ﬁrst classiﬁerˆyt = f1(X,θ1)\n(line 1), it will be further identiﬁed as a dynamical or a static\nactivity by the second or third classiﬁer (lines 3 and 4).\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n420 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nAlgorithm 4: The online learning algorithm.\nInput: the old HC classiﬁersf(X,θ) and new predicted\ninput {s∗\nt,ˆyt};\nOutput: the updated HC classiﬁersˆf(X, ˆθ);\n1: compute covariance coefﬁcients ρr = corr(s∗\nt,s∗\nqˆyt );\n2: if ρr ≥ η\n3: update training dataset {[s∗\ni;s∗\nt],[yi;nc +1]};\n4: if ˆyt ∈ ΛD\n5: retrain the second classiﬁer ˆf2(X, ˆθ) on\n{F[s∗\ni;s∗\nt],[yi;nc +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n6: else ˆyt ∈ ΛS\n7: retrain the third classiﬁer ˆf3(X, ˆθ) on\n{F[s∗\ni;s∗\nt],[yi;nc +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe', ' ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label the activities au-\ntonomously. The HC [7] algorithms are used to establish the\nclassiﬁer for recognizing the 12 original activities (described in\nSection I). In the recognition module, the obtained HC classiﬁer\nis implemented for HAR in real time by carrying a smartphone\non the waist or putting it in the left pant pocket. In the online\nlearning module, a new activity that is not included in the 12\noriginal activities will be identiﬁed in an unsupervised learning\nmanner. Meanwhile, the old classiﬁer is updated. The signal\npreprocessing and feature extraction models are shared in both\ncreation and recognition modules.\nA. Signal Preprocessing and Feature Extraction\nThe collected signals typically exhibit various turbulence\n(e.g., magnetic ﬁelds and movement artifact), which affects the\nrecognition ability of the classiﬁer. The derived sensors signals\n(e.g., gravity and linear acceleration) that are transferred by the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 417\nFig. 3. Schematic diagram of signal preprocessing model. The 9-D raw signals [Sα (accelerometer), Sβ (gyroscope), andSγ (magnetometer)] are processed\nby the L1-norm (Manhattan distance) transfer, third-order zero phases low-pass elliptical ﬁlter (LPEF), attitude and heading reference system algorithm (AHRS\nﬁlter), and sum of gyroscope signals algorithms to provide the inputs, namely, the linear L1 norm acceleration|Sα|l, YZ-axis L1 norm orientation|S⊖ YZ |,s u m\nof gyroscope\n∑\nSβ, and L1-norm accelerometer signals|Sα|.\noriginal IMU sensors, i.e.,Sα (accelerometer), Sβ (gyroscope),\nand Sγ (magnetometer), can solve the problems.\nFirst, the linear L1-norm accelerometer|Sα| [by (4)] can ﬁx\nthe three axes of acceleration for overcoming the inﬂuence of\nthe changes in direction and position\n||Sα|| =\nLd∑\ni=1\n|sαi | (4)\nwhere sα is the subsegment of Sα with Ld length. Then, a\nthird-order zero phase low-pass elliptical ﬁlter (LPEF) [31] is\nimplemented to decompose|Sα| into the gravity and the linear\nacceleration (|Sα|l) vectors to remove the high-order noises.\nSecond, the attitude and heading reference system algo-\nrithm (AHRS ﬁlter) [32] calculates the orientation axesS⊖ and\ndetermine the smartphone’s reference system to increase the\nidentiﬁcation ability']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2462.0,labeled
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,diagnostic,,,,"[6, 15, 1]","[' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', 'havefocusedonemergencymed-\nicalservicesandtelemedicinebecausethepreciseestimation\nof weight and BMI status in emergency medical care is very\nimportant for accurate counter-shock voltage calculation,\ndrug dosage estimation, intensive care, and elderly trauma\nmanagement [29, 54–57]. Although some issues must be\naddressed for accurate prediction of the BMI status, our\nmethod may have potential applications in telemedicine,\nremote healthcare, and real-time monitoring services to\nmonitor the BMI status of patients with long-term obesity-\nrelated diseases. Additionally, our method can be applied\nin the diagnosis of individual constitution types in remote\nhealthcare.Phametal.suggestedthattheBMIandcheek-to-\njaw width ratio were the most important predictive factors\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n8 Evidence-BasedComplementaryandAlternativeMedicine\nfor the TaeEum (TE) constitution type [58], and Chae et al.\nproposed that the TE type tends to have a higher BMI than\nothertypes[ 59].Furthermore,severalstudiesmentionedthat\nconstitutiontypesdifferedinspeechfeaturesandbodyshape\n(BMI)[60–62].Thus,throughmorestudiesonvoicesignals,\nu-healthcare, body shape, and constitutions, the proposed\nclassification method for BMI can be used to diagnose a\nconstitution for personalized medical care, as the BMI is\nimportantinbothalternativeandWesternmedicines.\n3.4. Limitations and Future Work.In our study, voice data of\nsubjects were collected by a recording equipment in hospital\nsite and research center site. In order to apply real-time\ndiagnosis in telemedicine or u-healthcare system, additional\nand important studies such as noise filter, adjustment tech-\nnique,andhandlingofatypicalspeechinemergency,should\nbe performed because of noise or interference generated by\nnetworkorequipmentduringtelecommunication.\nOur method classified only normal and overweight\nclasses and used voice data collected onlyfromKorea. So, in\norder to more accurately classify a broad range of classes—\nsuch as underweight, normal, overweight, obese 1, obese 2,\nand obese 3—according to WHO standard classification in\nvarious ethnic groups, we must collect more and varied data\nsets.\nIn our classification experiments, the AUC with feature\nselection in the female≥60 group was the highest among all\ngroups,althoughtherewerenosignificantlydifferentfeatures\nbetween the 2 classes among surviving features from the\nfeaturesubsetselectioninthefemale ≥60group.Weconsider\n2aspectsthatcouldberesponsiblefortheoccurrenceofthis\nproblem.First,thiscouldbeduetoacombinationproblemof\nfeaturesinwrapper-basedfeaturesubsetselectionandclassi-\nficationproblems.Fromtheperspectiveofmachinelearning\nanddatamining,machinelearningforwrapper-basedfeature\nselectionisconsideredaperfectblackbox.Ingeneral,greater\nnumbers of features exhibiting significant differences lead to\nbetter machine-learning performance. However, we cannot\nguarantee that a classification using only significant features\n(i.e., those withP values <0.05) always performs better than\none using a combination of significant and less significant\nfeatures.Therefore,themostimportantfactoristheselection\nand combination of the features of each group. For example,\nGuyonandElisseeff[ 43]suggestthattheperformanceofvari-\nables that are ineffective by themselves can be improved sig', ', and cancer [2, 5–\n8]. Therefore, it is important to recognize when patients are\noverweightorobese,andmanystudieshavebeenperformed\nabout the relationship of obesity, as determined by body\nmassindex(BMI),anddisease[ 4,6,7,9–11].BMI,proposed\nby Lambert Adolphe Jacques Quetelet, is a measurement\ncriterion presenting the relationship between body weight\nand height [3]a n dac o m m o n l yu s e dp u b l i ch e a l t hm e t h o d\nfor classifying underweight, normal, overweight, and obese\npatients.\nOn the other hand, research on the association of body\nshape (weight, height), age, and gender with speech signals\nhasbeenconductedoveralongperiodinvariousfieldssuch\nas speech recognition, security technology, and forensic and\nmedical science, and many studies have suggested a strong\norweakrelationshipbetweenbodyshapeandspeechsignals\n[12–28]. Previous analysis of body shape and speech signals\nhasdeterminedthattherearedifferencesbetweennormaland\nobese people in terms of the facial skeleton, the function of\ntheupperairway,andthesurroundingstructureoftheupper\nairway[12],andthatthereisasignificantassociationofbody\nshape with vocal tract length [13]. In various vocal features,\nthe fundamental frequency (pitch) of men was associated\nwith measurements of body shape and size such as chest\ncircumference and shoulder-hip ratio [14]. In more detail,\nEvansetal.suggestedthatthefundamentalfrequencyinmen\nis an indicator of body configuration based on their findings\n\n\n2 Evidence-BasedComplementaryandAlternativeMedicine\nof a significant association of large body shape with low fun-\ndamental frequency and a significantly negative correlation\nbetween weight and fundamental frequency [14]. Lass et al.\n[15, 16] showed a relationship among heights, weights, body\nsurfaceareas,andfundamentalfrequenciesofaspeakerusing\nPearson correlation coefficients, and they suggested acoustic\ncues for accurate estimation of speaker height and weight.\nvan Dommelen and Moxness [17] investigated the ability\nof listeners to determine the weight and height of speakers\nusing average fundamental frequency, energy below 1kHz,\nand speech rate. Although they did not find any significant\ncorrelations between these features and the height or weight\nofthespeaker, they suggested thatspeech rateis a goodpre-\ndictoroftheweightofmalespeakers.Gonz ´alez[18]examined\ntherelationshipbetween formantfrequenciesandtheheight\na n dw e i g h to fs u b j e c t sa g e d2 0t o3 0y e a r si nS p a i na n d\nreportedaweakrelationshipbetweenbodysizeandformant\nfrequencies in adults of the same gender; moreover, the\nrelationship was stronger in women than in men. His results\ncontradictedthoseofFitch,whoreportedastrongcorrelation\nbetween body size and formant dispersions in macaques.\nFurthermore, K¨unzel [19]a n a l y z e dt h er e l a t i o n s h i pb e t w e e n\naveragefundamentalfrequencyandtheweightandheightof\nsubjects in Wiesbaden, Germany, but found no significant\ncorrelations between vocal features and weight or height.\nMeanwhile, in previous studies of the association of gender,\nage, and cultural factor with speech signals, Childers and\nWu [20] studied the automatic recognition of gender using\nvocal features and found that the second formant frequency\nis a slightly better predictor of gender than the fundamental\nfrequency']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2568.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",Unknown from this paper.,,,,"[17, 1, 21]","[' of broad range, rich\ndata,andamoreaccurateclassificationmodel.\nAcknowledgment\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) Grant funded by the Korea government\n(MEST)(20120009001,2006-2005173).\nReferences\n[1] T. J. Parsons, O. Manor, and C. Power, “Physical activity\nand change in body mass index from adolescence to mid-\nadulthood in the 1958 British cohort,”International Journal of\nEpidemiology,vol.35,no .1,p p .197 –204,2006.\n[2] H. Hirose, T. Takayama, S. Hozawa, T. Hibi, and I. Saito, “Pre-\ndiction of metabolic syndrome using artificial neural network\nsystembasedonclinicaldataincludinginsulinresistanceindex\nand serum adiponectin,”Computers in Biology and Medicine,\nvol.41,pp.1051–1056,2011.\n[3] D. Gallagher, M. Visser, D. Sep´u l v e d a ,R.N .P i e r so n ,T .H a rri s ,\nand S. B. Heymsfieid, “How useful is body mass index for\ncomparisonofbodyfatnessacrossage,sex,andethnicgroups?”\nAmerican Journal of Epidemiology,v o l .1 4 3 ,n o .3 ,p p .2 2 8 – 2 3 9 ,\n1996.\n[ 4 ]E .A n u u r a d ,K .S h i w a k u ,A .N o g ie ta l . ,“Th en e wB M Ic r i t e r i a\nf o rA s i a n sb yt h er e g i o n a lo ffi c ef o rt h ew e s t e r np a c i fi cr e g i o n\nof WHO are suitable for screening of overweight to prevent\nmetabolic syndrome in elder Japanese workers,”Journal of\nOccupational Health,vol.45,no.6,pp.335–343,2003.\n[5]L.L.Y a n,M.L.Da vigl us,K.Li uetal.,“ BMIa ndheal th-r ela ted\nqualityoflifeinadults65yearsandolder,” ObesityResearch,vol.\n12,no .1,p p .69–7 6,2004.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 9\n[6] Asia Pacific Cohort Studies Collaboration, “Body mass index\nand cardiovascular disease in the Asia-Pacific Region: an\noverviewof33cohortsinvolving310000participants,” Interna-\ntional Journal of Epidemiology,vol.3 3,p p .7 51 –7 58,2004.\n[ 7 ]C .M .L e e ,S .C o l a g i u r i ,M .E z z a t i ,a n dM .W o o d w a r d ,“ Th e\nburden of cardiovascular disease associated with high body\nmass index in the Asia-Pacific region,”Obesity Reviews,v o l .12 ,\npp.e454–e459,2011.\n[8] L. Li, A. P. De Moira, and C. Power', ', and cancer [2, 5–\n8]. Therefore, it is important to recognize when patients are\noverweightorobese,andmanystudieshavebeenperformed\nabout the relationship of obesity, as determined by body\nmassindex(BMI),anddisease[ 4,6,7,9–11].BMI,proposed\nby Lambert Adolphe Jacques Quetelet, is a measurement\ncriterion presenting the relationship between body weight\nand height [3]a n dac o m m o n l yu s e dp u b l i ch e a l t hm e t h o d\nfor classifying underweight, normal, overweight, and obese\npatients.\nOn the other hand, research on the association of body\nshape (weight, height), age, and gender with speech signals\nhasbeenconductedoveralongperiodinvariousfieldssuch\nas speech recognition, security technology, and forensic and\nmedical science, and many studies have suggested a strong\norweakrelationshipbetweenbodyshapeandspeechsignals\n[12–28]. Previous analysis of body shape and speech signals\nhasdeterminedthattherearedifferencesbetweennormaland\nobese people in terms of the facial skeleton, the function of\ntheupperairway,andthesurroundingstructureoftheupper\nairway[12],andthatthereisasignificantassociationofbody\nshape with vocal tract length [13]. In various vocal features,\nthe fundamental frequency (pitch) of men was associated\nwith measurements of body shape and size such as chest\ncircumference and shoulder-hip ratio [14]. In more detail,\nEvansetal.suggestedthatthefundamentalfrequencyinmen\nis an indicator of body configuration based on their findings\n\n\n2 Evidence-BasedComplementaryandAlternativeMedicine\nof a significant association of large body shape with low fun-\ndamental frequency and a significantly negative correlation\nbetween weight and fundamental frequency [14]. Lass et al.\n[15, 16] showed a relationship among heights, weights, body\nsurfaceareas,andfundamentalfrequenciesofaspeakerusing\nPearson correlation coefficients, and they suggested acoustic\ncues for accurate estimation of speaker height and weight.\nvan Dommelen and Moxness [17] investigated the ability\nof listeners to determine the weight and height of speakers\nusing average fundamental frequency, energy below 1kHz,\nand speech rate. Although they did not find any significant\ncorrelations between these features and the height or weight\nofthespeaker, they suggested thatspeech rateis a goodpre-\ndictoroftheweightofmalespeakers.Gonz ´alez[18]examined\ntherelationshipbetween formantfrequenciesandtheheight\na n dw e i g h to fs u b j e c t sa g e d2 0t o3 0y e a r si nS p a i na n d\nreportedaweakrelationshipbetweenbodysizeandformant\nfrequencies in adults of the same gender; moreover, the\nrelationship was stronger in women than in men. His results\ncontradictedthoseofFitch,whoreportedastrongcorrelation\nbetween body size and formant dispersions in macaques.\nFurthermore, K¨unzel [19]a n a l y z e dt h er e l a t i o n s h i pb e t w e e n\naveragefundamentalfrequencyandtheweightandheightof\nsubjects in Wiesbaden, Germany, but found no significant\ncorrelations between vocal features and weight or height.\nMeanwhile, in previous studies of the association of gender,\nage, and cultural factor with speech signals, Childers and\nWu [20] studied the automatic recognition of gender using\nvocal features and found that the second formant frequency\nis a slightly better predictor of gender than the fundamental\nfrequency', 'en,“Theadoptionofmobileweightmanagementservices\nin a virtual community: the perspective of college students,”\nTelemedicine and e-Health,vol.16,no .4,p p .490–497 ,2010.\n[ 3 5 ]J .R .W a r r e n ,K .J .D a y ,C .P a t o ne ta l . ,“ I m p l e m e n t a t i o n s\nof health information technologies with consumers as users:\nfindingsfromasystematicreview,” HealthCareandInformatics\nReviewOnline,vol.1 4,no .3,p p .2–17 ,2010.\n[36] M. J. Mor´on, A. G´omez-Jaime, J. R. Luque, and E. Casilari,\n“DevelopmentandevaluationofaPythontelecaresystembased\nonaBluetoothBodyAreaNetwork,” EurasipJournalonWireless\nCommunications and Networking, vol. 2011, Article ID 629526,\n2011.\n[37] A. C. Norris, “Scope, Benefits and limitations of telemedicine,”\ninEssentialsofTelemedicineandTelecare ,pp.30–35,JohnWiley\n&Sons,Chichester,UK,2002.\n[38] K. H. Kim, B. Ku, S. Kang, Y. S. Kim, J. S. Jang, and J. Y. Kim,\n“Studyofavocalfeatureselectionmethodandvocalproperties\nfor discriminating four constitution types,” Evidence-Based\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n10 Evidence-BasedComplementaryandAlternativeMedicine\nComplementary and Alternative Medicine,v o l .2 0 1 2 ,A r t i c l eI D\n831543, 10pages,2012.\n[39] WHO Expert Consultation, “Appropriate body-mass index for\nAsianpopulationsanditsimplicationsforpolicyandinterven-\ntionstrategies,” TheLancet,vol.363,p p .157 –163,2004.\n[40] T .Haas,S.Svacina,J.Pav ,R.Hovorka,P .Sucharda,andJ.Sonka,\n“Risk calculation of type 2 diabetes,”Computer Methods and\nPrograms in Biomedicine,vol.41,no .3-4,p p .297 –303,1994.\n[41] K. K. Khokhar, G. Kaur, and S. Sidhu, “Prevalence of obesity\nin working premenopausal and postmenopausal women of\nJalandhar District, Punjab,”Journal of Human Ecology,v o l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2581.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Unknown from this paper.,,,,"[10, 5, 13]","[' 175 0.52 0.325 0.576 0.547\nOverweight 206 0.675 0.48 0.623 0.648\nMale: 40–50 Normal 77 0.377 0.14 0.537 0.443\nOverweight 179 0.86 0.623 0.762 0.808\nMale: 60 Normal 35 0.429 0.239 0.469 0.448\nOverweight 71 0.761 0.571 0.73 0.745\ngender-specific groups without feature selection. A goal of\nthe experiment was to measure the ability to distinguish the\nnormalandtheoverweightineachgroupusingfullfeatures.\nAlso,wewanttoidentifyamorecompactanddiscriminatory\nfeaturesetfordetailedclassificationofeachgroup.Therefore,\ni nt h es e c o n ds t e p ,w ea p p l i e daf e a t u r es u b s e ts e l e c t i o n\nmethod to all data sets used in the first experiment. 12\nclassificationmodelswerebuiltinthefirstandsecondsteps.\n3.1. Performance Evaluations. All of the performances in\nexperiments applied to feature selection (FS-feature sets) in\nage- and gender-specific experiments were superior than\nthose in experiments without feature selection (full-feature\nsets). Figures2 and 3 show that the improvements in AUC\nand accuracy offered by feature selection were statistically\nsignificant.Theaccuraciesforthe6groupsusingfull-feature\nsets ranged from 50.9 to 68.8%. After feature selection, the\naccuraciesforthe6groupsusingFS-featuresetsrangedfrom\n60.4 to 73.8%, and the average accuracy of the 6 groups\nimproved by about 8.4% compared with the use of full-\nfeature sets. The highest accuracy among the groups was\n73.8% (female: 20–30), and the lowest accuracy was 60.4%\n(male:20–30).\nHowever, AUC results based on sensitivity and false\npositive rates (1 specificity) were slightly different from the\naccuracy results. AUC using FS-feature sets ranged from\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n6 Evidence-BasedComplementaryandAlternativeMedicine\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nFemale: 20–30\nFemale: 40–50\nFemale: 60\nMale: 20–30\nMale: 40–50\nMale: 60\n68.8 \n54.8\n62.1 \n50.9 \n59.4 \n54.7 \n73.8 \n60.9 \n69.7 \n60.4 \n71.5 \n65.1 \nAccuracy\nAge- and gender-specific groups\nFull-feature set\nFS-feature set\nFigure2:Accuracycomparisonofexperimentresultsbetweenfull-\nfeaturesetandFS-featuresetin6groups.\n0.591 0.573 0.601 \n0.515 \n0.585 \n0.472 \n0.675 \n0.641 \n0.738 \n0.628 \n0.654 0.645 \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.', ' 23\nor over were labeled as overweight. Underweight patients\nwere passed over due the lack of a minimum number of\nsubjects. Finally, we divided the data set into 6 groups for\nage-andgender-specificclassification:female:20–30(females\naged20–39years),female:40–50(femalesaged40–59years),\nfemale: 60 (females aged 60 years and over), male: 20–30\n(males aged 20–39 years), male: 40–50 (males aged 40–59\nyears),andmale:60(malesaged60yearsandover).\nThe overall mean ages of the female and male subjects\nwere41.79and40.51,respectively.Themeanageandstandard\ndeviationoffemalesaged20–39yearswere28.22and ±6.326,\nand the mean BMI and standard deviation were 21.76 and\n±2.489. The rest of the groups are described inTable 2.Th e\nnumberofnormalandoverweightsubjectsinthe6groupsis\ndescribedin Table 4.\n      \nVoltage \n0.8\n0.2\n0.6\n0.4\n0\n−0.2\n−0.4\n−0.6\n−0.8\n5 1 01 52 0 25 30 35\n(s)\n(a)\nVoltage\n0.8\n0.2\n0.6\n0.4\n0\n−0.2\n−0.4\n−0.6\n−0.8\n(s)\n1.8 2 2.21.6 2.4 2.6\n(b)\nFigure 1: Sample of speech signal recording of 5 vowels and one\nsentence((a):signalsof5vowelsandonesentenceand(b):detailed\nsignalofonevoweltodemonstratethedifferencebetweennoiseand\nsignal).\n2.2. Feature Selection and Experiment Configurations.For\nfeature subset selection, we applied normalization(scale 0∼1\nv a l u e )t oa l ld a t as e t s .Th eW r a p p e r - b a s e df e a t u r es e l e c t i o n\napproach [43, 44] using machine learning of logistic regres-\nsion [30, 45] with genetic search was used to maximize the\narea under ROC curve (AUC). The selected features in each\ngroupareshownin Table 3.Allexperimentswereperformed\nusing logistic regression in Weka [46], and a 10-fold cross\nvalidation was performed [47]. We used the accuracy, true\npositiverate(sensitivity,TPR),falsepositiverate(1specificity,\nFPR), precision, and F measure as performance evaluation\ncriteria [47, 48]. A large proportion of classification algo-\nrithms may not solve the class-size imbalance problem [49].\nThus, the accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the', 'M F C C 4o fv o w e lEa n d\nMFCC4 of vowel O of normal subjects (1.277±6.836 and\n−4.087±5.624, resp.) were higher than those of overweight\nsubjects (−0.801±8.315 and −5.989±7.191,r e s p . )i nt h i s\ngroup. In addition, SITS was significantly different between\nthe 2 classes (𝑃<0.005 ,a d j u s t e d𝑃=0.01 ). This result\nindicates that the average intensity of sentences in normal\nsubjects (56.14±7.515) is higher than that of overweight\nsubjects(53.73±8.074)amongfemalesaged20–39years.\nI nt h em a l e2 0 – 3 0g r o u p ,o n ee M F C C 4f e a t u r ew a s\nsignificantlydifferentbetweenthenormalandtheoverweight\nclasses (𝑃<0.001 ,a d j u s t e d𝑃<0.01 ). The MFCC4 of\nvowelEinnormalsubjectswashigherthanthatofoverweight\nsubjectsinthisgroup.Noneofthefeaturesweresignificantly\ndifferentwithintheothergroups.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 7\nTable 5: Confusion matrix (also called contingency table or error matrix) of 6 models according to age and gender in classification\nexperimentswithfeatureselection.\nGroup\nClassificationmodela\nSubjectsb\nActual Predicted\nOverweight Normal Overweight Normal\nFemale: 20–30 Overweight 30 103 133 364\nNormal 27 337\nFemale: 40–50 Overweight 168 76 244 201\nNormal 98 103\nFemale: 60 Overweight 86 18 104 41\nNormal 26 15\nMale: 20–30 Overweight 139 67 206 175\nNormal 84 91\nMale: 40–50 Overweight 154 25 179 77\nNormal 48 29\nMale: 60 Overweight 54 17 71 35\nNormal 20 15\na\nResultsofconfusionmatrixbyclassificationmodel; bnumberofsubjectsofeachclass(overweightandnormal)inoriginaldata.\nTable6:Statisticalanalysisresultsbyindependenttwosample 𝑡-testandBenjamin-Hochberg’smethod.\nGroup Feature Class Mean Std. 𝑇𝑃 value Adj. 𝑃value\nFemale: 20–30\naF60 120 F240 480 Normal 0.834 0.390 3.474 <0.001 0.005\nOverweight 0.699 0.365\naF240 480 960 1960 Normal 2.285 0.818 3.510 <0.001 0.005\nOverweight 1.996 0.806\naF60 120 960 1960 Normal 2.135 1.416 3.618 <0.001 0.005\nOverweight 1.631 1.248\neF']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q4,How was the dataset described in this study before predictive modeling was performed?,Unknown from this paper.,,,,"[17, 6, 22]","[' of broad range, rich\ndata,andamoreaccurateclassificationmodel.\nAcknowledgment\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) Grant funded by the Korea government\n(MEST)(20120009001,2006-2005173).\nReferences\n[1] T. J. Parsons, O. Manor, and C. Power, “Physical activity\nand change in body mass index from adolescence to mid-\nadulthood in the 1958 British cohort,”International Journal of\nEpidemiology,vol.35,no .1,p p .197 –204,2006.\n[2] H. Hirose, T. Takayama, S. Hozawa, T. Hibi, and I. Saito, “Pre-\ndiction of metabolic syndrome using artificial neural network\nsystembasedonclinicaldataincludinginsulinresistanceindex\nand serum adiponectin,”Computers in Biology and Medicine,\nvol.41,pp.1051–1056,2011.\n[3] D. Gallagher, M. Visser, D. Sep´u l v e d a ,R.N .P i e r so n ,T .H a rri s ,\nand S. B. Heymsfieid, “How useful is body mass index for\ncomparisonofbodyfatnessacrossage,sex,andethnicgroups?”\nAmerican Journal of Epidemiology,v o l .1 4 3 ,n o .3 ,p p .2 2 8 – 2 3 9 ,\n1996.\n[ 4 ]E .A n u u r a d ,K .S h i w a k u ,A .N o g ie ta l . ,“Th en e wB M Ic r i t e r i a\nf o rA s i a n sb yt h er e g i o n a lo ffi c ef o rt h ew e s t e r np a c i fi cr e g i o n\nof WHO are suitable for screening of overweight to prevent\nmetabolic syndrome in elder Japanese workers,”Journal of\nOccupational Health,vol.45,no.6,pp.335–343,2003.\n[5]L.L.Y a n,M.L.Da vigl us,K.Li uetal.,“ BMIa ndheal th-r ela ted\nqualityoflifeinadults65yearsandolder,” ObesityResearch,vol.\n12,no .1,p p .69–7 6,2004.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 9\n[6] Asia Pacific Cohort Studies Collaboration, “Body mass index\nand cardiovascular disease in the Asia-Pacific Region: an\noverviewof33cohortsinvolving310000participants,” Interna-\ntional Journal of Epidemiology,vol.3 3,p p .7 51 –7 58,2004.\n[ 7 ]C .M .L e e ,S .C o l a g i u r i ,M .E z z a t i ,a n dM .W o o d w a r d ,“ Th e\nburden of cardiovascular disease associated with high body\nmass index in the Asia-Pacific region,”Obesity Reviews,v o l .12 ,\npp.e454–e459,2011.\n[8] L. Li, A. P. De Moira, and C. Power', ' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', ' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2549.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?",Unknown from this paper.,,,,"[4, 6, 0]","[' See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 3\nThe recording of the speakers’ speech was strictly con-\ntrolled by a standard operating procedure (SOP). The SOP\nwas established to capture the natural characteristics of the\nspeakers in short recordings. The speakers rested for 1 hour\nbefore actual recording to reduce suspense. An operator\ninstructedthespeakersregardingtherecordingcontent,and\nthespeakerswereaskedtopronouncewordsintheirnormal\ntonewithouttension.Theoperatorconstantlymonitoredthe\nspeakers’ speech and their distance from the microphone\nw h i l er e c o r d i n g .W h e nt h es p e a k e r sc o u l dn o tp r o d u c ea\nuniform tone for 5 vowels, their speech was rerecorded\nuntil they achieved a certain level of tone uniformity. Each\nsentence was recorded twice, and the value of each feature\nwas obtained by averaging the values of the 2 recordings for\nmorestablefeatures.\nAll features were extracted using 5 vowels (A, E, I,\nO, U) and 1 sentence [38]. For speech feature extraction,\nwe extracted 65 features from the collected data set. The\nextracted features consisted of pitch, average ratio of pitch\nperiod, correlation coefficient between F0 and intensity\n(CORR), absolute Jitter (Jita), and Mel frequency cepstral\ncoefficients (MFCC), among others [18, 23, 27]. The specific\ncontent of the extracted features is described in Table 1,\nand sample of speech signal recording of 5 vowels and one\nsentenceisshowedin Figure1.\n2.1.2. Class Label Decision for Normal and Overweight Sta-\ntuses. Obesity and BMI research is difficult due to different\nethnic groups and different national economic statuses [7].\nAlso, BMI values differ according to physiological factors\nand environmental factors, such as residing in a city or a\nrural area. For instance, BMI values of a population in an\nAsian region tend to be lower than those of a population\nin a Western region, whereas Asians have risk factors for\ncardiovascular disease and diabetes related to obesity at\nrelatively low BMI values [9, 39]. The BMI cutoff values for\noverweight and obesity depend on several factors including\nethnicity,rural/urbanresidence,andeconomicstatus[ 7,40].\nTherefore, we decided that this study’s overweight cutoff\npoint of BMI value was≥23kg/m\n2, according to suggestions\nby the World Health Organization and references [39, 41,\n42]. We refer here to only 2 classes: the “normal” and the\n“overweight.” Subjects in the BMI who range from 18.5 to\n22.9 were labeled normal, and subjects with a BMI of 23\nor over were labeled as overweight. Underweight patients\nwere passed over due the lack of a minimum number of\nsubjects. Finally, we divided the data set into 6 groups for\nage-andgender-specificclassification:female:20–30(females\naged20–39years),female:40–50(femalesaged40–59years),\nfemale: 60 (females aged 60 years and over), male: 20–30\n(males aged 20–39 years), male: 40–50 (males aged 40–59\nyears),andmale:60(malesaged60yearsandover).\nThe overall mean ages of the female and male subjects\nwere41.79and40.51,respectively.Themean', ' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', 'Hindawi Publishing Corporation\nEvidence-BasedComplementaryandAlternativeMedicine\nVolume2013,ArticleID150265, 10pages\nhttp://dx.doi.org/10.1155/2013/150265\nResearch Article\nA Novel Method for Classifying Body Mass Index on the Basis of\nSpeech Signals for Future Clinical Applications: A Pilot Study\nBum Ju Lee, Boncho Ku, Jun-Su Jang, and Jong Yeol Kim\nMedical Research Division, Korea Institute of Oriental Medicine, 1672 Yuseongdae-ro, Yuseong-gu,\nDeajeon305-811,RepublicofKorea\nCorrespondenceshouldbeaddressedtoJongYeolKim;ssmed@kiom.re.kr\nReceived5November2012;Revised11January2013;Accepted13January2013\nAcademicEditor:HyunsuBae\nCopyright © 2013 BumJuLeeetal. This is an open access article distributed under the Creative Commons Attribution License,\nwhichpermitsunrestricteduse,distribution,andreproductio ninanymedium,providedtheoriginalworkisproperlycited.\nObesity is a serious public health problem because of the risk factors for diseases and psychological problems. The focus of this\nstudyistodiagnosethepatientBMI(bodymassindex)statuswithoutweightandheightmeasurementsfortheuseinfutureclinical\napplications.Inthispaper,wefirstproposeamethodforclassifyingthenormalandtheoverweightusingonlyspeechsignals.Also,\nweperformastatisticalanalysisofthefeaturesfromspeechsig nals.Basedon1830subjects,theaccuracyandAUC(areaunderthe\nROCcurve)ofage-andgender-specificclassificationsrangedfrom60.4to73.8%andfrom0.628to0.738,respectively.Weidentified\nseveralfeaturesthatweresignificantlydifferentbetweennormalandoverweight subjects( 𝑃<0.05). Also,we foundcompact and\ndiscriminatoryfeaturesubsetsforbuildingmodelsfordiagnosing normaloroverweightindividualsthroughwrapper-basedfeature\nsubset selection. Our results showed that predicting BMI status is possible using a combination of speech features, even though\nsignificantfeaturesarerareandweakinage-andgender-specificgroupsandthattheclassificationaccuracywithfeatureselection\nwas higher than that without feature selection. Our method has the potential to be used in future clinical applications such as\nautomaticBMIdiagnosisintelemedicineorremotehealthcare.\n1. Introduction\nWorldwide, increasing numbers of people are becoming\nobese, including adults, adolescents, and children and both\nmenandwoman[ 1,2].Obesityreferstoexcessadiposetissue\ncausedbygeneticdeterminants,excessiveeating,insufficient\nphysical movement, and an inappropriate lifestyle [1, 3, 4].\nObesity and being overweight are serious public health\nproblems; obesity has a direct relationship with physical\nhealth and psychological health and is a potential risk\nfactor for many diseases, including cardiovascular diseases,\nstroke, ischemic heart disease, diabetes, and cancer [2, 5–\n8]. Therefore, it is important to recognize when patients are\noverweightorobese,andmanystudieshavebeenperformed\nabout the relationship of obesity, as determined by body\nmassindex(BMI),anddisease[ 4,6,7,9–11].BMI,proposed\nby Lambert Adolphe Jacques Quetelet, is a measurement\ncriterion presenting the relationship between body weight\nand height [3]a n dac o m m o n l yu s e dp u b l i ch e a l t hm e t h o d\nfor classifying underweight, normal, overweight, and obese\npatients.\nOn the other hand, research on the association of body\nshape (weight, height),']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2559.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q6,What preprocessing techniques on the included variables/features were applied in this study?,Wrapper-based feature selection; statistical analysis.,,,,"[12, 16, 6]","['overweight”class.\nOur experiments show that classification of normal and\noverweight status in the female: 40–50 and male: 20–30\ngroups was slightly difficult, compared with the other 4\ngroupsandthatclassificationofnormalstatusandoverweight\nstatus in the female: 20–30 and female: 60 groups was\nsuperior compared with the other groups. The classification\nperformancewithwrapper-basedfeatureselectionwasbetter\nthanthatwithoutfeatureselection.Manyoffeaturesselected\nby feature selection differed according to age- and gender-\nspecificgroups(see Table 3).\n3.2. Statistical Analysis of Features Associated with Normal\nWeight and Overweight.The statistical data are expressed as\nmean ±standard deviation. Comparisons between normal\nand overweight groups were performed using independent\ntwo-samplet-tests, and the𝑃values were adjusted using the\nBenjamin-Hochberg method to control the false discovery\nrate; P values <0.05 and adjusted𝑃values <0.05 were con-\nsidered statistically significant. Only statistically significant\nfeaturesamongallfeaturesselectedbywrapper-basedfeature\nsubset selection in each group are described inTable 6.A l l\nstatistical analyses were conducted using SPSS Statistics 19\nandRpackage2.15.0forWindows.\nI nt h ef e m a l e2 0 – 3 0g r o u p ,7f e a t u r e sw e r es i g n i fi -\ncantly different between the normal and the overweight\nclasses (𝑃 < 0.05and adjusted 𝑃 < 0.05). In this\ngroup, aF60\n120 F240 480, aF240480 960 1960, aF60120\n960 1960, and eF240480 960 1960 (features related to the\nratios of energies) were significantly different between the 2\nclasses (𝑃<0.001,a d j u s t e d𝑃=0.005; 𝑃<0.001,a d j u s t e d\n𝑃=0.005 ; 𝑃<0.001 ,a d j u s t e d𝑃=0.005 ;a n d𝑃<0.001 ,\nadjusted𝑃=0.01,resp.).Theseresultsindicatethattheratios\nof voice energies over the fixed frequency band in normal\nsubjects are higher than those of the overweight subjects\nin this group. There were statistically significant differences\nwithrespecttoeMFCC4andoMFCC4betweenthe2classes\n(𝑃<0.05 ,a d j u s t e d𝑃<0.05 ;a n d𝑃<0.01 ,a d j u s t e d\n𝑃<0.05 ,r e s p . ) ;p a r t i c u l a r l y ,t h eM F C C 4o fv o w e lEa n d\nMFCC4 of vowel O of normal subjects (1.277±6.836 and\n−4.087±5.624, resp.) were higher than those of overweight\nsubjects (−0.801±8.315 and −5.989±7.191,r e s p . )i nt h i s\ngroup. In addition, SITS was significantly different between\nthe 2 classes (𝑃<0.005 ,a d j u s t e d𝑃=0.01 ). This result\nindicates that the average intensity of sentences in normal\nsubjects (56.14±7.515) is higher than that of overweight\nsubjects(53.', 'of\nfeaturesinwrapper-basedfeaturesubsetselectionandclassi-\nficationproblems.Fromtheperspectiveofmachinelearning\nanddatamining,machinelearningforwrapper-basedfeature\nselectionisconsideredaperfectblackbox.Ingeneral,greater\nnumbers of features exhibiting significant differences lead to\nbetter machine-learning performance. However, we cannot\nguarantee that a classification using only significant features\n(i.e., those withP values <0.05) always performs better than\none using a combination of significant and less significant\nfeatures.Therefore,themostimportantfactoristheselection\nand combination of the features of each group. For example,\nGuyonandElisseeff[ 43]suggestthattheperformanceofvari-\nables that are ineffective by themselves can be improved sig-\nnificantly when combined with others. Furthermore, adding\npresumablyredundantvariablescanresultinnoisereduction\nand consequently better class separation. The other possible\nreasonfortheobservedproblemisthelackofsamples,which\ncanforceunder-oroverfittinginmachinelearning.Thesmall\nsample size is a critical limitation of this study, because our\nsample size was not representative of the population. Thus,\nthisstudyshouldbedesignatedasapilotobservationalstudy.\nIn order to reduce or understand this problem, we require\nmoresamplesandarecurrentlycollectingmoresamples.\nIn the future, we will investigate the extraction of useful\nfeatures that demonstrate statistical significance in all age-\nand gender-specific groups, build a more accurate classifi-\ncation model, and collect more data for better classification\nperformance. Furthermore, we will examine the association\nof the BMI with features such as respiration rate from\nnonstructuredspeechsignalsusinganewprotocol.\n4. Conclusions\nThe classification of normal and overweight according to\nbody mass index (BMI) is only possible through the mea-\nsurement and calculation of weight and height. This study\nsuggested a novel method for BMI classification by speech\nsignalandshowedthepossibilityofpredictingadiagnosisof\nnormal status or overweight status on the basis of voice and\nmachine learning. We found discriminatory feature subsets\nf o rd i a g n o s i n gn o r m a lo ro v e r w e i g h ti n d i v i d u a l sb yf e a t u r e\nselection. We proved that several features have a statistically\nsignificantdifferencebetweennormalandoverweightclasses\nin the female: 20–30 group and male: 20–30 group through\nstatisticalanalysisofthefeaturesselectedbyfeatureselection\nineachgroup.Ourfindingsshowedthepossibilitytopredict\nBMIdiagnosisusingacombinationofvoicefeatureswithout\nadditional weight and height measurements, even if signifi-\ncant features are rare and weak. The prediction performance\nwith feature selection was higher than that without feature\nselection. However, the accuracy and AUC achieved by our\nclassification experiment were not yet sufficient for rigorous\ndiagnosis and medical purposes. Therefore, we need more\nresearch about discriminatory features of broad range, rich\ndata,andamoreaccurateclassificationmodel.\nAcknowledgment\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) Grant funded by the Korea government\n(MEST)(20120009001,2006-2005173).\nReferences\n[1] T. J. Parsons, O. Manor, and C. Power, “Physical activity\nand change in body mass index from adolescence to mid-\nadulthood in the 1958 British cohort,”International Journal of\nEpidemiology,vol.35,no .1,p p .197 –204,2006.\n[2] H. Hirose, T. Takayama, S. Hozawa, T. Hibi, and I. Saito,', ' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2517.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q7,How is missing data handled in this study?,Unknown from this paper.,,,,"[23, 20, 17]","[' Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.MenonandA.M.Kelly,“Howaccurateisweightestimationin\nthe emergency department?”Emergency Medicine Australasia,\nvol.17,no.2,pp.113–116,2005.\n[57] R. J. Moran, R. B. Reilly, P. De Chazal, and P. D. Lacy,\n“Telephony-basedvoicepathologyassessmentusingautomated\nspeech analysis,”IEEE Transactions on Biomedical Engineering,\nvol.5 3,no .3,p p .468–477 ,2006.\n[ 5 8 ]D .D .P h a m ,J .H .D o ,B .K u ,H .J .L e e ,H .K i m ,a n dJ .Y .\nKim, “Body mass index and facial cues in Sasang typology\nforyoungandelderlypersons,”\nEvidence-BasedComplementary\nandAlternativeMedicine ,vol.2011,ArticleID749209,2011.\n[ 5 9 ]H .C h a e ,I .K .L y o o ,S .J .L e ee ta l . ,“ A na l t e r n a t i v ew a yt o\nindividualized medicine: psychological and physical traits of\nSasang typology,”Journal of Alternative and Complementary\nMedicine,vol.9 ,no .4,p p .519–5 28,2003.\n[60] B.J.Lee,B.Ku,K.Park,K.H.Kim,andJ.Y .Kim,“ Anewmethod\nof diagnosing constitutional types based on vocal and facial\nfeaturesforpersonalizedmedicine,” JournalofBiomedicineand\nBiotechnology,vol.2012,ArticleID818607 ,2012.\n[61] S. W. Lee, E. S. Jang, J. Lee, and J. Y. Kim, “Current researches\nonthemethodsofdiagnosingsasangconstitution:anoverview,”\nEvidence-based Complementary and Alternative Medicine,v o l .\n6,no .1,p p .43–49 ,2009 .\n[ 6 2 ]J .H .D o ,E .S .J a n g ,B .K u ,J .S .J a n g ,H .K i m ,a n dJ .Y .K i m ,\n“Development of an integrated Sasang constitution diagnosis\nmethodusingface,bodyshape,voice,andquestionnaireinfor-\nmation,”BMCComplementaryandAlternativeMedicine ,vol.12,\narticle9,2012.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n', ' 49–160,2009 .\n[25] J. Gonzalez, “Estimation of speakers’ weight and height from\nspeech: a re-analysis of data from multiple studies by lass and\ncolleagues,”Perceptual and Motor Skills,v o l .9 6 ,n o .1 ,p p .2 9 7 –\n304,2003.\n[26] R. Greisbach, “Estimation of speaker height from formant\nfrequencies,”ForensicLinguistics,vol.6,no.2,pp.265–277 ,1999.\n[27] G. Muhammad, T. A. Mesallam, K. H. Malki, M. Farahat, M.\nAlsulaiman, and M. Bukhari, “Formant analysis in dyspho-\nnic patients and automatic Arabic digit speech recognition,”\nBioMedical Engineering OnLine,vol.10,article41,2011.\n[28] K.UmapathyandS.Krishnan,“Featureanalysisofpathological\nspeechsignalsusinglocaldiscriminantbasestechnique,” Medi-\ncalandBiologicalEngineeringandComputing ,vol.43,no.4,pp.\n457–464,2005.\n[29] W. L. Hall, G. L. Larkin, M. J. Trujillo, J. L. Hinds, and K. A.\nDelaney,“Errorsinweightestimationintheemergencydepart-\nment: comparing performance by providers and patients,”\nJournalofEmergencyMedicine ,vol.27 ,no.3,pp.219–224,2004.\n[30] R. S. Lin, S. D. Horn, J. F. Hurdle, and A. S. Goldfarb-\nRumyantzev, “Single and multiple time-point prediction mod-\nelsinkidneytransplantoutcomes,” Journal of Biomedical Infor-\nmatics,vol.41,no .6,p p .944–95 2,2008.\n[31] G. D. Fiol and P. J. Haug, “Classification models for the pre-\ndiction of clinicians’ information needs,”Journal of Biomedical\nInformatics,vol.42,no .1,p p .82–89 ,2009 .\n[32] A. de Leiva, M. E. Hernando, M. Rigla et al., “Telemedi-\ncal artificial pancreas: PARIS (Pancreas Artificial Telemedico\nInteligente) research project,”Diabetes Care,v o l .3 2 ,p p .S 2 1 1 –\n216,2009.\n[33] J. M. Wojcicki, P. Ladyzynski, J. Krzymien et al., “What we can\nreallyexpectfromtelemedicineinintensivediabetestreatment:\nresults from 3-year study on type 1 pregnant diabetic women,”\nDiabetesTechnologyandTherapeutics ,vol.3,no .4,pp .581 –589 ,\n2001.\n[34] W.Y.Jen,“Theadoptionofmobileweightmanagementservices\nin a virtual community: the perspective of college students,”\nTelemedicine and e-Health,vol.16,no .4,p p .490–497 ,2010.\n[ 3 5 ]J .R .W a r r e n ,K .J .D a y ,C .P a t o ne ta l . ,“ I m p l e m e n t a t i o n s\nof health information technologies with consumers as users:\nfindingsfromasystematicreview,” HealthCareandInformatics\nReviewOnline,vol.1 4,no .3,p p .2–17 ,2010.\n[36] M. J. Mor´on, A. G´omez', ' of broad range, rich\ndata,andamoreaccurateclassificationmodel.\nAcknowledgment\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) Grant funded by the Korea government\n(MEST)(20120009001,2006-2005173).\nReferences\n[1] T. J. Parsons, O. Manor, and C. Power, “Physical activity\nand change in body mass index from adolescence to mid-\nadulthood in the 1958 British cohort,”International Journal of\nEpidemiology,vol.35,no .1,p p .197 –204,2006.\n[2] H. Hirose, T. Takayama, S. Hozawa, T. Hibi, and I. Saito, “Pre-\ndiction of metabolic syndrome using artificial neural network\nsystembasedonclinicaldataincludinginsulinresistanceindex\nand serum adiponectin,”Computers in Biology and Medicine,\nvol.41,pp.1051–1056,2011.\n[3] D. Gallagher, M. Visser, D. Sep´u l v e d a ,R.N .P i e r so n ,T .H a rri s ,\nand S. B. Heymsfieid, “How useful is body mass index for\ncomparisonofbodyfatnessacrossage,sex,andethnicgroups?”\nAmerican Journal of Epidemiology,v o l .1 4 3 ,n o .3 ,p p .2 2 8 – 2 3 9 ,\n1996.\n[ 4 ]E .A n u u r a d ,K .S h i w a k u ,A .N o g ie ta l . ,“Th en e wB M Ic r i t e r i a\nf o rA s i a n sb yt h er e g i o n a lo ffi c ef o rt h ew e s t e r np a c i fi cr e g i o n\nof WHO are suitable for screening of overweight to prevent\nmetabolic syndrome in elder Japanese workers,”Journal of\nOccupational Health,vol.45,no.6,pp.335–343,2003.\n[5]L.L.Y a n,M.L.Da vigl us,K.Li uetal.,“ BMIa ndheal th-r ela ted\nqualityoflifeinadults65yearsandolder,” ObesityResearch,vol.\n12,no .1,p p .69–7 6,2004.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 9\n[6] Asia Pacific Cohort Studies Collaboration, “Body mass index\nand cardiovascular disease in the Asia-Pacific Region: an\noverviewof33cohortsinvolving310000participants,” Interna-\ntional Journal of Epidemiology,vol.3 3,p p .7 51 –7 58,2004.\n[ 7 ]C .M .L e e ,S .C o l a g i u r i ,M .E z z a t i ,a n dM .W o o d w a r d ,“ Th e\nburden of cardiovascular disease associated with high body\nmass index in the Asia-Pacific region,”Obesity Reviews,v o l .12 ,\npp.e454–e459,2011.\n[8] L. Li, A. P. De Moira, and C. Power']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2544.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[23, 22, 19]","[' Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.MenonandA.M.Kelly,“Howaccurateisweightestimationin\nthe emergency department?”Emergency Medicine Australasia,\nvol.17,no.2,pp.113–116,2005.\n[57] R. J. Moran, R. B. Reilly, P. De Chazal, and P. D. Lacy,\n“Telephony-basedvoicepathologyassessmentusingautomated\nspeech analysis,”IEEE Transactions on Biomedical Engineering,\nvol.5 3,no .3,p p .468–477 ,2006.\n[ 5 8 ]D .D .P h a m ,J .H .D o ,B .K u ,H .J .L e e ,H .K i m ,a n dJ .Y .\nKim, “Body mass index and facial cues in Sasang typology\nforyoungandelderlypersons,”\nEvidence-BasedComplementary\nandAlternativeMedicine ,vol.2011,ArticleID749209,2011.\n[ 5 9 ]H .C h a e ,I .K .L y o o ,S .J .L e ee ta l . ,“ A na l t e r n a t i v ew a yt o\nindividualized medicine: psychological and physical traits of\nSasang typology,”Journal of Alternative and Complementary\nMedicine,vol.9 ,no .4,p p .519–5 28,2003.\n[60] B.J.Lee,B.Ku,K.Park,K.H.Kim,andJ.Y .Kim,“ Anewmethod\nof diagnosing constitutional types based on vocal and facial\nfeaturesforpersonalizedmedicine,” JournalofBiomedicineand\nBiotechnology,vol.2012,ArticleID818607 ,2012.\n[61] S. W. Lee, E. S. Jang, J. Lee, and J. Y. Kim, “Current researches\nonthemethodsofdiagnosingsasangconstitution:anoverview,”\nEvidence-based Complementary and Alternative Medicine,v o l .\n6,no .1,p p .43–49 ,2009 .\n[ 6 2 ]J .H .D o ,E .S .J a n g ,B .K u ,J .S .J a n g ,H .K i m ,a n dJ .Y .K i m ,\n“Development of an integrated Sasang constitution diagnosis\nmethodusingface,bodyshape,voice,andquestionnaireinfor-\nmation,”BMCComplementaryandAlternativeMedicine ,vol.12,\narticle9,2012.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n', ' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon', ' characteristics and body size and shape in human males:\nan evolutionary explanation for a deep male voice,”Biological\nPsychology,vol.7 2,no .2,p p .160–163,2006.\n[15] N. J. Lass, “Correlational study of speakers’ heights, weights,\nbody surface areas, and speaking fundamental frequencies,”\nJ o u r n a lo ft h eA c o u s t i c a lS o c i e t yo fA m e r i c a,v o l .6 3 ,n o .4 ,p p .\n1218–1220,1978.\n[16] N.J.Lass,J.K.Phillips,andC.A.Bruchey ,“Theeffectoffiltered\nspeech on speaker height and weight identification,”Journal of\nPhonetics,vol.8,p p .91 –100,1980.\n[17] W. A. van Dommelen and B. H. Moxness, “Acoustic param-\neters in speaker height and weight identification: sex-specific\nbehaviour,”Language and speech,v o l .3 8 ,n o .3 ,p p .2 6 7 – 2 8 7 ,\n1995.\n[18] J. Gonz´alez, “Formant frequencies and body size of speaker: a\nweak relationship in adult humans,”Journal of Phonetics,v o l .\n3 2,no .2,p p .277 –287 ,2004.\n[19] H. J. Kunzel, “How well does average fundamental frequency\ncorrelate with speaker height and weight?”Phonetica,v o l .4 6 ,\nno.1–3,pp.117–125,1989.\n[20] D. G. Childers and K. Wu, “Gender recognition from speech.\nPart II: fine analysis,” Journal of the Acoustical Society of\nAmerica,vol.90,p p .1841 –1856,1991.\n[21] L. Bruckert, J. S. Li´enard, A. Lacroix, M. Kreutzer, and G.\nLeboucher, “Women use voice parameters to assess men’s\ncharacteristics,”Proceedings of the Royal Society,v o l .2 7 3 ,n o .\n1582,pp.83–89,2006.\n[22] P.Belin,S.Fecteau,andC.B ´edard,“Thinkingthevoice:neural\ncorrelatesofvoiceperception,” TrendsinCognitiveSciences ,vol.\n8,no.3,pp.129–135,2004.\n[23] M. Jessen, “Speaker classification in forensic phonetics and\nacoustics,” inSpeaker Classification I, pp. 180–204, Springer,\nBerlin,Germany,2007.\n[24] I. Mporas and T. Ganchev, “Estimation of unknown speaker’s\nheightfromspeech,” InternationalJournalofSpeechTechnology ,\nvol.12,no .4,p p .1 49–160,2009 .\n[25] J. Gonzalez, “Estimation of speakers’ weight and height from\nspeech: a re-analysis of data from multiple studies by lass and\ncolleagues,”Perceptual and Motor Skills,v o l .9 6 ,n o .1 ,p p .2 9 7 –\n304,2003.\n[26] R. Greisbach, “Estimation of speaker height from formant\nfrequencies,”ForensicLinguistics,vol.6,no.2,pp.265–277 ,1999.\n[27] G. Muhammad, T. A. Mesallam, K. H. Malki, M. Farahat, M.\nAlsulaiman, and M. B']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2545.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q9,Which prediction models were used in this study?,Unknown from this paper.,,,,"[23, 22, 15]","[' Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.MenonandA.M.Kelly,“Howaccurateisweightestimationin\nthe emergency department?”Emergency Medicine Australasia,\nvol.17,no.2,pp.113–116,2005.\n[57] R. J. Moran, R. B. Reilly, P. De Chazal, and P. D. Lacy,\n“Telephony-basedvoicepathologyassessmentusingautomated\nspeech analysis,”IEEE Transactions on Biomedical Engineering,\nvol.5 3,no .3,p p .468–477 ,2006.\n[ 5 8 ]D .D .P h a m ,J .H .D o ,B .K u ,H .J .L e e ,H .K i m ,a n dJ .Y .\nKim, “Body mass index and facial cues in Sasang typology\nforyoungandelderlypersons,”\nEvidence-BasedComplementary\nandAlternativeMedicine ,vol.2011,ArticleID749209,2011.\n[ 5 9 ]H .C h a e ,I .K .L y o o ,S .J .L e ee ta l . ,“ A na l t e r n a t i v ew a yt o\nindividualized medicine: psychological and physical traits of\nSasang typology,”Journal of Alternative and Complementary\nMedicine,vol.9 ,no .4,p p .519–5 28,2003.\n[60] B.J.Lee,B.Ku,K.Park,K.H.Kim,andJ.Y .Kim,“ Anewmethod\nof diagnosing constitutional types based on vocal and facial\nfeaturesforpersonalizedmedicine,” JournalofBiomedicineand\nBiotechnology,vol.2012,ArticleID818607 ,2012.\n[61] S. W. Lee, E. S. Jang, J. Lee, and J. Y. Kim, “Current researches\nonthemethodsofdiagnosingsasangconstitution:anoverview,”\nEvidence-based Complementary and Alternative Medicine,v o l .\n6,no .1,p p .43–49 ,2009 .\n[ 6 2 ]J .H .D o ,E .S .J a n g ,B .K u ,J .S .J a n g ,H .K i m ,a n dJ .Y .K i m ,\n“Development of an integrated Sasang constitution diagnosis\nmethodusingface,bodyshape,voice,andquestionnaireinfor-\nmation,”BMCComplementaryandAlternativeMedicine ,vol.12,\narticle9,2012.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n', ' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon', 'havefocusedonemergencymed-\nicalservicesandtelemedicinebecausethepreciseestimation\nof weight and BMI status in emergency medical care is very\nimportant for accurate counter-shock voltage calculation,\ndrug dosage estimation, intensive care, and elderly trauma\nmanagement [29, 54–57]. Although some issues must be\naddressed for accurate prediction of the BMI status, our\nmethod may have potential applications in telemedicine,\nremote healthcare, and real-time monitoring services to\nmonitor the BMI status of patients with long-term obesity-\nrelated diseases. Additionally, our method can be applied\nin the diagnosis of individual constitution types in remote\nhealthcare.Phametal.suggestedthattheBMIandcheek-to-\njaw width ratio were the most important predictive factors\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n8 Evidence-BasedComplementaryandAlternativeMedicine\nfor the TaeEum (TE) constitution type [58], and Chae et al.\nproposed that the TE type tends to have a higher BMI than\nothertypes[ 59].Furthermore,severalstudiesmentionedthat\nconstitutiontypesdifferedinspeechfeaturesandbodyshape\n(BMI)[60–62].Thus,throughmorestudiesonvoicesignals,\nu-healthcare, body shape, and constitutions, the proposed\nclassification method for BMI can be used to diagnose a\nconstitution for personalized medical care, as the BMI is\nimportantinbothalternativeandWesternmedicines.\n3.4. Limitations and Future Work.In our study, voice data of\nsubjects were collected by a recording equipment in hospital\nsite and research center site. In order to apply real-time\ndiagnosis in telemedicine or u-healthcare system, additional\nand important studies such as noise filter, adjustment tech-\nnique,andhandlingofatypicalspeechinemergency,should\nbe performed because of noise or interference generated by\nnetworkorequipmentduringtelecommunication.\nOur method classified only normal and overweight\nclasses and used voice data collected onlyfromKorea. So, in\norder to more accurately classify a broad range of classes—\nsuch as underweight, normal, overweight, obese 1, obese 2,\nand obese 3—according to WHO standard classification in\nvarious ethnic groups, we must collect more and varied data\nsets.\nIn our classification experiments, the AUC with feature\nselection in the female≥60 group was the highest among all\ngroups,althoughtherewerenosignificantlydifferentfeatures\nbetween the 2 classes among surviving features from the\nfeaturesubsetselectioninthefemale ≥60group.Weconsider\n2aspectsthatcouldberesponsiblefortheoccurrenceofthis\nproblem.First,thiscouldbeduetoacombinationproblemof\nfeaturesinwrapper-basedfeaturesubsetselectionandclassi-\nficationproblems.Fromtheperspectiveofmachinelearning\nanddatamining,machinelearningforwrapper-basedfeature\nselectionisconsideredaperfectblackbox.Ingeneral,greater\nnumbers of features exhibiting significant differences lead to\nbetter machine-learning performance. However, we cannot\nguarantee that a classification using only significant features\n(i.e., those withP values <0.05) always performs better than\none using a combination of significant and less significant\nfeatures.Therefore,themostimportantfactoristheselection\nand combination of the features of each group. For example,\nGuyonandElisseeff[ 43]suggestthattheperformanceofvari-\nables that are ineffective by themselves can be improved sig']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2553.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,Unknown from this paper.,,,,"[22, 15, 23]","[' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon', 'havefocusedonemergencymed-\nicalservicesandtelemedicinebecausethepreciseestimation\nof weight and BMI status in emergency medical care is very\nimportant for accurate counter-shock voltage calculation,\ndrug dosage estimation, intensive care, and elderly trauma\nmanagement [29, 54–57]. Although some issues must be\naddressed for accurate prediction of the BMI status, our\nmethod may have potential applications in telemedicine,\nremote healthcare, and real-time monitoring services to\nmonitor the BMI status of patients with long-term obesity-\nrelated diseases. Additionally, our method can be applied\nin the diagnosis of individual constitution types in remote\nhealthcare.Phametal.suggestedthattheBMIandcheek-to-\njaw width ratio were the most important predictive factors\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n8 Evidence-BasedComplementaryandAlternativeMedicine\nfor the TaeEum (TE) constitution type [58], and Chae et al.\nproposed that the TE type tends to have a higher BMI than\nothertypes[ 59].Furthermore,severalstudiesmentionedthat\nconstitutiontypesdifferedinspeechfeaturesandbodyshape\n(BMI)[60–62].Thus,throughmorestudiesonvoicesignals,\nu-healthcare, body shape, and constitutions, the proposed\nclassification method for BMI can be used to diagnose a\nconstitution for personalized medical care, as the BMI is\nimportantinbothalternativeandWesternmedicines.\n3.4. Limitations and Future Work.In our study, voice data of\nsubjects were collected by a recording equipment in hospital\nsite and research center site. In order to apply real-time\ndiagnosis in telemedicine or u-healthcare system, additional\nand important studies such as noise filter, adjustment tech-\nnique,andhandlingofatypicalspeechinemergency,should\nbe performed because of noise or interference generated by\nnetworkorequipmentduringtelecommunication.\nOur method classified only normal and overweight\nclasses and used voice data collected onlyfromKorea. So, in\norder to more accurately classify a broad range of classes—\nsuch as underweight, normal, overweight, obese 1, obese 2,\nand obese 3—according to WHO standard classification in\nvarious ethnic groups, we must collect more and varied data\nsets.\nIn our classification experiments, the AUC with feature\nselection in the female≥60 group was the highest among all\ngroups,althoughtherewerenosignificantlydifferentfeatures\nbetween the 2 classes among surviving features from the\nfeaturesubsetselectioninthefemale ≥60group.Weconsider\n2aspectsthatcouldberesponsiblefortheoccurrenceofthis\nproblem.First,thiscouldbeduetoacombinationproblemof\nfeaturesinwrapper-basedfeaturesubsetselectionandclassi-\nficationproblems.Fromtheperspectiveofmachinelearning\nanddatamining,machinelearningforwrapper-basedfeature\nselectionisconsideredaperfectblackbox.Ingeneral,greater\nnumbers of features exhibiting significant differences lead to\nbetter machine-learning performance. However, we cannot\nguarantee that a classification using only significant features\n(i.e., those withP values <0.05) always performs better than\none using a combination of significant and less significant\nfeatures.Therefore,themostimportantfactoristheselection\nand combination of the features of each group. For example,\nGuyonandElisseeff[ 43]suggestthattheperformanceofvari-\nables that are ineffective by themselves can be improved sig', ' Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.MenonandA.M.Kelly,“Howaccurateisweightestimationin\nthe emergency department?”Emergency Medicine Australasia,\nvol.17,no.2,pp.113–116,2005.\n[57] R. J. Moran, R. B. Reilly, P. De Chazal, and P. D. Lacy,\n“Telephony-basedvoicepathologyassessmentusingautomated\nspeech analysis,”IEEE Transactions on Biomedical Engineering,\nvol.5 3,no .3,p p .468–477 ,2006.\n[ 5 8 ]D .D .P h a m ,J .H .D o ,B .K u ,H .J .L e e ,H .K i m ,a n dJ .Y .\nKim, “Body mass index and facial cues in Sasang typology\nforyoungandelderlypersons,”\nEvidence-BasedComplementary\nandAlternativeMedicine ,vol.2011,ArticleID749209,2011.\n[ 5 9 ]H .C h a e ,I .K .L y o o ,S .J .L e ee ta l . ,“ A na l t e r n a t i v ew a yt o\nindividualized medicine: psychological and physical traits of\nSasang typology,”Journal of Alternative and Complementary\nMedicine,vol.9 ,no .4,p p .519–5 28,2003.\n[60] B.J.Lee,B.Ku,K.Park,K.H.Kim,andJ.Y .Kim,“ Anewmethod\nof diagnosing constitutional types based on vocal and facial\nfeaturesforpersonalizedmedicine,” JournalofBiomedicineand\nBiotechnology,vol.2012,ArticleID818607 ,2012.\n[61] S. W. Lee, E. S. Jang, J. Lee, and J. Y. Kim, “Current researches\nonthemethodsofdiagnosingsasangconstitution:anoverview,”\nEvidence-based Complementary and Alternative Medicine,v o l .\n6,no .1,p p .43–49 ,2009 .\n[ 6 2 ]J .H .D o ,E .S .J a n g ,B .K u ,J .S .J a n g ,H .K i m ,a n dJ .Y .K i m ,\n“Development of an integrated Sasang constitution diagnosis\nmethodusingface,bodyshape,voice,andquestionnaireinfor-\nmation,”BMCComplementaryandAlternativeMedicine ,vol.12,\narticle9,2012.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2559.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q11,How was data augmentation or generation used in this study?,Unknown from this paper,,,,"[14, 5, 19]","['.\nGroup Feature Class Mean Std. 𝑇𝑃 value Adj. 𝑃value\nFemale: 20–30\naF60 120 F240 480 Normal 0.834 0.390 3.474 <0.001 0.005\nOverweight 0.699 0.365\naF240 480 960 1960 Normal 2.285 0.818 3.510 <0.001 0.005\nOverweight 1.996 0.806\naF60 120 960 1960 Normal 2.135 1.416 3.618 <0.001 0.005\nOverweight 1.631 1.248\neF240 480 960 1960 Normal 3.033 0.627 3.342 <0.001 <0.01\nOverweight 2.818 0.660\neMFCC4 Normal 1.277 6.836 2.581 <0.05 <0.05\nOverweight −0.801 8.315\noMFCC4 Normal −4.087 5.624 2.757 <0.01 <0.05\nOverweight −5.989 7.191\nSITS Normal 56.14 7.515 3.106 <0.005 0.01\nOverweight 53.73 8.074\nMale: 20–30 eMFCC4 Normal 5.057 6.678 3.393 <0.001 <0.01\nOverweight 2.679 6.929\n𝑃value<0.05wasconsideredstatisticallysignificant.The 𝑃valueswereadjustedusingtheBenjamin-Hochbergmethodtocontrolthefalsediscoveryrate.Only\ns t a t i s t i c a l l ys i g n i fi c a n tf e a t u r e sa m o n ga l lf e a t u r e ss e l e c t e db yw r a p p e r - b a s e df e a t u r es u b s e ts e l e c t i o ni ne a c hg r o u pa r ed e s c r i b e di nt h i st a b l e(Std: standard\ndeviation,Adj:adjusted).\nDespite the high accuracy and AUC of classification in\nthe female≥60 group, no statistically significant differences\nwere detected between the normal and overweight classes.\nF u r t h e r m o r e ,w ed i dn o tfi n df e a t u r e sw i t hab r o a dr a n g e\nof applicability for classifying the normal and overweight\nstatuses in the age-or gender-specific classifications. We will\ndiscusstheseproblemsfurtherin Section3.4.\n3.3. Scalability and Applications.Some studies on patient\nBMIandweightestimationhavefocusedonemergencymed-\nicalservicesandtelemedicinebecausethepreciseestimation\nof weight and BMI status in emergency medical care is very\nimportant for accurate counter-shock voltage calculation,\ndrug dosage estimation, intensive care, and elderly trauma\nmanagement [29, 54–57]. Although some issues must be\naddressed for accurate prediction of the BMI status, our\nmethod may have potential applications in telemedicine,\nremote healthcare, and real-time monitoring services to\nmonitor the BMI status of patients with long-term obesity-\nrelated diseases. Additionally, our method can be applied\nin the diagnosis of individual constitution types in remote\nhealthcare.Phametal.suggestedthattheBMIandcheek-to-\njaw width ratio were the most important predictive factors\n ', ' 23\nor over were labeled as overweight. Underweight patients\nwere passed over due the lack of a minimum number of\nsubjects. Finally, we divided the data set into 6 groups for\nage-andgender-specificclassification:female:20–30(females\naged20–39years),female:40–50(femalesaged40–59years),\nfemale: 60 (females aged 60 years and over), male: 20–30\n(males aged 20–39 years), male: 40–50 (males aged 40–59\nyears),andmale:60(malesaged60yearsandover).\nThe overall mean ages of the female and male subjects\nwere41.79and40.51,respectively.Themeanageandstandard\ndeviationoffemalesaged20–39yearswere28.22and ±6.326,\nand the mean BMI and standard deviation were 21.76 and\n±2.489. The rest of the groups are described inTable 2.Th e\nnumberofnormalandoverweightsubjectsinthe6groupsis\ndescribedin Table 4.\n      \nVoltage \n0.8\n0.2\n0.6\n0.4\n0\n−0.2\n−0.4\n−0.6\n−0.8\n5 1 01 52 0 25 30 35\n(s)\n(a)\nVoltage\n0.8\n0.2\n0.6\n0.4\n0\n−0.2\n−0.4\n−0.6\n−0.8\n(s)\n1.8 2 2.21.6 2.4 2.6\n(b)\nFigure 1: Sample of speech signal recording of 5 vowels and one\nsentence((a):signalsof5vowelsandonesentenceand(b):detailed\nsignalofonevoweltodemonstratethedifferencebetweennoiseand\nsignal).\n2.2. Feature Selection and Experiment Configurations.For\nfeature subset selection, we applied normalization(scale 0∼1\nv a l u e )t oa l ld a t as e t s .Th eW r a p p e r - b a s e df e a t u r es e l e c t i o n\napproach [43, 44] using machine learning of logistic regres-\nsion [30, 45] with genetic search was used to maximize the\narea under ROC curve (AUC). The selected features in each\ngroupareshownin Table 3.Allexperimentswereperformed\nusing logistic regression in Weka [46], and a 10-fold cross\nvalidation was performed [47]. We used the accuracy, true\npositiverate(sensitivity,TPR),falsepositiverate(1specificity,\nFPR), precision, and F measure as performance evaluation\ncriteria [47, 48]. A large proportion of classification algo-\nrithms may not solve the class-size imbalance problem [49].\nThus, the accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the', ' characteristics and body size and shape in human males:\nan evolutionary explanation for a deep male voice,”Biological\nPsychology,vol.7 2,no .2,p p .160–163,2006.\n[15] N. J. Lass, “Correlational study of speakers’ heights, weights,\nbody surface areas, and speaking fundamental frequencies,”\nJ o u r n a lo ft h eA c o u s t i c a lS o c i e t yo fA m e r i c a,v o l .6 3 ,n o .4 ,p p .\n1218–1220,1978.\n[16] N.J.Lass,J.K.Phillips,andC.A.Bruchey ,“Theeffectoffiltered\nspeech on speaker height and weight identification,”Journal of\nPhonetics,vol.8,p p .91 –100,1980.\n[17] W. A. van Dommelen and B. H. Moxness, “Acoustic param-\neters in speaker height and weight identification: sex-specific\nbehaviour,”Language and speech,v o l .3 8 ,n o .3 ,p p .2 6 7 – 2 8 7 ,\n1995.\n[18] J. Gonz´alez, “Formant frequencies and body size of speaker: a\nweak relationship in adult humans,”Journal of Phonetics,v o l .\n3 2,no .2,p p .277 –287 ,2004.\n[19] H. J. Kunzel, “How well does average fundamental frequency\ncorrelate with speaker height and weight?”Phonetica,v o l .4 6 ,\nno.1–3,pp.117–125,1989.\n[20] D. G. Childers and K. Wu, “Gender recognition from speech.\nPart II: fine analysis,” Journal of the Acoustical Society of\nAmerica,vol.90,p p .1841 –1856,1991.\n[21] L. Bruckert, J. S. Li´enard, A. Lacroix, M. Kreutzer, and G.\nLeboucher, “Women use voice parameters to assess men’s\ncharacteristics,”Proceedings of the Royal Society,v o l .2 7 3 ,n o .\n1582,pp.83–89,2006.\n[22] P.Belin,S.Fecteau,andC.B ´edard,“Thinkingthevoice:neural\ncorrelatesofvoiceperception,” TrendsinCognitiveSciences ,vol.\n8,no.3,pp.129–135,2004.\n[23] M. Jessen, “Speaker classification in forensic phonetics and\nacoustics,” inSpeaker Classification I, pp. 180–204, Springer,\nBerlin,Germany,2007.\n[24] I. Mporas and T. Ganchev, “Estimation of unknown speaker’s\nheightfromspeech,” InternationalJournalofSpeechTechnology ,\nvol.12,no .4,p p .1 49–160,2009 .\n[25] J. Gonzalez, “Estimation of speakers’ weight and height from\nspeech: a re-analysis of data from multiple studies by lass and\ncolleagues,”Perceptual and Motor Skills,v o l .9 6 ,n o .1 ,p p .2 9 7 –\n304,2003.\n[26] R. Greisbach, “Estimation of speaker height from formant\nfrequencies,”ForensicLinguistics,vol.6,no.2,pp.265–277 ,1999.\n[27] G. Muhammad, T. A. Mesallam, K. H. Malki, M. Farahat, M.\nAlsulaiman, and M. B']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,,,,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared with and without feature selection.",,,,"[6, 12, 11]","[' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', 'overweight”class.\nOur experiments show that classification of normal and\noverweight status in the female: 40–50 and male: 20–30\ngroups was slightly difficult, compared with the other 4\ngroupsandthatclassificationofnormalstatusandoverweight\nstatus in the female: 20–30 and female: 60 groups was\nsuperior compared with the other groups. The classification\nperformancewithwrapper-basedfeatureselectionwasbetter\nthanthatwithoutfeatureselection.Manyoffeaturesselected\nby feature selection differed according to age- and gender-\nspecificgroups(see Table 3).\n3.2. Statistical Analysis of Features Associated with Normal\nWeight and Overweight.The statistical data are expressed as\nmean ±standard deviation. Comparisons between normal\nand overweight groups were performed using independent\ntwo-samplet-tests, and the𝑃values were adjusted using the\nBenjamin-Hochberg method to control the false discovery\nrate; P values <0.05 and adjusted𝑃values <0.05 were con-\nsidered statistically significant. Only statistically significant\nfeaturesamongallfeaturesselectedbywrapper-basedfeature\nsubset selection in each group are described inTable 6.A l l\nstatistical analyses were conducted using SPSS Statistics 19\nandRpackage2.15.0forWindows.\nI nt h ef e m a l e2 0 – 3 0g r o u p ,7f e a t u r e sw e r es i g n i fi -\ncantly different between the normal and the overweight\nclasses (𝑃 < 0.05and adjusted 𝑃 < 0.05). In this\ngroup, aF60\n120 F240 480, aF240480 960 1960, aF60120\n960 1960, and eF240480 960 1960 (features related to the\nratios of energies) were significantly different between the 2\nclasses (𝑃<0.001,a d j u s t e d𝑃=0.005; 𝑃<0.001,a d j u s t e d\n𝑃=0.005 ; 𝑃<0.001 ,a d j u s t e d𝑃=0.005 ;a n d𝑃<0.001 ,\nadjusted𝑃=0.01,resp.).Theseresultsindicatethattheratios\nof voice energies over the fixed frequency band in normal\nsubjects are higher than those of the overweight subjects\nin this group. There were statistically significant differences\nwithrespecttoeMFCC4andoMFCC4betweenthe2classes\n(𝑃<0.05 ,a d j u s t e d𝑃<0.05 ;a n d𝑃<0.01 ,a d j u s t e d\n𝑃<0.05 ,r e s p . ) ;p a r t i c u l a r l y ,t h eM F C C 4o fv o w e lEa n d\nMFCC4 of vowel O of normal subjects (1.277±6.836 and\n−4.087±5.624, resp.) were higher than those of overweight\nsubjects (−0.801±8.315 and −5.989±7.191,r e s p . )i nt h i s\ngroup. In addition, SITS was significantly different between\nthe 2 classes (𝑃<0.005 ,a d j u s t e d𝑃=0.01 ). This result\nindicates that the average intensity of sentences in normal\nsubjects (56.14±7.515) is higher than that of overweight\nsubjects(53.', '60\n68.8 \n54.8\n62.1 \n50.9 \n59.4 \n54.7 \n73.8 \n60.9 \n69.7 \n60.4 \n71.5 \n65.1 \nAccuracy\nAge- and gender-specific groups\nFull-feature set\nFS-feature set\nFigure2:Accuracycomparisonofexperimentresultsbetweenfull-\nfeaturesetandFS-featuresetin6groups.\n0.591 0.573 0.601 \n0.515 \n0.585 \n0.472 \n0.675 \n0.641 \n0.738 \n0.628 \n0.654 0.645 \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAUC\nFull-feature set\nFS-feature setFemale: 20–30\nFemale: 40–50\nFemale: 60\nMale: 20–30\nMale: 40–50\nMale: 60\nAge- and gender-specific groups\nFigure 3: AUC comparison of experiment results between full-\nfeaturesetandFS-featuresetin6groups.\n0.628to0.738.Theaccuracyofthefemale:60groupwaslower\nthan that of female: 20–30 and male: 40–50, but the AUC of\nfemale: 60 was the highest among the 6 groups. The specific\nperformance results of female: 60 using the FS-feature set\nincluded a sensitivity of 0.366, FPR of 0.173, precision of\n0.455, and F measure of 0.405 in the normal weight class\nand a sensitivity of 0.827, FPR of 0.634, precision of 0.768,\na n dFm e a s u r eo f0 . 7 9 6i nt h eo v e r w e i g h tc l a s s .Th el o w e s t\nAUCof0.628wasobservedinthemale:20–30group.Specific\nexperimentresultsofallgroupsaredescribedin Table 4.\nTheconfusionma trix(alsocalledacon tingencytable)in\nTable 5describes more detailed performances of 6 models\naccording to age and gender. For example, the classification\nmodel of the female: 20–30 group correctly predicted that\n337 of 364 subjects with actual normal weight belonged to\nt h e“ n o r m a l ”c l a s sa n dt h a t3 0o f1 3 3s u b j e c t sw i t ha c t u a l\noverweight belonged to the “overweight” class. Moreover,\nthe female: 40–50 model correctly predicted that 103 of 201\nsubjectswithactualnormalweightbelongedtothe“normal”\nclass and that 168 of 244 subjects with actual overweight\nbelongedtothe“overweight”class.\nOur experiments show that classification of normal and\noverweight status in the female: 40–50 and male: 20–30\ngroups was slightly difficult, compared with the other 4\ngroupsandthatclassificationofnormalstatusandoverweight\nstatus in the female: 20–30 and female: 60 groups was\nsuperior compared with the other groups. The classification\nperformancewithwrapper-basedfeatureselectionwasbetter\nthanthatwithoutfeatureselection.Manyoffeaturesselected\nby feature selection differed according to age- and gender-\nspecificgroups(see Table 3).\n3.2. Statistical Analysis of Features Associated with Normal\nWeight and Overweight.The statistical data are expressed as\nmean ±standard deviation. Comparisons between normal\nand overweight groups were']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2531.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q13,Which type of explainability techniques are used?,Unknown from this paper.,,,,"[10, 6, 11]","[' 175 0.52 0.325 0.576 0.547\nOverweight 206 0.675 0.48 0.623 0.648\nMale: 40–50 Normal 77 0.377 0.14 0.537 0.443\nOverweight 179 0.86 0.623 0.762 0.808\nMale: 60 Normal 35 0.429 0.239 0.469 0.448\nOverweight 71 0.761 0.571 0.73 0.745\ngender-specific groups without feature selection. A goal of\nthe experiment was to measure the ability to distinguish the\nnormalandtheoverweightineachgroupusingfullfeatures.\nAlso,wewanttoidentifyamorecompactanddiscriminatory\nfeaturesetfordetailedclassificationofeachgroup.Therefore,\ni nt h es e c o n ds t e p ,w ea p p l i e daf e a t u r es u b s e ts e l e c t i o n\nmethod to all data sets used in the first experiment. 12\nclassificationmodelswerebuiltinthefirstandsecondsteps.\n3.1. Performance Evaluations. All of the performances in\nexperiments applied to feature selection (FS-feature sets) in\nage- and gender-specific experiments were superior than\nthose in experiments without feature selection (full-feature\nsets). Figures2 and 3 show that the improvements in AUC\nand accuracy offered by feature selection were statistically\nsignificant.Theaccuraciesforthe6groupsusingfull-feature\nsets ranged from 50.9 to 68.8%. After feature selection, the\naccuraciesforthe6groupsusingFS-featuresetsrangedfrom\n60.4 to 73.8%, and the average accuracy of the 6 groups\nimproved by about 8.4% compared with the use of full-\nfeature sets. The highest accuracy among the groups was\n73.8% (female: 20–30), and the lowest accuracy was 60.4%\n(male:20–30).\nHowever, AUC results based on sensitivity and false\npositive rates (1 specificity) were slightly different from the\naccuracy results. AUC using FS-feature sets ranged from\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n6 Evidence-BasedComplementaryandAlternativeMedicine\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nFemale: 20–30\nFemale: 40–50\nFemale: 60\nMale: 20–30\nMale: 40–50\nMale: 60\n68.8 \n54.8\n62.1 \n50.9 \n59.4 \n54.7 \n73.8 \n60.9 \n69.7 \n60.4 \n71.5 \n65.1 \nAccuracy\nAge- and gender-specific groups\nFull-feature set\nFS-feature set\nFigure2:Accuracycomparisonofexperimentresultsbetweenfull-\nfeaturesetandFS-featuresetin6groups.\n0.591 0.573 0.601 \n0.515 \n0.585 \n0.472 \n0.675 \n0.641 \n0.738 \n0.628 \n0.654 0.645 \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.', ' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', '60\n68.8 \n54.8\n62.1 \n50.9 \n59.4 \n54.7 \n73.8 \n60.9 \n69.7 \n60.4 \n71.5 \n65.1 \nAccuracy\nAge- and gender-specific groups\nFull-feature set\nFS-feature set\nFigure2:Accuracycomparisonofexperimentresultsbetweenfull-\nfeaturesetandFS-featuresetin6groups.\n0.591 0.573 0.601 \n0.515 \n0.585 \n0.472 \n0.675 \n0.641 \n0.738 \n0.628 \n0.654 0.645 \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAUC\nFull-feature set\nFS-feature setFemale: 20–30\nFemale: 40–50\nFemale: 60\nMale: 20–30\nMale: 40–50\nMale: 60\nAge- and gender-specific groups\nFigure 3: AUC comparison of experiment results between full-\nfeaturesetandFS-featuresetin6groups.\n0.628to0.738.Theaccuracyofthefemale:60groupwaslower\nthan that of female: 20–30 and male: 40–50, but the AUC of\nfemale: 60 was the highest among the 6 groups. The specific\nperformance results of female: 60 using the FS-feature set\nincluded a sensitivity of 0.366, FPR of 0.173, precision of\n0.455, and F measure of 0.405 in the normal weight class\nand a sensitivity of 0.827, FPR of 0.634, precision of 0.768,\na n dFm e a s u r eo f0 . 7 9 6i nt h eo v e r w e i g h tc l a s s .Th el o w e s t\nAUCof0.628wasobservedinthemale:20–30group.Specific\nexperimentresultsofallgroupsaredescribedin Table 4.\nTheconfusionma trix(alsocalledacon tingencytable)in\nTable 5describes more detailed performances of 6 models\naccording to age and gender. For example, the classification\nmodel of the female: 20–30 group correctly predicted that\n337 of 364 subjects with actual normal weight belonged to\nt h e“ n o r m a l ”c l a s sa n dt h a t3 0o f1 3 3s u b j e c t sw i t ha c t u a l\noverweight belonged to the “overweight” class. Moreover,\nthe female: 40–50 model correctly predicted that 103 of 201\nsubjectswithactualnormalweightbelongedtothe“normal”\nclass and that 168 of 244 subjects with actual overweight\nbelongedtothe“overweight”class.\nOur experiments show that classification of normal and\noverweight status in the female: 40–50 and male: 20–30\ngroups was slightly difficult, compared with the other 4\ngroupsandthatclassificationofnormalstatusandoverweight\nstatus in the female: 20–30 and female: 60 groups was\nsuperior compared with the other groups. The classification\nperformancewithwrapper-basedfeatureselectionwasbetter\nthanthatwithoutfeatureselection.Manyoffeaturesselected\nby feature selection differed according to age- and gender-\nspecificgroups(see Table 3).\n3.2. Statistical Analysis of Features Associated with Normal\nWeight and Overweight.The statistical data are expressed as\nmean ±standard deviation. Comparisons between normal\nand overweight groups were']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2558.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,Accuracy and AUC,,,,"[6, 22, 16]","[' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', ' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon', 'of\nfeaturesinwrapper-basedfeaturesubsetselectionandclassi-\nficationproblems.Fromtheperspectiveofmachinelearning\nanddatamining,machinelearningforwrapper-basedfeature\nselectionisconsideredaperfectblackbox.Ingeneral,greater\nnumbers of features exhibiting significant differences lead to\nbetter machine-learning performance. However, we cannot\nguarantee that a classification using only significant features\n(i.e., those withP values <0.05) always performs better than\none using a combination of significant and less significant\nfeatures.Therefore,themostimportantfactoristheselection\nand combination of the features of each group. For example,\nGuyonandElisseeff[ 43]suggestthattheperformanceofvari-\nables that are ineffective by themselves can be improved sig-\nnificantly when combined with others. Furthermore, adding\npresumablyredundantvariablescanresultinnoisereduction\nand consequently better class separation. The other possible\nreasonfortheobservedproblemisthelackofsamples,which\ncanforceunder-oroverfittinginmachinelearning.Thesmall\nsample size is a critical limitation of this study, because our\nsample size was not representative of the population. Thus,\nthisstudyshouldbedesignatedasapilotobservationalstudy.\nIn order to reduce or understand this problem, we require\nmoresamplesandarecurrentlycollectingmoresamples.\nIn the future, we will investigate the extraction of useful\nfeatures that demonstrate statistical significance in all age-\nand gender-specific groups, build a more accurate classifi-\ncation model, and collect more data for better classification\nperformance. Furthermore, we will examine the association\nof the BMI with features such as respiration rate from\nnonstructuredspeechsignalsusinganewprotocol.\n4. Conclusions\nThe classification of normal and overweight according to\nbody mass index (BMI) is only possible through the mea-\nsurement and calculation of weight and height. This study\nsuggested a novel method for BMI classification by speech\nsignalandshowedthepossibilityofpredictingadiagnosisof\nnormal status or overweight status on the basis of voice and\nmachine learning. We found discriminatory feature subsets\nf o rd i a g n o s i n gn o r m a lo ro v e r w e i g h ti n d i v i d u a l sb yf e a t u r e\nselection. We proved that several features have a statistically\nsignificantdifferencebetweennormalandoverweightclasses\nin the female: 20–30 group and male: 20–30 group through\nstatisticalanalysisofthefeaturesselectedbyfeatureselection\nineachgroup.Ourfindingsshowedthepossibilitytopredict\nBMIdiagnosisusingacombinationofvoicefeatureswithout\nadditional weight and height measurements, even if signifi-\ncant features are rare and weak. The prediction performance\nwith feature selection was higher than that without feature\nselection. However, the accuracy and AUC achieved by our\nclassification experiment were not yet sufficient for rigorous\ndiagnosis and medical purposes. Therefore, we need more\nresearch about discriminatory features of broad range, rich\ndata,andamoreaccurateclassificationmodel.\nAcknowledgment\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) Grant funded by the Korea government\n(MEST)(20120009001,2006-2005173).\nReferences\n[1] T. J. Parsons, O. Manor, and C. Power, “Physical activity\nand change in body mass index from adolescence to mid-\nadulthood in the 1958 British cohort,”International Journal of\nEpidemiology,vol.35,no .1,p p .197 –204,2006.\n[2] H. Hirose, T. Takayama, S. Hozawa, T. Hibi, and I. Saito,']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2547.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,AUC and accuracy were used.,,,,"[6, 10, 22]","[' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', ' 175 0.52 0.325 0.576 0.547\nOverweight 206 0.675 0.48 0.623 0.648\nMale: 40–50 Normal 77 0.377 0.14 0.537 0.443\nOverweight 179 0.86 0.623 0.762 0.808\nMale: 60 Normal 35 0.429 0.239 0.469 0.448\nOverweight 71 0.761 0.571 0.73 0.745\ngender-specific groups without feature selection. A goal of\nthe experiment was to measure the ability to distinguish the\nnormalandtheoverweightineachgroupusingfullfeatures.\nAlso,wewanttoidentifyamorecompactanddiscriminatory\nfeaturesetfordetailedclassificationofeachgroup.Therefore,\ni nt h es e c o n ds t e p ,w ea p p l i e daf e a t u r es u b s e ts e l e c t i o n\nmethod to all data sets used in the first experiment. 12\nclassificationmodelswerebuiltinthefirstandsecondsteps.\n3.1. Performance Evaluations. All of the performances in\nexperiments applied to feature selection (FS-feature sets) in\nage- and gender-specific experiments were superior than\nthose in experiments without feature selection (full-feature\nsets). Figures2 and 3 show that the improvements in AUC\nand accuracy offered by feature selection were statistically\nsignificant.Theaccuraciesforthe6groupsusingfull-feature\nsets ranged from 50.9 to 68.8%. After feature selection, the\naccuraciesforthe6groupsusingFS-featuresetsrangedfrom\n60.4 to 73.8%, and the average accuracy of the 6 groups\nimproved by about 8.4% compared with the use of full-\nfeature sets. The highest accuracy among the groups was\n73.8% (female: 20–30), and the lowest accuracy was 60.4%\n(male:20–30).\nHowever, AUC results based on sensitivity and false\npositive rates (1 specificity) were slightly different from the\naccuracy results. AUC using FS-feature sets ranged from\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n6 Evidence-BasedComplementaryandAlternativeMedicine\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nFemale: 20–30\nFemale: 40–50\nFemale: 60\nMale: 20–30\nMale: 40–50\nMale: 60\n68.8 \n54.8\n62.1 \n50.9 \n59.4 \n54.7 \n73.8 \n60.9 \n69.7 \n60.4 \n71.5 \n65.1 \nAccuracy\nAge- and gender-specific groups\nFull-feature set\nFS-feature set\nFigure2:Accuracycomparisonofexperimentresultsbetweenfull-\nfeaturesetandFS-featuresetin6groups.\n0.591 0.573 0.601 \n0.515 \n0.585 \n0.472 \n0.675 \n0.641 \n0.738 \n0.628 \n0.654 0.645 \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.', ' l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S. le Cessie and J. C. van Houwelingen, “Ridge estimators in\nlogistic regression,”Applied Statistics,v o l .4 1 ,n o .1 ,p p .1 9 1 – 2 0 1 ,\n1992.\n[46] H. Ian, Data Mining: Practical Machine Learning Tools and\nTechniques,MorganKaufmann,SanFrancisco,Calif,USA,2nd\nedition,2005.\n[47] J. Han and M. Kamber,Data Mining: Concepts and Techniques,\nMorgan Kaufmann, San Francisco, Calif, USA, 2nd edition,\n2006.\n[48] B. J. Lee, M. S. Shin, Y. J. Oh, H. S. Oh, and K. H. Ryu,\n“Identification of protein functions using a machine-learning\napproach based on sequence-derived properties,” Proteome\nScience,vol.7 ,article27 ,2009 .\n[49] P. N. Tan, M. Steinbach, and V. Kumar,Introduction to Data\nMining,AddisonWesley,Boston,Mass,USA,2006.\n[50] J.HuangandC.X.Ling,“UsingAUCandaccuracyinevaluating\nlearningalgorithms,”IEEETransactionsonKnowledgeandData\nEngineering,vol.17 ,p p .299–3 10,2005.\n[51] D.J.Hand,“Evaluatingdiagnostictests:theareaundertheROC\nc u r v ea n dt h eb a l a n c eo fe r r o r s , ”Statistics in Medicine,v o l .2 9 ,\nno .1 4,p p .150 2–1510,2010.\n[52] C.E.Metz,“ROCanalysisinmedicalimaging:atutorialreview\nof the literature,”Radiological physics and technology,v o l .1 ,n o .\n1,pp.2–12,2008.\n[53] R. Kumar and A. Indrayan, “Receiver operating characteristic\n(ROC)curveformedicalresearchers,” IndianPediatrics,vol.48,\nno.4,pp.277–287,2011.\n[54] D. Krieser, K. Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.Menon']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2557.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?",Using AUC and ROC curves.,,,,"[6, 23, 17]","[' accuracy of many classification experiments is\nhigherforamajorityclassthanforaminorityclass.Therefore,\nwe also evaluated performance using AUC. An ROC curve\n(receiver operating characteristic curve) represents the bal-\nance of sensitivity versus 1 specificity [50]. Because the AUC\nis a threshold-independent measure, AUC is a widely used\ntoquantifythequalityofapredictionorclassificationmodel\nin medical science, bioinformatics, medicine statistics, and\nbiology [31, 51–53]. An AUC of 1 means a perfect diagnosis\nmodel,anAUCof0.5israndomdiagnosis,andanAUCof0\nisaperfectlywrongdiagnosis.\n3. Results and Discussion\nOur experiments were divided into two steps. In the first\nexperiment, we conducted classification of normal and\noverweight classes with six data sets according to age-and\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Evidence-BasedComplementaryandAlternativeMedicine\nTable1:Allfeaturesusedinthisstudyandbriefdescriptions.\nFeature Briefdescription Feature Briefdescription\naF0 BasicpitchofA oPPQ SmoothingvaluearoundJITAofO\naJITA MeanratioofchangeinpitchperiodofA oF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofO\naJITT PercentageofJITAvalueofA oF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofO\naPPQ SmoothingvaluearoundJITAofA oF60 120 oF960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofO\naF60 120 F240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofA oF1 Formant of first in 4 frequency periods of O\naF240 480 960 1920(energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofA oF2 F o rman to fseco ndin4fr equency\nperiodsofO\naF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960∼1920Hz) ofA oF2 F1 Differenceoffrequencies(oF2-F1)\naF1 Formant of first in 4 frequency periods of A uF0 Basic pitch of U\naF2 Formantofsecondin4frequencyperiodsofA uJITA Mean ratio of change in pitch period of U\naF2\nF1 aF2/F1 uJITT PercentageofJITAvalueofU\neF0 BasicpitchofE uPPQ SmoothingvaluearoundJITAofU\neJITA MeanratioofchangeinpitchperiodofE uF60 120 240 480 (energyof60 ∼120Hz)/(energyof\n240∼480Hz)ofU\neJITT PercentageofJITAvalueofE uF240 480 960 1920 (energyof240 ∼480Hz)/(energyof\n960∼1920Hz) ofU\nePPQ SmoothingvaluearoundJITAofE uF60 120 960 1920 (energyof60 ∼120Hz)/(energyof\n960�', ' Nguyen, D. Kerr, D. Jolley, M. Clooney, and A.\nM. Kelly, “Parental weight estimation of their child’s weight\nis more accurate than other weight estimation methods for\ndetermining children’s weight in an emergency department?”\nEmergency Medicine Journal,vol.2 4,no .11,p p .7 56–7 59 ,2007 .\n[55] T. R. Coe, M. Halkes, K. Houghton, and D. Jefferson, “The\naccuracy of visual estimation of weight and height in pre-\noperative supine patients,”Anaesthesia,v o l .5 4 ,n o .6 ,p p .5 8 2 –\n586,1999.\n[56] S.MenonandA.M.Kelly,“Howaccurateisweightestimationin\nthe emergency department?”Emergency Medicine Australasia,\nvol.17,no.2,pp.113–116,2005.\n[57] R. J. Moran, R. B. Reilly, P. De Chazal, and P. D. Lacy,\n“Telephony-basedvoicepathologyassessmentusingautomated\nspeech analysis,”IEEE Transactions on Biomedical Engineering,\nvol.5 3,no .3,p p .468–477 ,2006.\n[ 5 8 ]D .D .P h a m ,J .H .D o ,B .K u ,H .J .L e e ,H .K i m ,a n dJ .Y .\nKim, “Body mass index and facial cues in Sasang typology\nforyoungandelderlypersons,”\nEvidence-BasedComplementary\nandAlternativeMedicine ,vol.2011,ArticleID749209,2011.\n[ 5 9 ]H .C h a e ,I .K .L y o o ,S .J .L e ee ta l . ,“ A na l t e r n a t i v ew a yt o\nindividualized medicine: psychological and physical traits of\nSasang typology,”Journal of Alternative and Complementary\nMedicine,vol.9 ,no .4,p p .519–5 28,2003.\n[60] B.J.Lee,B.Ku,K.Park,K.H.Kim,andJ.Y .Kim,“ Anewmethod\nof diagnosing constitutional types based on vocal and facial\nfeaturesforpersonalizedmedicine,” JournalofBiomedicineand\nBiotechnology,vol.2012,ArticleID818607 ,2012.\n[61] S. W. Lee, E. S. Jang, J. Lee, and J. Y. Kim, “Current researches\nonthemethodsofdiagnosingsasangconstitution:anoverview,”\nEvidence-based Complementary and Alternative Medicine,v o l .\n6,no .1,p p .43–49 ,2009 .\n[ 6 2 ]J .H .D o ,E .S .J a n g ,B .K u ,J .S .J a n g ,H .K i m ,a n dJ .Y .K i m ,\n“Development of an integrated Sasang constitution diagnosis\nmethodusingface,bodyshape,voice,andquestionnaireinfor-\nmation,”BMCComplementaryandAlternativeMedicine ,vol.12,\narticle9,2012.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n', ' of broad range, rich\ndata,andamoreaccurateclassificationmodel.\nAcknowledgment\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) Grant funded by the Korea government\n(MEST)(20120009001,2006-2005173).\nReferences\n[1] T. J. Parsons, O. Manor, and C. Power, “Physical activity\nand change in body mass index from adolescence to mid-\nadulthood in the 1958 British cohort,”International Journal of\nEpidemiology,vol.35,no .1,p p .197 –204,2006.\n[2] H. Hirose, T. Takayama, S. Hozawa, T. Hibi, and I. Saito, “Pre-\ndiction of metabolic syndrome using artificial neural network\nsystembasedonclinicaldataincludinginsulinresistanceindex\nand serum adiponectin,”Computers in Biology and Medicine,\nvol.41,pp.1051–1056,2011.\n[3] D. Gallagher, M. Visser, D. Sep´u l v e d a ,R.N .P i e r so n ,T .H a rri s ,\nand S. B. Heymsfieid, “How useful is body mass index for\ncomparisonofbodyfatnessacrossage,sex,andethnicgroups?”\nAmerican Journal of Epidemiology,v o l .1 4 3 ,n o .3 ,p p .2 2 8 – 2 3 9 ,\n1996.\n[ 4 ]E .A n u u r a d ,K .S h i w a k u ,A .N o g ie ta l . ,“Th en e wB M Ic r i t e r i a\nf o rA s i a n sb yt h er e g i o n a lo ffi c ef o rt h ew e s t e r np a c i fi cr e g i o n\nof WHO are suitable for screening of overweight to prevent\nmetabolic syndrome in elder Japanese workers,”Journal of\nOccupational Health,vol.45,no.6,pp.335–343,2003.\n[5]L.L.Y a n,M.L.Da vigl us,K.Li uetal.,“ BMIa ndheal th-r ela ted\nqualityoflifeinadults65yearsandolder,” ObesityResearch,vol.\n12,no .1,p p .69–7 6,2004.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 9\n[6] Asia Pacific Cohort Studies Collaboration, “Body mass index\nand cardiovascular disease in the Asia-Pacific Region: an\noverviewof33cohortsinvolving310000participants,” Interna-\ntional Journal of Epidemiology,vol.3 3,p p .7 51 –7 58,2004.\n[ 7 ]C .M .L e e ,S .C o l a g i u r i ,M .E z z a t i ,a n dM .W o o d w a r d ,“ Th e\nburden of cardiovascular disease associated with high body\nmass index in the Asia-Pacific region,”Obesity Reviews,v o l .12 ,\npp.e454–e459,2011.\n[8] L. Li, A. P. De Moira, and C. Power']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2535.0,free_text
A_Novel_Method_for_Classifying_Body_Mass_Index_on_the_Basis_of_Speech_Signals_for_Future_Clinical_Applications_A_Pilot_Study,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[17, 21, 19]","[' of broad range, rich\ndata,andamoreaccurateclassificationmodel.\nAcknowledgment\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) Grant funded by the Korea government\n(MEST)(20120009001,2006-2005173).\nReferences\n[1] T. J. Parsons, O. Manor, and C. Power, “Physical activity\nand change in body mass index from adolescence to mid-\nadulthood in the 1958 British cohort,”International Journal of\nEpidemiology,vol.35,no .1,p p .197 –204,2006.\n[2] H. Hirose, T. Takayama, S. Hozawa, T. Hibi, and I. Saito, “Pre-\ndiction of metabolic syndrome using artificial neural network\nsystembasedonclinicaldataincludinginsulinresistanceindex\nand serum adiponectin,”Computers in Biology and Medicine,\nvol.41,pp.1051–1056,2011.\n[3] D. Gallagher, M. Visser, D. Sep´u l v e d a ,R.N .P i e r so n ,T .H a rri s ,\nand S. B. Heymsfieid, “How useful is body mass index for\ncomparisonofbodyfatnessacrossage,sex,andethnicgroups?”\nAmerican Journal of Epidemiology,v o l .1 4 3 ,n o .3 ,p p .2 2 8 – 2 3 9 ,\n1996.\n[ 4 ]E .A n u u r a d ,K .S h i w a k u ,A .N o g ie ta l . ,“Th en e wB M Ic r i t e r i a\nf o rA s i a n sb yt h er e g i o n a lo ffi c ef o rt h ew e s t e r np a c i fi cr e g i o n\nof WHO are suitable for screening of overweight to prevent\nmetabolic syndrome in elder Japanese workers,”Journal of\nOccupational Health,vol.45,no.6,pp.335–343,2003.\n[5]L.L.Y a n,M.L.Da vigl us,K.Li uetal.,“ BMIa ndheal th-r ela ted\nqualityoflifeinadults65yearsandolder,” ObesityResearch,vol.\n12,no .1,p p .69–7 6,2004.\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nEvidence-BasedComplementaryandAlternativeMedicine 9\n[6] Asia Pacific Cohort Studies Collaboration, “Body mass index\nand cardiovascular disease in the Asia-Pacific Region: an\noverviewof33cohortsinvolving310000participants,” Interna-\ntional Journal of Epidemiology,vol.3 3,p p .7 51 –7 58,2004.\n[ 7 ]C .M .L e e ,S .C o l a g i u r i ,M .E z z a t i ,a n dM .W o o d w a r d ,“ Th e\nburden of cardiovascular disease associated with high body\nmass index in the Asia-Pacific region,”Obesity Reviews,v o l .12 ,\npp.e454–e459,2011.\n[8] L. Li, A. P. De Moira, and C. Power', 'en,“Theadoptionofmobileweightmanagementservices\nin a virtual community: the perspective of college students,”\nTelemedicine and e-Health,vol.16,no .4,p p .490–497 ,2010.\n[ 3 5 ]J .R .W a r r e n ,K .J .D a y ,C .P a t o ne ta l . ,“ I m p l e m e n t a t i o n s\nof health information technologies with consumers as users:\nfindingsfromasystematicreview,” HealthCareandInformatics\nReviewOnline,vol.1 4,no .3,p p .2–17 ,2010.\n[36] M. J. Mor´on, A. G´omez-Jaime, J. R. Luque, and E. Casilari,\n“DevelopmentandevaluationofaPythontelecaresystembased\nonaBluetoothBodyAreaNetwork,” EurasipJournalonWireless\nCommunications and Networking, vol. 2011, Article ID 629526,\n2011.\n[37] A. C. Norris, “Scope, Benefits and limitations of telemedicine,”\ninEssentialsofTelemedicineandTelecare ,pp.30–35,JohnWiley\n&Sons,Chichester,UK,2002.\n[38] K. H. Kim, B. Ku, S. Kang, Y. S. Kim, J. S. Jang, and J. Y. Kim,\n“Studyofavocalfeatureselectionmethodandvocalproperties\nfor discriminating four constitution types,” Evidence-Based\n 4747, 2013, 1, Downloaded from https://onlinelibrary.wiley.com/doi/10.1155/2013/150265, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n10 Evidence-BasedComplementaryandAlternativeMedicine\nComplementary and Alternative Medicine,v o l .2 0 1 2 ,A r t i c l eI D\n831543, 10pages,2012.\n[39] WHO Expert Consultation, “Appropriate body-mass index for\nAsianpopulationsanditsimplicationsforpolicyandinterven-\ntionstrategies,” TheLancet,vol.363,p p .157 –163,2004.\n[40] T .Haas,S.Svacina,J.Pav ,R.Hovorka,P .Sucharda,andJ.Sonka,\n“Risk calculation of type 2 diabetes,”Computer Methods and\nPrograms in Biomedicine,vol.41,no .3-4,p p .297 –303,1994.\n[41] K. K. Khokhar, G. Kaur, and S. Sidhu, “Prevalence of obesity\nin working premenopausal and postmenopausal women of\nJalandhar District, Punjab,”Journal of Human Ecology,v o l .2 9 ,\np p .57 –6 2,2010.\n[42] World Health Organisation, The Asia-Pacific Perspective:\nRedefiningObesityandItsTreatment ,HealthCommunications,\nSydney,Australia,2000.\n[43] I. Guyon and A. Elisseeff, “An introduction to variable and\nfeature selection,”Journal of Machine Learning Research,v o l .3 ,\npp.1157–1182,2003.\n[44] R. Kohavi and G. H. John, “Wrappers for feature subset\nselection,” Artificial Intelligence,v o l .9 7 ,n o .1 - 2 ,p p .2 7 3 – 3 2 4 ,\n1997.\n[45] S', ' characteristics and body size and shape in human males:\nan evolutionary explanation for a deep male voice,”Biological\nPsychology,vol.7 2,no .2,p p .160–163,2006.\n[15] N. J. Lass, “Correlational study of speakers’ heights, weights,\nbody surface areas, and speaking fundamental frequencies,”\nJ o u r n a lo ft h eA c o u s t i c a lS o c i e t yo fA m e r i c a,v o l .6 3 ,n o .4 ,p p .\n1218–1220,1978.\n[16] N.J.Lass,J.K.Phillips,andC.A.Bruchey ,“Theeffectoffiltered\nspeech on speaker height and weight identification,”Journal of\nPhonetics,vol.8,p p .91 –100,1980.\n[17] W. A. van Dommelen and B. H. Moxness, “Acoustic param-\neters in speaker height and weight identification: sex-specific\nbehaviour,”Language and speech,v o l .3 8 ,n o .3 ,p p .2 6 7 – 2 8 7 ,\n1995.\n[18] J. Gonz´alez, “Formant frequencies and body size of speaker: a\nweak relationship in adult humans,”Journal of Phonetics,v o l .\n3 2,no .2,p p .277 –287 ,2004.\n[19] H. J. Kunzel, “How well does average fundamental frequency\ncorrelate with speaker height and weight?”Phonetica,v o l .4 6 ,\nno.1–3,pp.117–125,1989.\n[20] D. G. Childers and K. Wu, “Gender recognition from speech.\nPart II: fine analysis,” Journal of the Acoustical Society of\nAmerica,vol.90,p p .1841 –1856,1991.\n[21] L. Bruckert, J. S. Li´enard, A. Lacroix, M. Kreutzer, and G.\nLeboucher, “Women use voice parameters to assess men’s\ncharacteristics,”Proceedings of the Royal Society,v o l .2 7 3 ,n o .\n1582,pp.83–89,2006.\n[22] P.Belin,S.Fecteau,andC.B ´edard,“Thinkingthevoice:neural\ncorrelatesofvoiceperception,” TrendsinCognitiveSciences ,vol.\n8,no.3,pp.129–135,2004.\n[23] M. Jessen, “Speaker classification in forensic phonetics and\nacoustics,” inSpeaker Classification I, pp. 180–204, Springer,\nBerlin,Germany,2007.\n[24] I. Mporas and T. Ganchev, “Estimation of unknown speaker’s\nheightfromspeech,” InternationalJournalofSpeechTechnology ,\nvol.12,no .4,p p .1 49–160,2009 .\n[25] J. Gonzalez, “Estimation of speakers’ weight and height from\nspeech: a re-analysis of data from multiple studies by lass and\ncolleagues,”Perceptual and Motor Skills,v o l .9 6 ,n o .1 ,p p .2 9 7 –\n304,2003.\n[26] R. Greisbach, “Estimation of speaker height from formant\nfrequencies,”ForensicLinguistics,vol.6,no.2,pp.265–277 ,1999.\n[27] G. Muhammad, T. A. Mesallam, K. H. Malki, M. Farahat, M.\nAlsulaiman, and M. B']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2559.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,Diagnostic.,,,,"[7, 12, 3]","['3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN', 'iﬁcant impact on the anal-\nysis of artiﬁcial intelligence. Therefore, the future work will\ngive more priority to the improvement of the hardware design\nand the efﬁciency of image acquisition device, and further work\nwill focus on improving algorithm efﬁciency and recognition\nrate, reducing false alarm rate, and setting up a larger image\ndata set for dental diseases.\nV. C ONCLUSION\nThis paper proposes an iHome smart dental Health-IoT sys-\ntem based on intelligent hardware, deep learning and mobile\nterminal, aiming to regulate as well as optimize the accessibil-\nity of dental treatment and provide home-based dental health\ncare service more efciently. The trained model was used to\nrealize the detection and classication of dental diseases, and\napplication software (Apps) on mobile terminal was designed\nfor client-side and dentist-side. The software platform with the\nfunctions including pre-examination of dental disease, consulta-\ntion, appointment, and evaluation, etc, made the service docking\nbetween the patient and the dentist resources a reality. The AI\nalgorithm achieved more than 90% recognition rate for seven\ndental diseases., which has greatly improved the patient rate and\nthe resource utilization rate of the dental clinic through a one-\nmonth systematic testing in 10 private dental clinics, showing\nhigh reliability in practical application.\nREFERENCES\n[1] Z. Qian, “Opportunities abound for dental care in China,” China\nBrieﬁng, Feb. 2015. [Online]. Available: http://www.china-brieﬁng.com/\nnews/2015/02/27/opportunities-abound-dental-care-china.html\n[2] P . J. Pussinen, P . Jousilahti, G. Alfthan, T. Palosuo, S. Asikainen, and V .\nSalomaa, “Antibodies to periodontal pathogens are associated with coro-\nnary heart disease,” Arteriosclerosis Thrombosis V ascular Biol. , vol. 23,\nno. 7, 2003, pp. 1250–1254.\n[3] H. Jansson et al. , “Type 2 diabetes and risk for periodontal disease: A\nrole for dental health awareness,” J. Clinical Periodontol. , vol. 33, no. 6,\npp. 408–414, 2006.\n[4] N. W. Johnson, “The mouth in HIV/AIDS: Markers of disease status and\nmanagement challenges for the dental profession,” Australian Dental J. ,\nvol. 55, no. s1, pp. 85–102, 2010.\n[5] L. Atzori, A. Iera, and G. Morabito, The Internet of Things: A Survey .\nAmsterdam, The Netherlands: Elsevier, 2010.\n[6] D. Metcalf, S. T. Milliard, M. Gomez, and M. Schwartz, “Wearables and\nthe internet of things for health: Wearable, interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L.', 'long-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed-loop\nsystem of pre-screening, consultation, appointment, treatment\nand evaluation of dental disease is established, thereby mak-\nFig. 1. The network architecture of the iHome dental health-IoT\nsystem.\ning the rapid and effective relationship between patients and\ndoctors’ resources a reality.\nA. The Architecture of iHome DentalHealth-IoT System\nThe network architecture of the iHome dental health-IoT sys-\ntem is shown in Fig. 1 , which consists of three network layers:\n1) dental medical service layer; 2) smart dental service layer; 3)\ndental image data acquisition layer.\nThe dental medical service layer is directly connected to pro-\nfessional medical facilities such as hospital, dental clinic, regis-\ntered dentist and dental healthcare product supplier.\nThe dental clinics or hospitals equipped with certain quali-\nﬁcations and idle resources can attract customers through their\nown medical technology and price advantage, thereby making\nit possible for them to make an early judgment with the help\nof self-help screening results, and to make a new e-prescription\naccordingly or treatment programs based on that. Additionally,\nbased on the platform, the doctors can not only manage their\npatients, interact with patients and track the effectiveness of\ntreatment effectively, but also identify the patient group whose\nhealth conditions have improved easily, and inform them of their\nprogress, thereby facilitating the building of positive loops of\nrehabilitation and self-care. The dentists can further improve\nservice quality and efﬁciency through patient evaluations.\nThe smart dental service layer is the core layer where data\nprocessing provides the analysis results of dental symptoms or\naesthetic needs, which also serves as an efﬁcient way to meet\nusers’ daily needs in dental health monitoring. This layer has\nthe following three functions:\nr Intelligent diagnosis and treatment recommendation: It\ncan recommend the nearby excellent doctor for those\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n900 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nFig. 2. Application software system diagram.\nconsumers after the AI diagnose, and prompt them to seek\nfor low-price as well as time-saving medical treatment.\nr Group purchase (E-MALL) service: It can classify the cus-\ntomer needs as well as suitable product services and pro-\nmote different group purchase & product services based\non AI diagnosis results.\nr Large data customization service: It can classify the cus-\ntomer needs and provide the customized or personalized\nproducts as well as the services according to AI diagnosis\nresults.\nMainly consisting of dental image data acquisition devices\nand the mobile terminal, local computing, and processing units,\nwireless transmitting modules and display terminal, the dental\nimage data acquisition layer serves as the basis of the entire\nplatform. The dental image data is uploaded to the smart dental\nservice layer via the network (Wi-Fi, 3']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2568.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",consent of subjects with dental diseases,,,,"[6, 13, 14]","['.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', ' interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L. Oliveira, and M. A.\nGuevara Lopez, “Convolutional neural networks for mammography mass\nlesion classiﬁcation,” in Proc. Conf. IEEE Eng. Med. Biol. Soc. , 2015,\nvol. 2015, pp. 797–800.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n906 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\n[9] H. I. Suk, S. W. Lee, and D. Shen, “Deep sparse multi-task learning for\nfeature selection in alzheimer’s disease diagnosis,” Brain Struct. Funct. ,\nvol. 221, no. 5, pp. 2569–2587, 2016.\n[10] A. Kwasigroch, A. Mikoajczyk, and M. Grochowski, “Deep con-\nvolutional neural networks as a decision support tool in medical\nproblems—Malignant melanoma case study,” in Trends in Advanced\nIntelligent Control, Optimization and Automation (Advances in In-\ntelligent Systems and Computing). New Y ork, NY , USA: Springer,\n2017.\n[11] A. Esteva et al. , “Dermatologist-level classiﬁcation of skin cancer\nwith deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118,\n2017.\n[12] A. B. Oktay and Y . S. Akgul, “Diagnosis of degenerative intervertebral\ndisc disease with deep networks and SVM,” in Computer and Information\nSciences. New Y ork, NY , USA: Springer, 2016.\n[13] D. H. Wolpert and W. G. Macready, “No free lunch theorems for op-\ntimization,” IEEE Trans. Evol. Comput. , vol. 1, no. 1, pp. 67–82, Apr.\n1997.\n[14] S. A. Prajapati, R. Nagaraj, and S. Mitra, “Classiﬁcation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—', 'cation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—V olume II (Lecture Notes in\nElectrical Engineering). New Y ork, NY , USA: Springer, 2014, pp. 631–\n639.\n[17] E. Provenzi, C. L. De, A. Rizzi, and D. Marini, “Mathematical deﬁnition\nand analysis of the Retinex algorithm,” J. Opt. Soc. Am. A , vol. 22, no. 12,\npp. 2613–2621, 2005.\n[18] S. Y . Liao and T. Q. Huang, “Video copy-move forgery detection and\nlocalization based on Tamura texture features,” in Proc. Int. Congr . Image\nSignal Process., 2014, pp. 864–868.\n[19] X. Wang, K. Chen, Z. Huang, C. Y ao, and W. Liu, “Point linking network\nfor object detection,” 2017, arXiv preprint arXiv:1706.03646.\n[20] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\nmulti-task network cascades,” in Proc. IEEE Conf. Comput. Vision Pattern\nRecognit., 2015, pp. 3150–3158.\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit. , 2015,\npp. 770–778.\n[22] S. Ren, R. Girshick, R. Girshick, and J. Sun, “Faster R-CNN: Towards\nreal-time object detection with region proposal networks,” IEEE Trans.\nPattern Anal. & Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, Jun. 2017.\n[23] Z. Xie, X. Zhang, D. Zeng, X. Chen, and W. Feng, “Design and imple-\nmentation of the remote wireless intraoral endoscope system,” in Proc.\nInt. Ind. Inform. Comput. Eng. Conf. , 2015, pp. 975–980.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2422.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",by convenience,,,,"[6, 1, 5]","['.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', ' part by the Shanghai Pujiang Program under Grant\n17PJ1400800, and in part by Shanghai Institute of Intelligent Electronics\nand Systems. (Corresponding authors: Li-Rong Zheng; Zhuo Zou; Shih-\nChing Y eh.)\nThe authors are with the School of Information Science and T ech-\nnology, Fudan University, Shanghai 200433 China (e-mail: , isreason@\n126.com; jwxu16@fudan.edu.cn; yxhuan@kth.se; zhuo@kth.se; yeshi\nqing@fudan.edu.cn; lirong@kth.se).\nDigital Object Identiﬁer 10.1109/JBHI.2019.2919916\nstrong sense of tooth protection, can be problematic. The latest\nsurvey on oral health shows that a shocking 94% of popula-\ntion suffers from various forms of dental problems in China [1].\nHowever, dental diseases in many cases can be prevented, and\nserious problems can be avoided if teeth are regularly moni-\ntored. Moreover, the monitoring of periodontal, gingival and\noral mucosa can play a signiﬁcant role in monitoring the pa-\ntients’ cardiovascular and cerebrovascular diseases, diabetes,\nAIDS and other problems [2]–[4].\nNowadays, the IoT, a growing ubiquitous concept, has inu-\nenced various aspects of human life [5]. To be more speciﬁc,\nIoT-based healthcare service has been applied to a wide range of\nelds, and various healthcare solutions are thereby provided [6],\n[7]. It is possible to achieve signicant enhancement in healthcare\nsuch as early-detection and prediction. However, with the emer-\ngence of the IoT-based healthcare services, smart HomeCentric\ndental solution can be built to make treatment before getting\nillnessa reality. Speciﬁcally, such studies focus commonly on\nIoT-based healthcare services platforms, with few researches on\nIn-home dental healthcare and services.\nThe rapid development of wireless and mobile networks has\nbeen accompanied by mobile terminals serving as an effective\nsocial platform for everyone, and mobile application software\nis increasingly popular, thereby making it necessary to design\nthe dental healthcare APPs to realize the quick and effective\nconnection between patients and dental doctors.\nRecent years have witnessed deep learning showing potential\nin the versatile and highly variable tasks of a variety of ﬁne-\ngrained object classiﬁcations. Powered by advances in com-\nputation and excessively large datasets, deep learning algo-\nrithms have produced promising results in different medical\ntasks, which have also been applied in medical image analy-\nsis such as MRI, derma to scopic images and standard images.\nDeep neural networks were used to form a mammography mass\nlesion classiﬁcation [8], Alzheimer disease classiﬁcation [9],\nskin lesion and skin cancer classiﬁcation [10], [11] Ayse Betul\nOktay presented a method of detecting teeth in dental panoramic\nX-ray images with Convolutional Neural Network (CNN) [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE', 'namely, client, service platform and doctor at the application\nlogic level. Users capture pictures using intelligent dental im-\nage data acquisition device, and then upload the pictures to the\nservice platform through the mobile terminal, and these den-\ntal image data will be analyzed using AI methods. The system\nwill advise the user to seek for medical care if problems have\nbeen found in teeth after the conﬁrmation of the service by the\nuser. Nearby dental clinics or doctors based on their geographic\nlocation will be recommended at the service platform, thereby\nmaking it possible for users to consult with doctors to make\nappointments and complete ofﬂine medical procedures if the\nappointment is successful.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 901\nFig. 4. The intelligent dental image acquisition device hardware\ndiagram.\nC. Intelligent Hardware\nTraditional dental image collection methods can bring about\nthe inconvenience as follows:\nr Dentists are required to acquire knowledge on more shoot-\ning skills;\nr It is necessary to expand the auxiliary equipment such as\nthe oral expander, along with high cost of the SLR camera.\nr Only partial dental images can be obtained\nDue to the peculiarity of oral structure, the general image\nacquisition equipment (e.g. smart cell-phone) cant meet the im-\nage data collection requirement of 6 surfaces of teeth, thereby\nhelping explain that an intelligent dental image acquisition de-\nvice is designed to photograph dental images. As shown in the\nFig. 4 , the intelligent dental image acquisition device is mainly\ncomposed of an image module and the control board, and the\nmicro exible packaging technology is applied to manufacture\nthe image module. On the exible printed circuit board, it is in-\ntegrated with a 1-megapixel CMOS Sensor, the supplementary\nlighting LEDs, and a macro lens to form a module.\nIncluding processing chip, Wi-Fi module, gyroscope, mem-\nory, keys, LEDs, power management and wireless charger, the\nmain board has functions such as video encoding, interface con-\ntrol, light control and task processing. Additionally, the device\ncan be communicated with the mobile terminal through the Wi-\nFi module. When photographing the teeth, the left-side images\nand right-side images are ﬂipped, with the use of a gyroscope\nsensor in the hardware design [16]. Moreover, In the process\nof shooting, the processor can sense the change signal, and then\ncorrect the image position with the software automatically.\nIII. I NTELLIGENT DENT AL DIAGNOSIS\nA. System Flow and Data Acquisition\nAI detection of dental diseases is the most important part of\nthe intelligent dental Health-IoT platform, and the main steps\nFig. 5. Dental image analysis system ﬂow.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2559.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q4,How was the dataset described in this study before predictive modeling was performed?,"12,600 clinical images classified into 7 dental diseases.",,,,"[6, 7, 14]","['.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', '3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN', 'cation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—V olume II (Lecture Notes in\nElectrical Engineering). New Y ork, NY , USA: Springer, 2014, pp. 631–\n639.\n[17] E. Provenzi, C. L. De, A. Rizzi, and D. Marini, “Mathematical deﬁnition\nand analysis of the Retinex algorithm,” J. Opt. Soc. Am. A , vol. 22, no. 12,\npp. 2613–2621, 2005.\n[18] S. Y . Liao and T. Q. Huang, “Video copy-move forgery detection and\nlocalization based on Tamura texture features,” in Proc. Int. Congr . Image\nSignal Process., 2014, pp. 864–868.\n[19] X. Wang, K. Chen, Z. Huang, C. Y ao, and W. Liu, “Point linking network\nfor object detection,” 2017, arXiv preprint arXiv:1706.03646.\n[20] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\nmulti-task network cascades,” in Proc. IEEE Conf. Comput. Vision Pattern\nRecognit., 2015, pp. 3150–3158.\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit. , 2015,\npp. 770–778.\n[22] S. Ren, R. Girshick, R. Girshick, and J. Sun, “Faster R-CNN: Towards\nreal-time object detection with region proposal networks,” IEEE Trans.\nPattern Anal. & Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, Jun. 2017.\n[23] Z. Xie, X. Zhang, D. Zeng, X. Chen, and W. Feng, “Design and imple-\nmentation of the remote wireless intraoral endoscope system,” in Proc.\nInt. Ind. Inform. Comput. Eng. Conf. , 2015, pp. 975–980.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2437.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?","4/5 training, 1/5 testing, from different subjects.",,,,"[6, 8, 14]","['.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', ' image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN ANCHOR SCALES are set. The sample\ndata is transferred from the 16-bit grayscale to 8-bit grayscale,\nthen copied to the training directory to execute the training code.\nThe data sets for each dental disease were basically balanced,\nand the method of limiting training time is adopted to avoid\noverﬁtting, with the detection results using MASK R-CNN on\ndental images being shown in the right of Fig. 7 .\nD. Inference\nRecognition rate in this study refers to the rate at which dental\ndiseases can be correctly identiﬁed in these testing image data.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 903\nT ABLE II\nTHE RECOGNITION ACCURACY OF 7T YPES OF DENT AL DISEASES\nFig. 8. A board level prototype of the intelligent dental image acquisition\ndevicet.\nThe algorithm above is followed to train the model following the\n4-step training of Faster R-CNN [22], with the Table II showing\nthe recognition accuracy of each dental disease. The table shows\nthat we can ﬁnd good results achieved by MASK R-CNN even\nunder challenging conditions, such as the reection of saliva,\ntooth gap, etc. The recognition rate of the algorithm is over 90%\nfor these seven major dental diseases, with the recognition rate\nof dental plaque as 100%, and the recognition rate of decayed\ntooth as 90.1%. It is analyzed that relatively low recognition rate\nof decayed tooth is attributed to the device using a xed focus\nlens, which will be affected by the shooting, such as smear\nand defocusing blurring. In addition, coding errors and missing\nframes can also be responsible for it. Compared with the related\nwork in [14], the accuracy of the presented transfer learning\nmethod for the classication of dental caries and periodontitis\nare all 87.5 %. This work achieves an accuracy of 90.1% and\n94.3% respectively, indicating that the classication accuracy of\ncorresponding dental diseases has been greatly improved in our\nwork.\nIV . S YSTEM INTEGRA TION AND PROTOTYPE IMPLEMENT A TION\nA. System Implementation\nThe PCB hardware of the intelligent dental image acquisition\ndevice is designed and completed, as well as the APP soft-\nware for android and apple system, with the motherboard and\nthe phone communicating via Wi-Fi. As shown in Fig. 8 ,t h e\nhardware motherboard is a circular plate with 6cm of diameter,\nwhich is suitable for being held in hands, and can be inserted\ninto the mouth exibly to collect images of the teeth. A small\npercentage of the collected images, due to the lack of', 'cation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—V olume II (Lecture Notes in\nElectrical Engineering). New Y ork, NY , USA: Springer, 2014, pp. 631–\n639.\n[17] E. Provenzi, C. L. De, A. Rizzi, and D. Marini, “Mathematical deﬁnition\nand analysis of the Retinex algorithm,” J. Opt. Soc. Am. A , vol. 22, no. 12,\npp. 2613–2621, 2005.\n[18] S. Y . Liao and T. Q. Huang, “Video copy-move forgery detection and\nlocalization based on Tamura texture features,” in Proc. Int. Congr . Image\nSignal Process., 2014, pp. 864–868.\n[19] X. Wang, K. Chen, Z. Huang, C. Y ao, and W. Liu, “Point linking network\nfor object detection,” 2017, arXiv preprint arXiv:1706.03646.\n[20] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\nmulti-task network cascades,” in Proc. IEEE Conf. Comput. Vision Pattern\nRecognit., 2015, pp. 3150–3158.\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit. , 2015,\npp. 770–778.\n[22] S. Ren, R. Girshick, R. Girshick, and J. Sun, “Faster R-CNN: Towards\nreal-time object detection with region proposal networks,” IEEE Trans.\nPattern Anal. & Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, Jun. 2017.\n[23] Z. Xie, X. Zhang, D. Zeng, X. Chen, and W. Feng, “Design and imple-\nmentation of the remote wireless intraoral endoscope system,” in Proc.\nInt. Ind. Inform. Comput. Eng. Conf. , 2015, pp. 975–980.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2442.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q6,What preprocessing techniques on the included variables/features were applied in this study?,"Image enhancement, grayscale conversion, ResNet-50-C4 backbone.",,,,"[6, 7, 8]","['.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', '3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN', ' image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN ANCHOR SCALES are set. The sample\ndata is transferred from the 16-bit grayscale to 8-bit grayscale,\nthen copied to the training directory to execute the training code.\nThe data sets for each dental disease were basically balanced,\nand the method of limiting training time is adopted to avoid\noverﬁtting, with the detection results using MASK R-CNN on\ndental images being shown in the right of Fig. 7 .\nD. Inference\nRecognition rate in this study refers to the rate at which dental\ndiseases can be correctly identiﬁed in these testing image data.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 903\nT ABLE II\nTHE RECOGNITION ACCURACY OF 7T YPES OF DENT AL DISEASES\nFig. 8. A board level prototype of the intelligent dental image acquisition\ndevicet.\nThe algorithm above is followed to train the model following the\n4-step training of Faster R-CNN [22], with the Table II showing\nthe recognition accuracy of each dental disease. The table shows\nthat we can ﬁnd good results achieved by MASK R-CNN even\nunder challenging conditions, such as the reection of saliva,\ntooth gap, etc. The recognition rate of the algorithm is over 90%\nfor these seven major dental diseases, with the recognition rate\nof dental plaque as 100%, and the recognition rate of decayed\ntooth as 90.1%. It is analyzed that relatively low recognition rate\nof decayed tooth is attributed to the device using a xed focus\nlens, which will be affected by the shooting, such as smear\nand defocusing blurring. In addition, coding errors and missing\nframes can also be responsible for it. Compared with the related\nwork in [14], the accuracy of the presented transfer learning\nmethod for the classication of dental caries and periodontitis\nare all 87.5 %. This work achieves an accuracy of 90.1% and\n94.3% respectively, indicating that the classication accuracy of\ncorresponding dental diseases has been greatly improved in our\nwork.\nIV . S YSTEM INTEGRA TION AND PROTOTYPE IMPLEMENT A TION\nA. System Implementation\nThe PCB hardware of the intelligent dental image acquisition\ndevice is designed and completed, as well as the APP soft-\nware for android and apple system, with the motherboard and\nthe phone communicating via Wi-Fi. As shown in Fig. 8 ,t h e\nhardware motherboard is a circular plate with 6cm of diameter,\nwhich is suitable for being held in hands, and can be inserted\ninto the mouth exibly to collect images of the teeth. A small\npercentage of the collected images, due to the lack of']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2581.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q7,How is missing data handled in this study?,Unknown from this paper.,,,,"[6, 2, 13]","['.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', ' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', ' interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L. Oliveira, and M. A.\nGuevara Lopez, “Convolutional neural networks for mammography mass\nlesion classiﬁcation,” in Proc. Conf. IEEE Eng. Med. Biol. Soc. , 2015,\nvol. 2015, pp. 797–800.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n906 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\n[9] H. I. Suk, S. W. Lee, and D. Shen, “Deep sparse multi-task learning for\nfeature selection in alzheimer’s disease diagnosis,” Brain Struct. Funct. ,\nvol. 221, no. 5, pp. 2569–2587, 2016.\n[10] A. Kwasigroch, A. Mikoajczyk, and M. Grochowski, “Deep con-\nvolutional neural networks as a decision support tool in medical\nproblems—Malignant melanoma case study,” in Trends in Advanced\nIntelligent Control, Optimization and Automation (Advances in In-\ntelligent Systems and Computing). New Y ork, NY , USA: Springer,\n2017.\n[11] A. Esteva et al. , “Dermatologist-level classiﬁcation of skin cancer\nwith deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118,\n2017.\n[12] A. B. Oktay and Y . S. Akgul, “Diagnosis of degenerative intervertebral\ndisc disease with deep networks and SVM,” in Computer and Information\nSciences. New Y ork, NY , USA: Springer, 2016.\n[13] D. H. Wolpert and W. G. Macready, “No free lunch theorems for op-\ntimization,” IEEE Trans. Evol. Comput. , vol. 1, no. 1, pp. 67–82, Apr.\n1997.\n[14] S. A. Prajapati, R. Nagaraj, and S. Mitra, “Classiﬁcation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2538.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[14, 2, 1]","['cation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—V olume II (Lecture Notes in\nElectrical Engineering). New Y ork, NY , USA: Springer, 2014, pp. 631–\n639.\n[17] E. Provenzi, C. L. De, A. Rizzi, and D. Marini, “Mathematical deﬁnition\nand analysis of the Retinex algorithm,” J. Opt. Soc. Am. A , vol. 22, no. 12,\npp. 2613–2621, 2005.\n[18] S. Y . Liao and T. Q. Huang, “Video copy-move forgery detection and\nlocalization based on Tamura texture features,” in Proc. Int. Congr . Image\nSignal Process., 2014, pp. 864–868.\n[19] X. Wang, K. Chen, Z. Huang, C. Y ao, and W. Liu, “Point linking network\nfor object detection,” 2017, arXiv preprint arXiv:1706.03646.\n[20] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\nmulti-task network cascades,” in Proc. IEEE Conf. Comput. Vision Pattern\nRecognit., 2015, pp. 3150–3158.\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit. , 2015,\npp. 770–778.\n[22] S. Ren, R. Girshick, R. Girshick, and J. Sun, “Faster R-CNN: Towards\nreal-time object detection with region proposal networks,” IEEE Trans.\nPattern Anal. & Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, Jun. 2017.\n[23] Z. Xie, X. Zhang, D. Zeng, X. Chen, and W. Feng, “Design and imple-\nmentation of the remote wireless intraoral endoscope system,” in Proc.\nInt. Ind. Inform. Comput. Eng. Conf. , 2015, pp. 975–980.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. ', ' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', ' part by the Shanghai Pujiang Program under Grant\n17PJ1400800, and in part by Shanghai Institute of Intelligent Electronics\nand Systems. (Corresponding authors: Li-Rong Zheng; Zhuo Zou; Shih-\nChing Y eh.)\nThe authors are with the School of Information Science and T ech-\nnology, Fudan University, Shanghai 200433 China (e-mail: , isreason@\n126.com; jwxu16@fudan.edu.cn; yxhuan@kth.se; zhuo@kth.se; yeshi\nqing@fudan.edu.cn; lirong@kth.se).\nDigital Object Identiﬁer 10.1109/JBHI.2019.2919916\nstrong sense of tooth protection, can be problematic. The latest\nsurvey on oral health shows that a shocking 94% of popula-\ntion suffers from various forms of dental problems in China [1].\nHowever, dental diseases in many cases can be prevented, and\nserious problems can be avoided if teeth are regularly moni-\ntored. Moreover, the monitoring of periodontal, gingival and\noral mucosa can play a signiﬁcant role in monitoring the pa-\ntients’ cardiovascular and cerebrovascular diseases, diabetes,\nAIDS and other problems [2]–[4].\nNowadays, the IoT, a growing ubiquitous concept, has inu-\nenced various aspects of human life [5]. To be more speciﬁc,\nIoT-based healthcare service has been applied to a wide range of\nelds, and various healthcare solutions are thereby provided [6],\n[7]. It is possible to achieve signicant enhancement in healthcare\nsuch as early-detection and prediction. However, with the emer-\ngence of the IoT-based healthcare services, smart HomeCentric\ndental solution can be built to make treatment before getting\nillnessa reality. Speciﬁcally, such studies focus commonly on\nIoT-based healthcare services platforms, with few researches on\nIn-home dental healthcare and services.\nThe rapid development of wireless and mobile networks has\nbeen accompanied by mobile terminals serving as an effective\nsocial platform for everyone, and mobile application software\nis increasingly popular, thereby making it necessary to design\nthe dental healthcare APPs to realize the quick and effective\nconnection between patients and dental doctors.\nRecent years have witnessed deep learning showing potential\nin the versatile and highly variable tasks of a variety of ﬁne-\ngrained object classiﬁcations. Powered by advances in com-\nputation and excessively large datasets, deep learning algo-\nrithms have produced promising results in different medical\ntasks, which have also been applied in medical image analy-\nsis such as MRI, derma to scopic images and standard images.\nDeep neural networks were used to form a mammography mass\nlesion classiﬁcation [8], Alzheimer disease classiﬁcation [9],\nskin lesion and skin cancer classiﬁcation [10], [11] Ayse Betul\nOktay presented a method of detecting teeth in dental panoramic\nX-ray images with Convolutional Neural Network (CNN) [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2415.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q9,Which prediction models were used in this study?,Mask R-CNN and ResNet-50-C4 backbone.,,,,"[6, 2, 8]","['.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', ' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', ' image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN ANCHOR SCALES are set. The sample\ndata is transferred from the 16-bit grayscale to 8-bit grayscale,\nthen copied to the training directory to execute the training code.\nThe data sets for each dental disease were basically balanced,\nand the method of limiting training time is adopted to avoid\noverﬁtting, with the detection results using MASK R-CNN on\ndental images being shown in the right of Fig. 7 .\nD. Inference\nRecognition rate in this study refers to the rate at which dental\ndiseases can be correctly identiﬁed in these testing image data.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 903\nT ABLE II\nTHE RECOGNITION ACCURACY OF 7T YPES OF DENT AL DISEASES\nFig. 8. A board level prototype of the intelligent dental image acquisition\ndevicet.\nThe algorithm above is followed to train the model following the\n4-step training of Faster R-CNN [22], with the Table II showing\nthe recognition accuracy of each dental disease. The table shows\nthat we can ﬁnd good results achieved by MASK R-CNN even\nunder challenging conditions, such as the reection of saliva,\ntooth gap, etc. The recognition rate of the algorithm is over 90%\nfor these seven major dental diseases, with the recognition rate\nof dental plaque as 100%, and the recognition rate of decayed\ntooth as 90.1%. It is analyzed that relatively low recognition rate\nof decayed tooth is attributed to the device using a xed focus\nlens, which will be affected by the shooting, such as smear\nand defocusing blurring. In addition, coding errors and missing\nframes can also be responsible for it. Compared with the related\nwork in [14], the accuracy of the presented transfer learning\nmethod for the classication of dental caries and periodontitis\nare all 87.5 %. This work achieves an accuracy of 90.1% and\n94.3% respectively, indicating that the classication accuracy of\ncorresponding dental diseases has been greatly improved in our\nwork.\nIV . S YSTEM INTEGRA TION AND PROTOTYPE IMPLEMENT A TION\nA. System Implementation\nThe PCB hardware of the intelligent dental image acquisition\ndevice is designed and completed, as well as the APP soft-\nware for android and apple system, with the motherboard and\nthe phone communicating via Wi-Fi. As shown in Fig. 8 ,t h e\nhardware motherboard is a circular plate with 6cm of diameter,\nwhich is suitable for being held in hands, and can be inserted\ninto the mouth exibly to collect images of the teeth. A small\npercentage of the collected images, due to the lack of']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2559.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,"ResNet-50 backbone, Mask R-CNN, TensorFlow, hyperparameters set.",,,,"[8, 13, 0]","[' image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN ANCHOR SCALES are set. The sample\ndata is transferred from the 16-bit grayscale to 8-bit grayscale,\nthen copied to the training directory to execute the training code.\nThe data sets for each dental disease were basically balanced,\nand the method of limiting training time is adopted to avoid\noverﬁtting, with the detection results using MASK R-CNN on\ndental images being shown in the right of Fig. 7 .\nD. Inference\nRecognition rate in this study refers to the rate at which dental\ndiseases can be correctly identiﬁed in these testing image data.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 903\nT ABLE II\nTHE RECOGNITION ACCURACY OF 7T YPES OF DENT AL DISEASES\nFig. 8. A board level prototype of the intelligent dental image acquisition\ndevicet.\nThe algorithm above is followed to train the model following the\n4-step training of Faster R-CNN [22], with the Table II showing\nthe recognition accuracy of each dental disease. The table shows\nthat we can ﬁnd good results achieved by MASK R-CNN even\nunder challenging conditions, such as the reection of saliva,\ntooth gap, etc. The recognition rate of the algorithm is over 90%\nfor these seven major dental diseases, with the recognition rate\nof dental plaque as 100%, and the recognition rate of decayed\ntooth as 90.1%. It is analyzed that relatively low recognition rate\nof decayed tooth is attributed to the device using a xed focus\nlens, which will be affected by the shooting, such as smear\nand defocusing blurring. In addition, coding errors and missing\nframes can also be responsible for it. Compared with the related\nwork in [14], the accuracy of the presented transfer learning\nmethod for the classication of dental caries and periodontitis\nare all 87.5 %. This work achieves an accuracy of 90.1% and\n94.3% respectively, indicating that the classication accuracy of\ncorresponding dental diseases has been greatly improved in our\nwork.\nIV . S YSTEM INTEGRA TION AND PROTOTYPE IMPLEMENT A TION\nA. System Implementation\nThe PCB hardware of the intelligent dental image acquisition\ndevice is designed and completed, as well as the APP soft-\nware for android and apple system, with the motherboard and\nthe phone communicating via Wi-Fi. As shown in Fig. 8 ,t h e\nhardware motherboard is a circular plate with 6cm of diameter,\nwhich is suitable for being held in hands, and can be inserted\ninto the mouth exibly to collect images of the teeth. A small\npercentage of the collected images, due to the lack of', ' interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L. Oliveira, and M. A.\nGuevara Lopez, “Convolutional neural networks for mammography mass\nlesion classiﬁcation,” in Proc. Conf. IEEE Eng. Med. Biol. Soc. , 2015,\nvol. 2015, pp. 797–800.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n906 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\n[9] H. I. Suk, S. W. Lee, and D. Shen, “Deep sparse multi-task learning for\nfeature selection in alzheimer’s disease diagnosis,” Brain Struct. Funct. ,\nvol. 221, no. 5, pp. 2569–2587, 2016.\n[10] A. Kwasigroch, A. Mikoajczyk, and M. Grochowski, “Deep con-\nvolutional neural networks as a decision support tool in medical\nproblems—Malignant melanoma case study,” in Trends in Advanced\nIntelligent Control, Optimization and Automation (Advances in In-\ntelligent Systems and Computing). New Y ork, NY , USA: Springer,\n2017.\n[11] A. Esteva et al. , “Dermatologist-level classiﬁcation of skin cancer\nwith deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118,\n2017.\n[12] A. B. Oktay and Y . S. Akgul, “Diagnosis of degenerative intervertebral\ndisc disease with deep networks and SVM,” in Computer and Information\nSciences. New Y ork, NY , USA: Springer, 2016.\n[13] D. H. Wolpert and W. G. Macready, “No free lunch theorems for op-\ntimization,” IEEE Trans. Evol. Comput. , vol. 1, no. 1, pp. 67–82, Apr.\n1997.\n[14] S. A. Prajapati, R. Nagaraj, and S. Mitra, “Classiﬁcation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—', '898 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nA Smart Dental Health-IoT Platform Based on\nIntelligent Hardware, Deep Learning, and\nMobile T erminal\nLizheng Liu , Student Member, IEEE,J i a w e iX u , Student Member, IEEE,\nY uxiang Huan , Student Member, IEEE, Zhuo Zou , Senior Member, IEEE,\nShih-Ching Y eh , Member, IEEE, and Li-Rong Zheng , Senior Member, IEEE\nAbstract— The dental disease is a common disease for a\nhuman. Screening and visual diagnosis that are currently\nperformed in clinics possibly cost a lot in various manners.\nAlong with the progress of the Internet of Things (IoT) and\nartiﬁcial intelligence, the internet-based intelligent system\nhave shown great potential in applying home-based health-\ncare. Therefore, a smart dental health-IoT system based on\nintelligent hardware, deep learning, and mobile terminal is\nproposed in this paper, aiming at exploring the feasibility\nof its application on in-home dental healthcare. Moreover, a\nsmart dental device is designed and developed in this study\nto perform the image acquisition of teeth. Based on the data\nset of 12 600 clinical images collected by the proposed de-\nvice from 10 private dental clinics, an automatic diagnosis\nmodel trained by MASK R-CNN is developed for the detec-\ntion and classiﬁcation of 7 different dental diseases includ-\ning decayed tooth, dental plaque, uorosis, and periodontal\ndisease, with the diagnosis accuracy of them reaching up to\n90%, along with high sensitivity and high speciﬁcity. Follow-\ning the one-month test in ten clinics, compared with that last\nmonth when the platform was not used, the mean diagnosis\ntime reduces by 37.5% for each patient, helping explain the\nincrease in the number of treated patients by 18.4%. Fur-\nthermore, application software (APPs) on mobile terminal\nfor client side and for dentist side are implemented to pro-\nvide service of pre-examination, consultation, appointment,\nand evaluation.\nIndex Terms— Health-IoT, MASK R-CNN, deep learning,\nartiﬁcial intelligence, intelligent hardware, mobile terminal.\nI. I NTRODUCTION\nD\nENTAL diseases (such as dental caries, periodontal dis-\nease, dental ﬂuorosis, etc.) are becoming increasingly\ncommon. Almost everyones teeth, even though he/she has a\nManuscript received October 19, 2018; revised February 16, 2019\nand March 30, 2019; accepted May 24, 2019. Date of publication June\n7, 2019; date of current version March 6, 2020. This work was supported\nin part by the National Natural Science Foundation of China under Grants\n61571137 and 61876037, in part by the Shanghai Innovation Program\n17JC1401400, and in part by the Shanghai Pujiang Program under Grant\n17PJ1400800, and in part by Shanghai Institute of Intelligent Electronics\nand Systems. (Corresponding authors: Li-Rong Zheng; Zhuo Zou; Shih-\nChing Y eh.)\nThe authors are with the School of Information Science and T ech-\nnology, Fudan University, Shanghai 200433 China (e-mail: , isreason@\n126.com; jwxu16@fudan.edu.cn; yxhuan@kth.se; zhuo@kth.se; yeshi\nqing@fudan.edu.cn; lirong@kth.se).\nDigital Object Identiﬁer 10.1109/JBHI.2019.2919916\nstrong sense of tooth protection, can']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2573.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q11,How was data augmentation or generation used in this study?,Unknown from this paper.,,,,"[2, 6, 7]","[' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', '.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art', '3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2559.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared to related work.",,,,"[8, 10, 7]","[' image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN ANCHOR SCALES are set. The sample\ndata is transferred from the 16-bit grayscale to 8-bit grayscale,\nthen copied to the training directory to execute the training code.\nThe data sets for each dental disease were basically balanced,\nand the method of limiting training time is adopted to avoid\noverﬁtting, with the detection results using MASK R-CNN on\ndental images being shown in the right of Fig. 7 .\nD. Inference\nRecognition rate in this study refers to the rate at which dental\ndiseases can be correctly identiﬁed in these testing image data.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 903\nT ABLE II\nTHE RECOGNITION ACCURACY OF 7T YPES OF DENT AL DISEASES\nFig. 8. A board level prototype of the intelligent dental image acquisition\ndevicet.\nThe algorithm above is followed to train the model following the\n4-step training of Faster R-CNN [22], with the Table II showing\nthe recognition accuracy of each dental disease. The table shows\nthat we can ﬁnd good results achieved by MASK R-CNN even\nunder challenging conditions, such as the reection of saliva,\ntooth gap, etc. The recognition rate of the algorithm is over 90%\nfor these seven major dental diseases, with the recognition rate\nof dental plaque as 100%, and the recognition rate of decayed\ntooth as 90.1%. It is analyzed that relatively low recognition rate\nof decayed tooth is attributed to the device using a xed focus\nlens, which will be affected by the shooting, such as smear\nand defocusing blurring. In addition, coding errors and missing\nframes can also be responsible for it. Compared with the related\nwork in [14], the accuracy of the presented transfer learning\nmethod for the classication of dental caries and periodontitis\nare all 87.5 %. This work achieves an accuracy of 90.1% and\n94.3% respectively, indicating that the classication accuracy of\ncorresponding dental diseases has been greatly improved in our\nwork.\nIV . S YSTEM INTEGRA TION AND PROTOTYPE IMPLEMENT A TION\nA. System Implementation\nThe PCB hardware of the intelligent dental image acquisition\ndevice is designed and completed, as well as the APP soft-\nware for android and apple system, with the motherboard and\nthe phone communicating via Wi-Fi. As shown in Fig. 8 ,t h e\nhardware motherboard is a circular plate with 6cm of diameter,\nwhich is suitable for being held in hands, and can be inserted\ninto the mouth exibly to collect images of the teeth. A small\npercentage of the collected images, due to the lack of', ' as dental caries\nbetween the teeth, smoke scale.\nC. System Integration Testing\nVia the client-side APP , the users, if logging in for the ﬁrst\ntime, have to register information, and set up the network\nconnection parameters for the device, then enter the dental\ndisease detection process normally, with the dental photo\nbeing uploaded to the algorithm server for artiﬁcial intelligence\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n904 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nSENSITIVITY AND SPECIFICITY OF 7T YPES OF DENT AL DISEASES\nFig. 9. The APPs running interface.\nanalysis, and the patient will enter the doctor’s appointment pro-\ncess if the dental disease is found in the analysis results. It is also\npossible for users to learn about dental care knowledge, the lat-\nest treatment technologies and products through the client-side\nAPP .\nVia the doctor-side APP , the dentist can enter the normal work\nmode after ﬁnishing the registration, and the dentist, if receiv-\ning the patient diagnosis request, will respond to the request,\ncoordinate treatment time with the patient. Meanwhile, dentists\ncan manage their own patients and track the effects, with Fig. 9\nshowing parts of the APPs running interface.\nD. The Diagnostic Efﬁciency\nWe carried on a systematic testing in 10 private dental clinics,\nwith a total of 25 dentists being used the dentist-side APP , and\nFig. 10. (a) Comparison chart of patient numbers. (b) The mean diag-\nnosis time comparison chart.\nthen counted the working hours of each dentist, the number of\npatients received and the mean time of diagnosis after a month.\nFig. 10 shows the statistical results, suggesting that compared\nwith the traditional way, the number of treated patients with\nthe help of smart dental service increases by 18.4%, while the\nmean diagnosis time reduces by at least 37.5%. Investigation\nand analysis show that patients capable of using the intelligent\ndental image acquisition device and AI analysis to perform pre-\nscreening at home are responsible for it. In short, compared to\nthe traditional way of checking in the dental clinic, this method\ncan save 25–30 minutes of the diagnosis time. Another interest-\ning factor is that dentists often take some time to explain and\ncommunicate with the patients after diagnosis in private dental\nclinics, while patients can acquire the relevant knowledge on\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 905\nT ABLE IV\nTHE RECOGNITION RESUL TS\ndental diseases at the iHome smart dental Health-IoT platform\nafter self-examination.\nIn this test, the recognition results of the algorithm in the\nreal environment have bene counted, with the sample distribu-\ntion and recognition results being shown in Table IV . Statis-\ntically, we found that the reason for the signiﬁcant inﬂuence\non the recognition rate was still owing to the fact that the pa-\ntients were not particularly skillful at using the intelligent den-\ntal image acquisition equipment. Compared with the results in\nTable II above, the results in Table IV show no signiﬁcant ﬂuctu-\nation, indicating that the algorithm is highly reliable', '3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2576.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q13,Which type of explainability techniques are used?,Unknown from this paper.,,,,"[2, 7, 5]","[' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', '3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN', 'namely, client, service platform and doctor at the application\nlogic level. Users capture pictures using intelligent dental im-\nage data acquisition device, and then upload the pictures to the\nservice platform through the mobile terminal, and these den-\ntal image data will be analyzed using AI methods. The system\nwill advise the user to seek for medical care if problems have\nbeen found in teeth after the conﬁrmation of the service by the\nuser. Nearby dental clinics or doctors based on their geographic\nlocation will be recommended at the service platform, thereby\nmaking it possible for users to consult with doctors to make\nappointments and complete ofﬂine medical procedures if the\nappointment is successful.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 901\nFig. 4. The intelligent dental image acquisition device hardware\ndiagram.\nC. Intelligent Hardware\nTraditional dental image collection methods can bring about\nthe inconvenience as follows:\nr Dentists are required to acquire knowledge on more shoot-\ning skills;\nr It is necessary to expand the auxiliary equipment such as\nthe oral expander, along with high cost of the SLR camera.\nr Only partial dental images can be obtained\nDue to the peculiarity of oral structure, the general image\nacquisition equipment (e.g. smart cell-phone) cant meet the im-\nage data collection requirement of 6 surfaces of teeth, thereby\nhelping explain that an intelligent dental image acquisition de-\nvice is designed to photograph dental images. As shown in the\nFig. 4 , the intelligent dental image acquisition device is mainly\ncomposed of an image module and the control board, and the\nmicro exible packaging technology is applied to manufacture\nthe image module. On the exible printed circuit board, it is in-\ntegrated with a 1-megapixel CMOS Sensor, the supplementary\nlighting LEDs, and a macro lens to form a module.\nIncluding processing chip, Wi-Fi module, gyroscope, mem-\nory, keys, LEDs, power management and wireless charger, the\nmain board has functions such as video encoding, interface con-\ntrol, light control and task processing. Additionally, the device\ncan be communicated with the mobile terminal through the Wi-\nFi module. When photographing the teeth, the left-side images\nand right-side images are ﬂipped, with the use of a gyroscope\nsensor in the hardware design [16]. Moreover, In the process\nof shooting, the processor can sense the change signal, and then\ncorrect the image position with the software automatically.\nIII. I NTELLIGENT DENT AL DIAGNOSIS\nA. System Flow and Data Acquisition\nAI detection of dental diseases is the most important part of\nthe intelligent dental Health-IoT platform, and the main steps\nFig. 5. Dental image analysis system ﬂow.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2571.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,Unknown from this paper.,,,,"[13, 7, 6]","[' interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L. Oliveira, and M. A.\nGuevara Lopez, “Convolutional neural networks for mammography mass\nlesion classiﬁcation,” in Proc. Conf. IEEE Eng. Med. Biol. Soc. , 2015,\nvol. 2015, pp. 797–800.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n906 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\n[9] H. I. Suk, S. W. Lee, and D. Shen, “Deep sparse multi-task learning for\nfeature selection in alzheimer’s disease diagnosis,” Brain Struct. Funct. ,\nvol. 221, no. 5, pp. 2569–2587, 2016.\n[10] A. Kwasigroch, A. Mikoajczyk, and M. Grochowski, “Deep con-\nvolutional neural networks as a decision support tool in medical\nproblems—Malignant melanoma case study,” in Trends in Advanced\nIntelligent Control, Optimization and Automation (Advances in In-\ntelligent Systems and Computing). New Y ork, NY , USA: Springer,\n2017.\n[11] A. Esteva et al. , “Dermatologist-level classiﬁcation of skin cancer\nwith deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118,\n2017.\n[12] A. B. Oktay and Y . S. Akgul, “Diagnosis of degenerative intervertebral\ndisc disease with deep networks and SVM,” in Computer and Information\nSciences. New Y ork, NY , USA: Springer, 2016.\n[13] D. H. Wolpert and W. G. Macready, “No free lunch theorems for op-\ntimization,” IEEE Trans. Evol. Comput. , vol. 1, no. 1, pp. 67–82, Apr.\n1997.\n[14] S. A. Prajapati, R. Nagaraj, and S. Mitra, “Classiﬁcation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—', '3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN', '.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2558.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,Unknown from this paper.,,,,"[13, 7, 6]","[' interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L. Oliveira, and M. A.\nGuevara Lopez, “Convolutional neural networks for mammography mass\nlesion classiﬁcation,” in Proc. Conf. IEEE Eng. Med. Biol. Soc. , 2015,\nvol. 2015, pp. 797–800.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n906 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\n[9] H. I. Suk, S. W. Lee, and D. Shen, “Deep sparse multi-task learning for\nfeature selection in alzheimer’s disease diagnosis,” Brain Struct. Funct. ,\nvol. 221, no. 5, pp. 2569–2587, 2016.\n[10] A. Kwasigroch, A. Mikoajczyk, and M. Grochowski, “Deep con-\nvolutional neural networks as a decision support tool in medical\nproblems—Malignant melanoma case study,” in Trends in Advanced\nIntelligent Control, Optimization and Automation (Advances in In-\ntelligent Systems and Computing). New Y ork, NY , USA: Springer,\n2017.\n[11] A. Esteva et al. , “Dermatologist-level classiﬁcation of skin cancer\nwith deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118,\n2017.\n[12] A. B. Oktay and Y . S. Akgul, “Diagnosis of degenerative intervertebral\ndisc disease with deep networks and SVM,” in Computer and Information\nSciences. New Y ork, NY , USA: Springer, 2016.\n[13] D. H. Wolpert and W. G. Macready, “No free lunch theorems for op-\ntimization,” IEEE Trans. Evol. Comput. , vol. 1, no. 1, pp. 67–82, Apr.\n1997.\n[14] S. A. Prajapati, R. Nagaraj, and S. Mitra, “Classiﬁcation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—', '3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through articial selection, along with the possible adding of\nthe undetected target images. The nished data is applied as the\ntraining sample data for the detection and classication of dental\ndiseases, ending up with the summary of the recorded frame\nimages to establish the dental training sample set.\nC. Model Training\nCommon target detection algorithms including RCNN,\nYOLO, SSD [19] only classify the target in the image. However,\nthe accurate target segmentation is benecial to the auxiliary med-\nical diagnosis, to achieve the aim of fast and accurate detection\nof dental diseases and dental area accurate segmentation. Based\non the established dental sample library, the MASK R-CNN\nmethod is chosen for the accurate detection and segmentation\nof the dental target.\nThe designed MASK R-CNN network is shown in the left\nof Fig. 6 , in which, MASK R-CNN adds a branch predicting\nthe segmentation mask in each interested area based on Faster\nR-CNN [20]. In Faster R-CNN, region of interest (RoI) pool\nis used to extract the small features from each RoI. Firstly,\nRoIPool scales the RoI represented by oating point numbers to\nFig. 7. Left: MASK R-CNN network. Right: example detections using\nMASK R-CNN on dental images test.\nthe granularity matching the feature graph, followed by parti-\ntioning of the scaled ROI, ending up with the summary of the\ncharacteristic value of each block coverage area. Such calcula-\ntions misplace the ROI and the extracted features, which, though\npossibly not affecting the classication, for the classication has\nthe certain robustness to the small amplitude transformation,\nexerts a great negative impact on the prediction of the precise\nmask of pixel level. Mask R-CNN proposes the RoI Align layer\nto remove the dislocation of RoIPool and aligns the extracted\nfeatures with the input accurately. In RoI Align, a bilinear in-\nterpolation is used to calculate the exact value of each position,\nalong with the summary of the results by using the maximum\nor average pooling, and the extracted features and input can be\naligned with the pixel to pixel, thereby improving the segmen-\ntation accuracy effectively. For the lower convolution network\nused for feature extraction on the whole image, the ResNet-50-\nC4 backbone [20], [21] is adopted with the depth of 50 layers,\nand extract features from the nal convolution layer of the fourth\nstage in the MASK R-CNN. For the upper network, we extend\nthe Faster R-CNN upper network proposed in ResNet, adding a\nmask branch respectively. TensorFlow and Mask R-CNN open\nsource are adopted in the training model, along with modiﬁca-\ntion of ShapesCong class, and GPU COUNT and IMAGES PER\nGPU are set to 1 according to the training server conguration.\nAt the same time, according to the training sample library, cate-\ngories NUM CLASSES, image size IMAGE MIN DIM and the\nsize of the anchor RPN AN', '.\ninclude teeth cases images collection, the sample set building,\nand model training to achieve this goal. The system ﬂowchart is\nshown in Fig. 5 .\nDue to the lack of dental diseases standard data sets, we\nworked with some dental clinics (such as Beijing Mei-Xiao\ndental clinics). With the consent of the subjects, 300 subjects\nuse dental image acquisition device to sample the dental diseases\nvideo data, and the algorithm analyzes each frame of video, and\n3835 images of dental cases are sampled ﬁnally. These clinics\nalready have 8,765 images of dental cases, with a total of 12,600\nclinical images being collected. These images are classiﬁed into\n7 different types as shown in Table I, with the 7 types of dental\ndiseases as follows: dental caries, dental uorosis, periodontal\ndisease, cracked tooth, dental calculus, dental plaque, and tooth\nloss. 4/5 of these data was used in training, while 1/5 of them was\nin model testing, training data set and test data are respectively\nfrom different objects, thus without overlap between the testing\ndataset and the training dataset.\nB. Build Sample Set\nThe semi-automatic labeling method is applied to establish\nthe training sample set to improve the efﬁciency. The dental im-\nages were initially labeled by the design of detector for 7 types\nof dental diseases, as shown in Fig. 5 , the functions of the de-\ntector here include image enhancement, color texture matching,\ncoarse localization and classiﬁcation of disease. Followed by\nthe conﬁrmation of images with labeling error or classiﬁcation\nerror through manual screening method, the training samples\nset were ﬁnally calibrated by 20 dental disease experts, with the\ndetails as follows:\n1) Image Enhancement: In some dental images, the features\nof the tooth disease site without very high contrast are not very\nobvious, thereby making it necessary to enhance the images\nbefore designing the classiﬁcation algorithms. The Retinex al-\ngorithm is used [17], which adaptively prompts various types\nof images to achieve balance in dynamic range, edge, and color.\nThe image enhancement effect in Fig. 6 shows that the color\nand texture details of the dental disease are highlighted after the\nenhancement.\n2) Coarse Localization and Classiﬁcation of Dental Disease\nTarget Area: It is just a preliminary detecting, and only the areas\nclose to the color and texture of the image to be matched will\nbe detected as samples. Firstly, a histogram matching method\nwith fast calculation speed is used to select the region of color\nproximity, followed by the use of the Tamura texture feature\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n902 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE I\nDISTRIBUTION OF DENT AL DISEASE TYPES\nFig. 6. Dental images enhancement comparison chart.\nalgorithm by this design [18], which is applied to ﬁnd the\nimage area blocks close to the sample image.\n3) Artiﬁcial Screening Determines the Sample Library: For\nthe image areas after coarse location and classication, the anno-\ntation information of each frame of data needs to be manually\nconﬁrmed by dental disease experts. Labelme tool is used to gen-\nerate mask data set, along with description of the segmentation\nregion of dental disease as accurate as possible with multiple key\npoints. We delete the wrong label positions and ﬁx the wrong\ntags through art']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2559.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?",one-month testing in 10 clinics,,,,"[12, 11, 13]","['iﬁcant impact on the anal-\nysis of artiﬁcial intelligence. Therefore, the future work will\ngive more priority to the improvement of the hardware design\nand the efﬁciency of image acquisition device, and further work\nwill focus on improving algorithm efﬁciency and recognition\nrate, reducing false alarm rate, and setting up a larger image\ndata set for dental diseases.\nV. C ONCLUSION\nThis paper proposes an iHome smart dental Health-IoT sys-\ntem based on intelligent hardware, deep learning and mobile\nterminal, aiming to regulate as well as optimize the accessibil-\nity of dental treatment and provide home-based dental health\ncare service more efciently. The trained model was used to\nrealize the detection and classication of dental diseases, and\napplication software (Apps) on mobile terminal was designed\nfor client-side and dentist-side. The software platform with the\nfunctions including pre-examination of dental disease, consulta-\ntion, appointment, and evaluation, etc, made the service docking\nbetween the patient and the dentist resources a reality. The AI\nalgorithm achieved more than 90% recognition rate for seven\ndental diseases., which has greatly improved the patient rate and\nthe resource utilization rate of the dental clinic through a one-\nmonth systematic testing in 10 private dental clinics, showing\nhigh reliability in practical application.\nREFERENCES\n[1] Z. Qian, “Opportunities abound for dental care in China,” China\nBrieﬁng, Feb. 2015. [Online]. Available: http://www.china-brieﬁng.com/\nnews/2015/02/27/opportunities-abound-dental-care-china.html\n[2] P . J. Pussinen, P . Jousilahti, G. Alfthan, T. Palosuo, S. Asikainen, and V .\nSalomaa, “Antibodies to periodontal pathogens are associated with coro-\nnary heart disease,” Arteriosclerosis Thrombosis V ascular Biol. , vol. 23,\nno. 7, 2003, pp. 1250–1254.\n[3] H. Jansson et al. , “Type 2 diabetes and risk for periodontal disease: A\nrole for dental health awareness,” J. Clinical Periodontol. , vol. 33, no. 6,\npp. 408–414, 2006.\n[4] N. W. Johnson, “The mouth in HIV/AIDS: Markers of disease status and\nmanagement challenges for the dental profession,” Australian Dental J. ,\nvol. 55, no. s1, pp. 85–102, 2010.\n[5] L. Atzori, A. Iera, and G. Morabito, The Internet of Things: A Survey .\nAmsterdam, The Netherlands: Elsevier, 2010.\n[6] D. Metcalf, S. T. Milliard, M. Gomez, and M. Schwartz, “Wearables and\nthe internet of things for health: Wearable, interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L.', '905\nT ABLE IV\nTHE RECOGNITION RESUL TS\ndental diseases at the iHome smart dental Health-IoT platform\nafter self-examination.\nIn this test, the recognition results of the algorithm in the\nreal environment have bene counted, with the sample distribu-\ntion and recognition results being shown in Table IV . Statis-\ntically, we found that the reason for the signiﬁcant inﬂuence\non the recognition rate was still owing to the fact that the pa-\ntients were not particularly skillful at using the intelligent den-\ntal image acquisition equipment. Compared with the results in\nTable II above, the results in Table IV show no signiﬁcant ﬂuctu-\nation, indicating that the algorithm is highly reliable in practical\napplication.\nE. Discussion\nThe individual home environment is connected with the hos-\npital, dental clinics, and other medical facilities through the\ndeveloped iHome smart dental Health-IoT system, thereby pro-\nviding remote diagnosis and medical service, with the key in-\nnovation and functionality as follows.\n1) Oral Disease Prevention: The investigation showed a low\nawareness of tooth protection for most patients in China. Usu-\nally, they will choose to diagnose and treat their teeth after\nfeeling certain discomfort. The iHome smart dental Health-IoT\nsystem provides users with a convenient platform for the self-\nexamination. To be more speciﬁc, dental diseases of users can\nbe detected and prevented early with the help of AI analysis, as\nwell as raising individuals’ awareness of tooth protection.\n2) Intelligent Analysis: The proposed iHome smart dental\nHealth-IoT system performs AI analysis on dental images us-\ning deep learning algorithm, which can quickly and effectively\ndetect tooth diseases, thereby providing a diagnostic basis for\ndentists and saving treatment time.\n3) Remote Medical Service: The interactive platform of the\niHome smart dental Health-IoT system makes a direct ap-\npointment to the nearest dentists and prompts them to medi-\ncal treatment in a low-price and time-saving manner. This new\ninternet mode can mobilize the dental diagnostic resources rel-\natively idle. Patients are able to solve their dental problems\nat lower price, which is attributed to the fact that on the one\nhand, the intelligent dental image acquisition device costs less,\nand on the other hand, the platform has been able to mobi-\nlize some idle dental clinics where clients can go for treat-\nment. In this way, the cost per customer will be reduced as the\nnumber of users increases. The platform establishes a unique\nword-of-mouth system for dentists, with the evaluation from\nthe patients after treatment promoting further improvement in\nthe quality of dentist service, making it possible for the user to\nchoose some cost-effective clinics for treatment.\nThe implemented intelligent hardware provides a way to ob-\ntain dental image data, but the use of prime lens brings image\nblurring and ghosting in the process of test. In addition, the lack\nof lens angle results in the incomplete coverage of larger teeth.\nThese problems exert a relatively signiﬁcant impact on the anal-\nysis of artiﬁcial intelligence. Therefore, the future work will\ngive more priority to the improvement of the hardware design\nand the efﬁciency of image acquisition device, and further work\nwill focus on improving algorithm efﬁciency and recognition\nrate, reducing false alarm rate, and setting up a larger image\ndata set for dental diseases.\nV. C ONCLUSION\nThis paper proposes an iHome smart dental Health-IoT sys-\ntem based on intelligent hardware, deep learning and mobile\nterminal, aiming to regulate as well as optimize the accessibil-\nity of dental treatment and provide home-based dental health\ncare service more efciently. The trained model was used to\nrealize the detection and classication of dental diseases,', ' interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L. Oliveira, and M. A.\nGuevara Lopez, “Convolutional neural networks for mammography mass\nlesion classiﬁcation,” in Proc. Conf. IEEE Eng. Med. Biol. Soc. , 2015,\nvol. 2015, pp. 797–800.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n906 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\n[9] H. I. Suk, S. W. Lee, and D. Shen, “Deep sparse multi-task learning for\nfeature selection in alzheimer’s disease diagnosis,” Brain Struct. Funct. ,\nvol. 221, no. 5, pp. 2569–2587, 2016.\n[10] A. Kwasigroch, A. Mikoajczyk, and M. Grochowski, “Deep con-\nvolutional neural networks as a decision support tool in medical\nproblems—Malignant melanoma case study,” in Trends in Advanced\nIntelligent Control, Optimization and Automation (Advances in In-\ntelligent Systems and Computing). New Y ork, NY , USA: Springer,\n2017.\n[11] A. Esteva et al. , “Dermatologist-level classiﬁcation of skin cancer\nwith deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118,\n2017.\n[12] A. B. Oktay and Y . S. Akgul, “Diagnosis of degenerative intervertebral\ndisc disease with deep networks and SVM,” in Computer and Information\nSciences. New Y ork, NY , USA: Springer, 2016.\n[13] D. H. Wolpert and W. G. Macready, “No free lunch theorems for op-\ntimization,” IEEE Trans. Evol. Comput. , vol. 1, no. 1, pp. 67–82, Apr.\n1997.\n[14] S. A. Prajapati, R. Nagaraj, and S. Mitra, “Classiﬁcation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2538.0,free_text
A_Smart_Dental_Health_IoT_Platform_Based_on_Intelligent_Hardware_Deep_Learning_and_Mobile_Terminal,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[13, 2, 14]","[' interconnected devices promise\nmore efﬁcient and comprehensive health care,” IEEE Pulse , vol. 7, no. 5,\npp. 35–39, Sep./Oct. 2016.\n[7] P . Sundaravadivel, E. Kougianos, S. P . Mohanty, and M. K. Ganapathiraju,\n“Everything you wanted to know about smart health care: Evaluating the\ndifferent technologies and components of the internet of things for better\nhealth,” IEEE Consum. Electron. Mag. , vol. 7, no. 1, pp. 18–28, Jan. 2017.\n[8] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L. Oliveira, and M. A.\nGuevara Lopez, “Convolutional neural networks for mammography mass\nlesion classiﬁcation,” in Proc. Conf. IEEE Eng. Med. Biol. Soc. , 2015,\nvol. 2015, pp. 797–800.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\n906 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\n[9] H. I. Suk, S. W. Lee, and D. Shen, “Deep sparse multi-task learning for\nfeature selection in alzheimer’s disease diagnosis,” Brain Struct. Funct. ,\nvol. 221, no. 5, pp. 2569–2587, 2016.\n[10] A. Kwasigroch, A. Mikoajczyk, and M. Grochowski, “Deep con-\nvolutional neural networks as a decision support tool in medical\nproblems—Malignant melanoma case study,” in Trends in Advanced\nIntelligent Control, Optimization and Automation (Advances in In-\ntelligent Systems and Computing). New Y ork, NY , USA: Springer,\n2017.\n[11] A. Esteva et al. , “Dermatologist-level classiﬁcation of skin cancer\nwith deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118,\n2017.\n[12] A. B. Oktay and Y . S. Akgul, “Diagnosis of degenerative intervertebral\ndisc disease with deep networks and SVM,” in Computer and Information\nSciences. New Y ork, NY , USA: Springer, 2016.\n[13] D. H. Wolpert and W. G. Macready, “No free lunch theorems for op-\ntimization,” IEEE Trans. Evol. Comput. , vol. 1, no. 1, pp. 67–82, Apr.\n1997.\n[14] S. A. Prajapati, R. Nagaraj, and S. Mitra, “Classiﬁcation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—', ' [12],\nwhich is also used to implement the automated segmentation of\ngingival diseases from oral images [13]. In [14], the approaches\nof CNN and transfer learning are used in dental diseases, and\nthe above artiﬁcial intelligence methods for dental are based\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. \n\nLIU et al.: SMART DENT AL HEAL TH-IOT PLA TFORM BASED ON INTELLIGENT HARDWARE, DEEP LEARNING AND MOBILE TERMINAL 899\non X-ray. However, visual diagnosis serves as one of the most\nimportant parts of dental health care.\nThe smart dental health IoT system, due to the lack of coor-\ndinated IoT-based In-home dental health care services, is devel-\noped in this paper, which plays an important role in elaborating\nits applications in home and private dental clinic scenarios. The\nproposed system builds a home-centric, self-assisted, fully au-\ntomatic intelligent In-home dental healthcare solution by taking\nadvantage of embedded intelligent hardware technology, articial\nintelligence (AI), and the mobile terminal.\nThe system closely combines the personal self-service dental\nhealth examination and dental resources through studying the\nAI method to expand the scope and coverage of traditional\ndental health care, which is the major contribution made by the\nproposed iHome smart dental Health-IoT system. Patients can\nchoose medical resources online and receive an appointment for\nmedical treatment via self-service smart dental screening.\nThe rest parts of this paper are organized as follows.\nSection II discusses smart In-home dentist system, followed\nby Section III analyzing the intelligent dental diagnosis, intro-\nducing the algorithm ﬂow and test results. Section IV presents\nthe integrated system and hardware prototype, and reports the\napplication results, and a conclusion is given in Section V\nﬁnally.\nII. S MART IN-HOME DENTIST SYSTEM\nThe occurrence of dental disease can be well predictable. For\nexample, each stage from dental plaque, calculus to dental caries\nand periodontal disease shows different characteristics and so-\nlutions, thereby making it possible to monitor the whole body\nand reduce the risk of sudden diseases effectively. It can reduce\npain, save time and reduce the cost of money for patients. How-\never, most consumers lack the correct tooth awareness, thereby\nmaking the timely treatment for tooth disease difﬁcult. More-\nover, there is a relative shortage of dental resources in hospitals.\nThese main contradictions are explained in the following aspects\nin details:\nr First, there are a lot of patients with dental diseases, but\nfewer people pay attention to the teeth. Many individ-\nuals pay for treatment only when they have teeth with\nproblems, which may cause irreversible damage to the\nlong-term development of children.\nr Second, the cost of dental treatment is relatively high.\nr Third, dental disease, due to the characteristics of elective\ntreatment, is difﬁcult to get timely treatment.\nBased on what is discussed above, a promising trend in health-\ncare is to move routine medical checks and other healthcare ser-\nvices from hospital (Hospital-Centric) to the home environment\n(Home-centric) [15], through which, the patients can get seam-\nless health care at any time in a comfortable home environment.\nEven more, it can ensure that the overall iHome dental health-\ncare system including three partspersonalized services, dental\nservices and intelligent and interactive service could be opti-\nmized to a large extent. Based on this platform, the closed', 'cation of dental diseases\nusing CNN and transfer learning,” in Proc. Int. Symp. Comput. Bus. Intell. ,\n2017, pp. 70–74.\n[15] Z. Pang, Q. Chen, J. Tian, L. Zheng, and E. Dubrova, “Ecosystem analysis\nin the design of open platform-based in-home healthcare terminals towards\nthe internet-of-things,” in Proc. 15th Int. Conf. Adv. Commun. Technol. ,\n2013, pp. 529–534.\n[16] G. Xin, W. Chen, and J. Li, “Research on security monitoring and health\nmanagement system of UA V ,” in Proceedings of the First Symposium\non Aviation Maintenance and Management—V olume II (Lecture Notes in\nElectrical Engineering). New Y ork, NY , USA: Springer, 2014, pp. 631–\n639.\n[17] E. Provenzi, C. L. De, A. Rizzi, and D. Marini, “Mathematical deﬁnition\nand analysis of the Retinex algorithm,” J. Opt. Soc. Am. A , vol. 22, no. 12,\npp. 2613–2621, 2005.\n[18] S. Y . Liao and T. Q. Huang, “Video copy-move forgery detection and\nlocalization based on Tamura texture features,” in Proc. Int. Congr . Image\nSignal Process., 2014, pp. 864–868.\n[19] X. Wang, K. Chen, Z. Huang, C. Y ao, and W. Liu, “Point linking network\nfor object detection,” 2017, arXiv preprint arXiv:1706.03646.\n[20] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\nmulti-task network cascades,” in Proc. IEEE Conf. Comput. Vision Pattern\nRecognit., 2015, pp. 3150–3158.\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit. , 2015,\npp. 770–778.\n[22] S. Ren, R. Girshick, R. Girshick, and J. Sun, “Faster R-CNN: Towards\nreal-time object detection with region proposal networks,” IEEE Trans.\nPattern Anal. & Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, Jun. 2017.\n[23] Z. Xie, X. Zhang, D. Zeng, X. Chen, and W. Feng, “Design and imple-\nmentation of the remote wireless intraoral endoscope system,” in Proc.\nInt. Ind. Inform. Comput. Eng. Conf. , 2015, pp. 975–980.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 09:22:40 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2410.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,Diagnostic,,,,"[5, 4, 6]","['28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our knowledge,\nHeartPy is the most prominent and trusted public library to\nprovide HRV features from PPG signals. For the prediction\nof all the HRV features, we use a 4-layer TCN to get the\ninput representation from the PPG signal that, in turn is\npassed through 2 Fully Connected (FC) layers with hidden\nsizes of 64 to get the output prediction. We train the model\nusing a learning rate of 5e-05 and a dropout value of 0.3.\nWe evaluate the performance of our model against HeartPy\nusing mean absolute error (MAE) with the estimates from the\nBiopac sensor data considered the ground truth. We report\nthese results in Table I. Among all the parameters predicted,\nwe observe that for heart rate alone, HeartPy outperforms our\nproposed framework. For the remaining HRV parameters, our\nmodel performs better than HeartPy prediction. On average,\nthe improvement in HRV parameter estimation compared to\nHeartPy is around 26%.\nTo highlight the significance of fine-tuning, we predicted\nHRV features from the earbud PPG without any pre-training\nstep. The results of this experiment are shown in Table I.\nWith the exception of HR, we observe that transfer learning\nimproves performance for all the HRV features.\nThroughout all our experiments, we consistently observed\nrelatively poorer performance in average HR estimation using\nour proposed framework. This may be related to the fact that\nheart rate estimation is fundamentally different from HRV\nparameter estimation. While HRV estimation depends on the\nspecific sequence of individual peaks, HR is more of an\naverage of the distances between peaks. The framework was\nnot optimized for this averaging task and the removal of\noutliers that would adversely affect the average. Nevertheless,\nheart rate is an important feature and integrating its estimation\nin a unified framework can be the focus of future work.\nB. Stress detection\nIn this section, we report the performance of stress clas-\nsification using HRV parameters obtained from HeartPy and\nour model, respectively. The PPG window from a subject is\nlabeled as stress if it is sampled from the task that induces\nstress, which in our case are Dot-tracking, Speech-Preparation,\nSpeech, and Exciting Music tasks (see Figure 1 for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal', 'Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our', ' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2556.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",Unknown from this paper.,,,,"[2, 7, 8]","[' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN', ' detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI conference on human factors in computing systems. 2016.\n[6] Mishra, Varun, et al. ”Continuous detection of physiological stress with\ncommodity hardware.” ACM transactions on computing for healthcare\n1.2 (2020): 1-30.\n[7] Lee, Joong Hoon, et al. ”Stress monitoring using multimodal bio-sensing\nheadset.” Extended Abstracts of the 2020 CHI Conference on Human\nFactors in Computing Systems. 2020.\n[8] Song, J., Li, D., Ma, X., Teng, G. and Wei, J., 2019. PQR signal quality\nindexes: A method for real-time photoplethysmogram signal quality\nestimation based on noise interferences. Biomedical Signal Processing\nand Control, 47, pp.88-95.\n[9] Lea, C., Vidal, R., Reiter, A., & Hager, G. D. (2016). Temporal convolu-\ntional networks: A unified approach to action segmentation. In Computer\nVision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October\n8-10 and 15-16, 2016, Proceedings, Part III 14 (pp. 47-54). Springer\nInternational Publishing.\n[10] Van Gent, P., Farah, H., Van Nes, N., & Van Arem, B. (2019).\nHeartPy: A novel heart rate algorithm for the analysis of noisy signals.\nTransportation research part F: traffic psychology and behaviour, 66,\n368-378.\n[11] van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2019). Analysing\nnoisy driver physiology real-time using off-the-shelf sensors: Heart rate\nanalysis software from the taking the fast lane project. Journal of Open\nResearch Software, 7(1).\n[12] Jing, L., & Tian, Y . (2020). Self-supervised visual feature learning with\ndeep neural networks: A survey. IEEE transactions on pattern analysis\nand machine intelligence, 43(11), 4037-4058\n[13] Ziegler, A., & Asano, Y . M. (2022). Self-supervised learning of object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[', ' object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[16] Rahman, M. M., Xu, X., Nathan, V ., Ahmed, T., Ahmed, M. Y ., McCaf-\nfrey, D., ... & Gao, J. A. (2022, July). Detecting Physiological Responses\nUsing Multimodal Earbud Sensors. In 2022 44th Annual International\nConference of the IEEE Engineering in Medicine & Biology Society\n(EMBC) (pp. 01-05). IEEE.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2044.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Unknown from this paper.,,,,"[2, 3, 4]","[' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned', 'Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2558.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q4,How was the dataset described in this study before predictive modeling was performed?,Unknown from this paper.,,,,"[5, 2, 6]","['28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our knowledge,\nHeartPy is the most prominent and trusted public library to\nprovide HRV features from PPG signals. For the prediction\nof all the HRV features, we use a 4-layer TCN to get the\ninput representation from the PPG signal that, in turn is\npassed through 2 Fully Connected (FC) layers with hidden\nsizes of 64 to get the output prediction. We train the model\nusing a learning rate of 5e-05 and a dropout value of 0.3.\nWe evaluate the performance of our model against HeartPy\nusing mean absolute error (MAE) with the estimates from the\nBiopac sensor data considered the ground truth. We report\nthese results in Table I. Among all the parameters predicted,\nwe observe that for heart rate alone, HeartPy outperforms our\nproposed framework. For the remaining HRV parameters, our\nmodel performs better than HeartPy prediction. On average,\nthe improvement in HRV parameter estimation compared to\nHeartPy is around 26%.\nTo highlight the significance of fine-tuning, we predicted\nHRV features from the earbud PPG without any pre-training\nstep. The results of this experiment are shown in Table I.\nWith the exception of HR, we observe that transfer learning\nimproves performance for all the HRV features.\nThroughout all our experiments, we consistently observed\nrelatively poorer performance in average HR estimation using\nour proposed framework. This may be related to the fact that\nheart rate estimation is fundamentally different from HRV\nparameter estimation. While HRV estimation depends on the\nspecific sequence of individual peaks, HR is more of an\naverage of the distances between peaks. The framework was\nnot optimized for this averaging task and the removal of\noutliers that would adversely affect the average. Nevertheless,\nheart rate is an important feature and integrating its estimation\nin a unified framework can be the focus of future work.\nB. Stress detection\nIn this section, we report the performance of stress clas-\nsification using HRV parameters obtained from HeartPy and\nour model, respectively. The PPG window from a subject is\nlabeled as stress if it is sampled from the task that induces\nstress, which in our case are Dot-tracking, Speech-Preparation,\nSpeech, and Exciting Music tasks (see Figure 1 for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal', ' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN', ' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2546.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?",Unknown from this paper.,,,,"[3, 8, 6]","[' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned', ' object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[16] Rahman, M. M., Xu, X., Nathan, V ., Ahmed, T., Ahmed, M. Y ., McCaf-\nfrey, D., ... & Gao, J. A. (2022, July). Detecting Physiological Responses\nUsing Multimodal Earbud Sensors. In 2022 44th Annual International\nConference of the IEEE Engineering in Medicine & Biology Society\n(EMBC) (pp. 01-05). IEEE.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. ', ' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2055.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q6,What preprocessing techniques on the included variables/features were applied in this study?,"Bandpass filtering, windowing, rSQI removal, standardization.",,,,"[3, 2, 4]","[' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned', ' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN', 'Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2565.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q7,How is missing data handled in this study?,Signals with low rSQI removed; noisy data excluded.,,,,"[3, 2, 0]","[' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned', ' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN', 'A Framework for Extracting Heart Rate Variability\nFeatures from Earbud-PPG for Stress Detection\nBhanu Teja Gullapalli\nUniversity of California San Diego\nSan Diego, USA\nbgullapalli@ucsd.edu\nViswam Nathan\nDigital Health Lab\nSamsung Research America\nMountain View, USA\nviswam.nathan@samsung.com\nMd Mahbubur Rahman\nDigital Health Lab\nSamsung Research America\nMountain View, USA\nm.rahman2@samsung.com\nJilong Kuang\nDigital Health Lab\nSamsung Research America\nMountain View, USA\njilong.kuang@samsung.com\nJun Alex Gao\nDigital Health Lab\nSamsung Research America\nMountain View, USA\nalex.gao@samsung.com\nAbstract—Unmanaged stress can lead to serious physiological\nand mental health issues, making it important to monitor stress\ncontinuously using wearable sensors. In this work, we investigate\nthe use of photoplethysmography (PPG) sensors in consumer-\ngrade earbud devices to classify periods of stress. We propose a\nframework that initially employs deep learning to predict heart\nrate variability (HRV) features from the relatively noisy earbud\nPPG signal. Subsequently, we transfer this knowledge and use\nthese features for the downstream task of stress classification.\nThe proposed framework outperforms an existing state-of-the-\nart library on HRV feature extraction, resulting in at least 5%\nimprovement in accuracy, sensitivity, and specificity for stress\ndetection.\nIndex Terms—Photoplethysmography (PPG), heart rate vari-\nability (HRV), transfer learning, temporal convolutional networks\n(TCN)\nI. I NTRODUCTION\nStress is the body’s physical and emotional reaction in\nresponse to internal or external stressors. It is ubiquitous, with\nat least 1 in 2 Americans stressed during the day and around\n1 million workers missing work due to stress according to\nthe American Institute of Stress and American Psychological\nAssociation [1]. These numbers are higher than ever after\nthe COVID-19 pandemic, inflation, and global uncertainty.\nBeyond immediate symptoms such as sleep deprivation or\nheadaches, unchecked stress can lead to severe cardiovascular\nor neurological illness. Many individuals can be unaware of the\ntiming and extent of their stress episodes, making it challeng-\ning to utilize interventions such as meditation and mindfulness\nto alleviate stress. The efficiency of any stress management\nsolution would be greatly improved by continuous and real-\ntime detection of stress episodes over the course of each day.\nDigital health solutions, particularly wearable devices, offer\na convenient and low-cost avenue for detecting stress episodes.\nSeveral studies have shown that the sensor modalities in\nwearable devices are capable of automatically detecting stress.\nIn earlier works, built-in smartphone accelerometer sensors\nwere used to characterize the subject’s behavior which is\nsubsequently used to classify stress levels [2], [3], though this\ncan be prone to false positives and may not be discerning\nenough to specifically detect stress. In one previous work,\na smartphone and smartwatch were used to capture electro-\ndermal activity (EDA) to passively detect stress in field with\n75% accuracy [4]. Another study employed a custom-made\nwearable chest band for passive ECG sensing and detected\nstress with a 0.71 F1-score [5]. There has also been an\napproach that combined a commodity chestband to capture\nECG and a commodity wristband called Empatica for EDA\nsensor to detect stress with 0.94 F1-score [6]. Stress detection\nusing head-worn devices is relatively less common. Another\nrecent work demonstrated a head-neck device capable of\nsimultaneously capturing ECG and EEG, which can detect\n’stressed’ states with 75.8% accuracy [7]. While these results\nshow promise for the use of wearable devices in']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2569.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[7, 8, 2]","[' detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI conference on human factors in computing systems. 2016.\n[6] Mishra, Varun, et al. ”Continuous detection of physiological stress with\ncommodity hardware.” ACM transactions on computing for healthcare\n1.2 (2020): 1-30.\n[7] Lee, Joong Hoon, et al. ”Stress monitoring using multimodal bio-sensing\nheadset.” Extended Abstracts of the 2020 CHI Conference on Human\nFactors in Computing Systems. 2020.\n[8] Song, J., Li, D., Ma, X., Teng, G. and Wei, J., 2019. PQR signal quality\nindexes: A method for real-time photoplethysmogram signal quality\nestimation based on noise interferences. Biomedical Signal Processing\nand Control, 47, pp.88-95.\n[9] Lea, C., Vidal, R., Reiter, A., & Hager, G. D. (2016). Temporal convolu-\ntional networks: A unified approach to action segmentation. In Computer\nVision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October\n8-10 and 15-16, 2016, Proceedings, Part III 14 (pp. 47-54). Springer\nInternational Publishing.\n[10] Van Gent, P., Farah, H., Van Nes, N., & Van Arem, B. (2019).\nHeartPy: A novel heart rate algorithm for the analysis of noisy signals.\nTransportation research part F: traffic psychology and behaviour, 66,\n368-378.\n[11] van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2019). Analysing\nnoisy driver physiology real-time using off-the-shelf sensors: Heart rate\nanalysis software from the taking the fast lane project. Journal of Open\nResearch Software, 7(1).\n[12] Jing, L., & Tian, Y . (2020). Self-supervised visual feature learning with\ndeep neural networks: A survey. IEEE transactions on pattern analysis\nand machine intelligence, 43(11), 4037-4058\n[13] Ziegler, A., & Asano, Y . M. (2022). Self-supervised learning of object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[', ' object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[16] Rahman, M. M., Xu, X., Nathan, V ., Ahmed, T., Ahmed, M. Y ., McCaf-\nfrey, D., ... & Gao, J. A. (2022, July). Detecting Physiological Responses\nUsing Multimodal Earbud Sensors. In 2022 44th Annual International\nConference of the IEEE Engineering in Medicine & Biology Society\n(EMBC) (pp. 01-05). IEEE.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. ', ' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2030.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q9,Which prediction models were used in this study?,random forest classifier,,,,"[6, 1, 2]","[' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI', ' (EDA) to passively detect stress in field with\n75% accuracy [4]. Another study employed a custom-made\nwearable chest band for passive ECG sensing and detected\nstress with a 0.71 F1-score [5]. There has also been an\napproach that combined a commodity chestband to capture\nECG and a commodity wristband called Empatica for EDA\nsensor to detect stress with 0.94 F1-score [6]. Stress detection\nusing head-worn devices is relatively less common. Another\nrecent work demonstrated a head-neck device capable of\nsimultaneously capturing ECG and EEG, which can detect\n’stressed’ states with 75.8% accuracy [7]. While these results\nshow promise for the use of wearable devices in daily stress\ndetection, a common limitation, aside from smartwatches and\nsmartphones, is that these devices are expensive, invasive\nfor everyday use, and typically serve only a single purpose.\nEarbuds, as a wearable, have been gaining traction and in-\ncreasingly adopted. From a signal processing perspective, they\noffer certain advantages over smartwatches. These include\nlower susceptibility to motion, a relatively fixed orientation,\nand their more consistent wear pattern among users. Despite\nthese advantages, earbuds are more prone to noise interference\nthan the more stable but less conveniently wearable options\nlike chest bands or finger sensors. Therefore, earbuds could\ngreatly benefit from the application of more sophisticated\nsignal processing and machine learning approaches. The main\ncontribution of our work was designing a transfer-learning\nbased framework for heart rate variability (HRV) feature\nextraction from noisy earbud PPG signal. We also show the\nstress detection performance using our proposed framework on\nhuman subjects with reference sensors undergoing validated\nstressor tasks.\n2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) | 979-8-3503-7149-9/24/$31.00 ©2024 IEEE | DOI: 10.1109/EMBC53108.2024.10782088\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nII. D ATA COLLECTION AND CLEANING\nA. Study Protocol\nThis was an experimental study approved by the Institu-\ntional Review Board of the University of California, San Fran-\ncisco. The aim of this study was to detect stress from the PPG\nsignals captured by earbud sensors. Eighteen subjects were\nrecruited for this study, and they were all healthy individuals\nfrom the 19-22 age group to ensure that the stress responses\ninduced are minimally affected by other confounders and\ncomorbidities. Throughout the study, subjects wore earbuds\nthat captured PPG at 25Hz, accelerometer at 50Hz, and Core\nbody temperature (CBT) at 1Hz. The reference ground truth\nPPG from subjects was captured using a Biopac device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time', ' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,Temporal convolution network chosen; fine-tuned for HRV parameters.,,,,"[3, 7, 4]","[' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned', ' detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI conference on human factors in computing systems. 2016.\n[6] Mishra, Varun, et al. ”Continuous detection of physiological stress with\ncommodity hardware.” ACM transactions on computing for healthcare\n1.2 (2020): 1-30.\n[7] Lee, Joong Hoon, et al. ”Stress monitoring using multimodal bio-sensing\nheadset.” Extended Abstracts of the 2020 CHI Conference on Human\nFactors in Computing Systems. 2020.\n[8] Song, J., Li, D., Ma, X., Teng, G. and Wei, J., 2019. PQR signal quality\nindexes: A method for real-time photoplethysmogram signal quality\nestimation based on noise interferences. Biomedical Signal Processing\nand Control, 47, pp.88-95.\n[9] Lea, C., Vidal, R., Reiter, A., & Hager, G. D. (2016). Temporal convolu-\ntional networks: A unified approach to action segmentation. In Computer\nVision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October\n8-10 and 15-16, 2016, Proceedings, Part III 14 (pp. 47-54). Springer\nInternational Publishing.\n[10] Van Gent, P., Farah, H., Van Nes, N., & Van Arem, B. (2019).\nHeartPy: A novel heart rate algorithm for the analysis of noisy signals.\nTransportation research part F: traffic psychology and behaviour, 66,\n368-378.\n[11] van Gent, P., Farah, H., van Nes, N., & van Arem, B. (2019). Analysing\nnoisy driver physiology real-time using off-the-shelf sensors: Heart rate\nanalysis software from the taking the fast lane project. Journal of Open\nResearch Software, 7(1).\n[12] Jing, L., & Tian, Y . (2020). Self-supervised visual feature learning with\ndeep neural networks: A survey. IEEE transactions on pattern analysis\nand machine intelligence, 43(11), 4037-4058\n[13] Ziegler, A., & Asano, Y . M. (2022). Self-supervised learning of object\nparts for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 14502-\n14511).\n[14] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805\n[15] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., ... & Amodei, D. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33, 1877-1901.\n[', 'Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2563.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q11,How was data augmentation or generation used in this study?,Unknown from this paper.,,,,"[4, 6, 0]","['Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our', ' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI', 'A Framework for Extracting Heart Rate Variability\nFeatures from Earbud-PPG for Stress Detection\nBhanu Teja Gullapalli\nUniversity of California San Diego\nSan Diego, USA\nbgullapalli@ucsd.edu\nViswam Nathan\nDigital Health Lab\nSamsung Research America\nMountain View, USA\nviswam.nathan@samsung.com\nMd Mahbubur Rahman\nDigital Health Lab\nSamsung Research America\nMountain View, USA\nm.rahman2@samsung.com\nJilong Kuang\nDigital Health Lab\nSamsung Research America\nMountain View, USA\njilong.kuang@samsung.com\nJun Alex Gao\nDigital Health Lab\nSamsung Research America\nMountain View, USA\nalex.gao@samsung.com\nAbstract—Unmanaged stress can lead to serious physiological\nand mental health issues, making it important to monitor stress\ncontinuously using wearable sensors. In this work, we investigate\nthe use of photoplethysmography (PPG) sensors in consumer-\ngrade earbud devices to classify periods of stress. We propose a\nframework that initially employs deep learning to predict heart\nrate variability (HRV) features from the relatively noisy earbud\nPPG signal. Subsequently, we transfer this knowledge and use\nthese features for the downstream task of stress classification.\nThe proposed framework outperforms an existing state-of-the-\nart library on HRV feature extraction, resulting in at least 5%\nimprovement in accuracy, sensitivity, and specificity for stress\ndetection.\nIndex Terms—Photoplethysmography (PPG), heart rate vari-\nability (HRV), transfer learning, temporal convolutional networks\n(TCN)\nI. I NTRODUCTION\nStress is the body’s physical and emotional reaction in\nresponse to internal or external stressors. It is ubiquitous, with\nat least 1 in 2 Americans stressed during the day and around\n1 million workers missing work due to stress according to\nthe American Institute of Stress and American Psychological\nAssociation [1]. These numbers are higher than ever after\nthe COVID-19 pandemic, inflation, and global uncertainty.\nBeyond immediate symptoms such as sleep deprivation or\nheadaches, unchecked stress can lead to severe cardiovascular\nor neurological illness. Many individuals can be unaware of the\ntiming and extent of their stress episodes, making it challeng-\ning to utilize interventions such as meditation and mindfulness\nto alleviate stress. The efficiency of any stress management\nsolution would be greatly improved by continuous and real-\ntime detection of stress episodes over the course of each day.\nDigital health solutions, particularly wearable devices, offer\na convenient and low-cost avenue for detecting stress episodes.\nSeveral studies have shown that the sensor modalities in\nwearable devices are capable of automatically detecting stress.\nIn earlier works, built-in smartphone accelerometer sensors\nwere used to characterize the subject’s behavior which is\nsubsequently used to classify stress levels [2], [3], though this\ncan be prone to false positives and may not be discerning\nenough to specifically detect stress. In one previous work,\na smartphone and smartwatch were used to capture electro-\ndermal activity (EDA) to passively detect stress in field with\n75% accuracy [4]. Another study employed a custom-made\nwearable chest band for passive ECG sensing and detected\nstress with a 0.71 F1-score [5]. There has also been an\napproach that combined a commodity chestband to capture\nECG and a commodity wristband called Empatica for EDA\nsensor to detect stress with 0.94 F1-score [6]. Stress detection\nusing head-worn devices is relatively less common. Another\nrecent work demonstrated a head-neck device capable of\nsimultaneously capturing ECG and EEG, which can detect\n’stressed’ states with 75.8% accuracy [7]. While these results\nshow promise for the use of wearable devices in']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2568.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared to HeartPy baseline.",,,,"[6, 5, 3]","[' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI', '28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our knowledge,\nHeartPy is the most prominent and trusted public library to\nprovide HRV features from PPG signals. For the prediction\nof all the HRV features, we use a 4-layer TCN to get the\ninput representation from the PPG signal that, in turn is\npassed through 2 Fully Connected (FC) layers with hidden\nsizes of 64 to get the output prediction. We train the model\nusing a learning rate of 5e-05 and a dropout value of 0.3.\nWe evaluate the performance of our model against HeartPy\nusing mean absolute error (MAE) with the estimates from the\nBiopac sensor data considered the ground truth. We report\nthese results in Table I. Among all the parameters predicted,\nwe observe that for heart rate alone, HeartPy outperforms our\nproposed framework. For the remaining HRV parameters, our\nmodel performs better than HeartPy prediction. On average,\nthe improvement in HRV parameter estimation compared to\nHeartPy is around 26%.\nTo highlight the significance of fine-tuning, we predicted\nHRV features from the earbud PPG without any pre-training\nstep. The results of this experiment are shown in Table I.\nWith the exception of HR, we observe that transfer learning\nimproves performance for all the HRV features.\nThroughout all our experiments, we consistently observed\nrelatively poorer performance in average HR estimation using\nour proposed framework. This may be related to the fact that\nheart rate estimation is fundamentally different from HRV\nparameter estimation. While HRV estimation depends on the\nspecific sequence of individual peaks, HR is more of an\naverage of the distances between peaks. The framework was\nnot optimized for this averaging task and the removal of\noutliers that would adversely affect the average. Nevertheless,\nheart rate is an important feature and integrating its estimation\nin a unified framework can be the focus of future work.\nB. Stress detection\nIn this section, we report the performance of stress clas-\nsification using HRV parameters obtained from HeartPy and\nour model, respectively. The PPG window from a subject is\nlabeled as stress if it is sampled from the task that induces\nstress, which in our case are Dot-tracking, Speech-Preparation,\nSpeech, and Exciting Music tasks (see Figure 1 for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q13,Which type of explainability techniques are used?,Unknown from this paper.,,,,"[6, 4, 2]","[' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI', 'Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our', ' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2551.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,"Accuracy, Sensitivity, Specificity, MAE.",,,,"[6, 5, 3]","[' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI', '28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our knowledge,\nHeartPy is the most prominent and trusted public library to\nprovide HRV features from PPG signals. For the prediction\nof all the HRV features, we use a 4-layer TCN to get the\ninput representation from the PPG signal that, in turn is\npassed through 2 Fully Connected (FC) layers with hidden\nsizes of 64 to get the output prediction. We train the model\nusing a learning rate of 5e-05 and a dropout value of 0.3.\nWe evaluate the performance of our model against HeartPy\nusing mean absolute error (MAE) with the estimates from the\nBiopac sensor data considered the ground truth. We report\nthese results in Table I. Among all the parameters predicted,\nwe observe that for heart rate alone, HeartPy outperforms our\nproposed framework. For the remaining HRV parameters, our\nmodel performs better than HeartPy prediction. On average,\nthe improvement in HRV parameter estimation compared to\nHeartPy is around 26%.\nTo highlight the significance of fine-tuning, we predicted\nHRV features from the earbud PPG without any pre-training\nstep. The results of this experiment are shown in Table I.\nWith the exception of HR, we observe that transfer learning\nimproves performance for all the HRV features.\nThroughout all our experiments, we consistently observed\nrelatively poorer performance in average HR estimation using\nour proposed framework. This may be related to the fact that\nheart rate estimation is fundamentally different from HRV\nparameter estimation. While HRV estimation depends on the\nspecific sequence of individual peaks, HR is more of an\naverage of the distances between peaks. The framework was\nnot optimized for this averaging task and the removal of\noutliers that would adversely affect the average. Nevertheless,\nheart rate is an important feature and integrating its estimation\nin a unified framework can be the focus of future work.\nB. Stress detection\nIn this section, we report the performance of stress clas-\nsification using HRV parameters obtained from HeartPy and\nour model, respectively. The PPG window from a subject is\nlabeled as stress if it is sampled from the task that induces\nstress, which in our case are Dot-tracking, Speech-Preparation,\nSpeech, and Exciting Music tasks (see Figure 1 for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2555.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,Mean absolute error compared to Biopac ground truth.,,,,"[5, 2, 3]","['28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our knowledge,\nHeartPy is the most prominent and trusted public library to\nprovide HRV features from PPG signals. For the prediction\nof all the HRV features, we use a 4-layer TCN to get the\ninput representation from the PPG signal that, in turn is\npassed through 2 Fully Connected (FC) layers with hidden\nsizes of 64 to get the output prediction. We train the model\nusing a learning rate of 5e-05 and a dropout value of 0.3.\nWe evaluate the performance of our model against HeartPy\nusing mean absolute error (MAE) with the estimates from the\nBiopac sensor data considered the ground truth. We report\nthese results in Table I. Among all the parameters predicted,\nwe observe that for heart rate alone, HeartPy outperforms our\nproposed framework. For the remaining HRV parameters, our\nmodel performs better than HeartPy prediction. On average,\nthe improvement in HRV parameter estimation compared to\nHeartPy is around 26%.\nTo highlight the significance of fine-tuning, we predicted\nHRV features from the earbud PPG without any pre-training\nstep. The results of this experiment are shown in Table I.\nWith the exception of HR, we observe that transfer learning\nimproves performance for all the HRV features.\nThroughout all our experiments, we consistently observed\nrelatively poorer performance in average HR estimation using\nour proposed framework. This may be related to the fact that\nheart rate estimation is fundamentally different from HRV\nparameter estimation. While HRV estimation depends on the\nspecific sequence of individual peaks, HR is more of an\naverage of the distances between peaks. The framework was\nnot optimized for this averaging task and the removal of\noutliers that would adversely affect the average. Nevertheless,\nheart rate is an important feature and integrating its estimation\nin a unified framework can be the focus of future work.\nB. Stress detection\nIn this section, we report the performance of stress clas-\nsification using HRV parameters obtained from HeartPy and\nour model, respectively. The PPG window from a subject is\nlabeled as stress if it is sampled from the task that induces\nstress, which in our case are Dot-tracking, Speech-Preparation,\nSpeech, and Exciting Music tasks (see Figure 1 for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal', ' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2552.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?","Accuracy, specificity, and sensitivity metrics.",,,,"[1, 6, 3]","[' (EDA) to passively detect stress in field with\n75% accuracy [4]. Another study employed a custom-made\nwearable chest band for passive ECG sensing and detected\nstress with a 0.71 F1-score [5]. There has also been an\napproach that combined a commodity chestband to capture\nECG and a commodity wristband called Empatica for EDA\nsensor to detect stress with 0.94 F1-score [6]. Stress detection\nusing head-worn devices is relatively less common. Another\nrecent work demonstrated a head-neck device capable of\nsimultaneously capturing ECG and EEG, which can detect\n’stressed’ states with 75.8% accuracy [7]. While these results\nshow promise for the use of wearable devices in daily stress\ndetection, a common limitation, aside from smartwatches and\nsmartphones, is that these devices are expensive, invasive\nfor everyday use, and typically serve only a single purpose.\nEarbuds, as a wearable, have been gaining traction and in-\ncreasingly adopted. From a signal processing perspective, they\noffer certain advantages over smartwatches. These include\nlower susceptibility to motion, a relatively fixed orientation,\nand their more consistent wear pattern among users. Despite\nthese advantages, earbuds are more prone to noise interference\nthan the more stable but less conveniently wearable options\nlike chest bands or finger sensors. Therefore, earbuds could\ngreatly benefit from the application of more sophisticated\nsignal processing and machine learning approaches. The main\ncontribution of our work was designing a transfer-learning\nbased framework for heart rate variability (HRV) feature\nextraction from noisy earbud PPG signal. We also show the\nstress detection performance using our proposed framework on\nhuman subjects with reference sensors undergoing validated\nstressor tasks.\n2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) | 979-8-3503-7149-9/24/$31.00 ©2024 IEEE | DOI: 10.1109/EMBC53108.2024.10782088\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nII. D ATA COLLECTION AND CLEANING\nA. Study Protocol\nThis was an experimental study approved by the Institu-\ntional Review Board of the University of California, San Fran-\ncisco. The aim of this study was to detect stress from the PPG\nsignals captured by earbud sensors. Eighteen subjects were\nrecruited for this study, and they were all healthy individuals\nfrom the 19-22 age group to ensure that the stress responses\ninduced are minimally affected by other confounders and\ncomorbidities. Throughout the study, subjects wore earbuds\nthat captured PPG at 25Hz, accelerometer at 50Hz, and Core\nbody temperature (CBT) at 1Hz. The reference ground truth\nPPG from subjects was captured using a Biopac device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time', ' for more\ndetails), PPG windows from the remaining tasks are labeled\nHeartPy feature\nreplaced Accuracy Sensitivity Specificity\n- 0.81 0.74 0.83\nRMSSD 0.86 0.80 0.87\npNN50 0.86 0.80 0.88\nSDNN 0.86 0.79 0.87\nHFnorm 0.87 0.81 0.88\nTABLE II: Stress detection results when one of the HeartPy\nHRV features is replaced with our model predictions\nas no-stress. As a first step, we classify stress using all the\nHRV features that the HeartPy library outputs from a window\nof PPG signal, using a random forest classifier developed\nin our previous work. In the next step, we replace one of\nthe HeartPy’s RMSSD, pNN50, SDNN, HFnorm with our\nmodel predictions and repeat the stress classification task.\nThe increase/decrease in the performance of stress detection\nby replacing HeartPy predictions with our model predictions\ngives us a better understanding of how to evaluate these\nparameters. The results of this experiment are shown in Table\nII. We also experimented with stress detection by using all\nthe features as predicted by our framework. However, given\nthe prominent role of HR in estimating stress, this model’s\nperformance was relatively lower compared to ones reported\nhere. We evaluate the performance of stress detection using\nAccuracy, Specificity, and Sensitivity. From this table, we can\nsee that using the HRV parameter predicted by our framework,\ninstead of the HeartPy prediction, has resulted in at least a 5%\nimprovement in all evaluation metrics for all HRV features.\nThis demonstrates the impact of our proposed framework on\nfeature extraction in stress classification.\nV. C ONCLUSION\nEstimating HRV features using wearable sensors is critical\nfor stress management solutions. In our current work, we take\nan initial step toward using earbud PPG signals to estimate\nHRV features, employing a framework based on transfer\nlearning and TCN architecture. We have demonstrated that\nour framework estimates these features more accurately than\nthe existing open-source library, HeartPy. Subsequently, we\nobserved that using our model’s predicted HRV features in\nplace of HeartPy for stress detection results in improved\nperformance. Encouraged by these initial results, we plan to\nconduct our study on a larger and more diverse set of subjects\nto corroborate our findings.\nREFERENCES\n[1] Stress in America, American Psychological Association,\nhttps://www.apa.org/news/press/releases/stress\n[2] Garcia-Ceja, Enrique, Venet Osmani, and Oscar Mayora. ”Automatic\nstress detection in working environments from smartphones’ accelerom-\neter data: a first step.” IEEE journal of biomedical and health informatics\n20.4 (2015): 1053-1060.\n[3] Can, Yekta Said, Bert Arnrich, and Cem Ersoy. ”Stress detection in\ndaily life scenarios using smart phones and wearable sensors: A survey.”\nJournal of biomedical informatics 92 (2019): 103139.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\n[4] Sano, Akane, and Rosalind W. Picard. ”Stress recognition using wear-\nable sensors and mobile phones.” 2013 Humaine association conference\non affective computing and intelligent interaction. IEEE, 2013.\n[5] Sarker, Hillol, et al. ”Finding significant stress episodes in a discontin-\nuous time series of rapidly varying mobile sensor data.” Proceedings of\nthe 2016 CHI', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2570.0,free_text
A_Framework_for_Extracting_Heart_Rate_Variability_Features_from_Earbud_PPG_for_Stress_Detection,Q17,What limitations of the study were discussed?,Unknown from this paper,,,,"[2, 4, 3]","[' device placed\non the finger that sampled PPG at 1000Hz. For our current\nwork, we will only focus on the PPG signal.\nThe study involved each subject completing a set of validated\ntasks that induced stress, interspersed with tasks that help them\nrelax. The study started with a baseline-recording for 5 min-\nutes where the subject sat idle. The subject then performed a\nmultiple object tracking task for 3 minutes (cognitive stressor),\nfollowed by 3 minutes of progressive muscle relaxation. The\nsubject then underwent a “Speech” task; a research assistant,\nplaying the role of an interviewer, asked the subject to speak\non a challenging and personal topic (social stressor). The\nsubject was then given 3 minute time to prepare for the speech,\nafter which they entered a room consisting of evaluators. The\nresearch assistant purposefully delayed the start of the speech\nto increase the stress. After a 2-minute delay, the subject was\nasked to leave without completing the task. Subsequently, the\nsubject listened to relaxing music for 4 minutes, followed\nby exciting music. Finally, the subject was asked to rest and\nrecover. Between each task, the subject was given at least a\n2-minute break to offset the effect of the previous task and\nprepare for the next task. In Figure 1, we show the timeline\nof the tasks and how the heart rate varies during these tasks\nfor a sample subject.\nB. PPG signal-denoising\nThe raw PPG signals captured from the Earbud and Biopac\nwere first filtered by a third-order Butterworth bandpass filter\nwith cut-off frequencies set to 0.6-4 Hz. The filtered PPG\nsignal was then sliced into 30-second sliding windows with a\n20-second overlap for the prediction of HRV parameters and\nstress detection. Even after passing the PPG signal through a\nbandpass filter, the signal still contained motion artifacts, shifts\nin baseline wandering, and other high-frequency noises. To\naddress this, we estimated the quality of the PPG signal using\nthe PQR signal quality index [8]. In this method, we calculated\nP/Q/R indexes representing the signal quality for each window\nof PPG signal. The “P” index indicated the degree of high-\nfrequency noise in the signal, calculated using the change in\nthe number of extremum points after bandpass filtering. The\n“Q” index indicated the amount of baseline wandering, cal-\nculated using the signal’s maximum and minimum amplitude.\nThe ”R” index represented the frequency of motion artifacts\nby calculating the dispersion of peak and valley points. Using\nthese three indexes, the method calculated rSQI (relative signal\nquality index) to compare the quality of the PPG signal against\na clean reference signal. In our case, we used the signal\ncollected from the baseline-recording as a reference signal.\nThe PPG-windowed signals whose rSQI ≥ 0, indicating the\nsignal was comparable or better in quality than the reference\nsignal, were considered for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN', 'Speech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned to predict different HRV-based features from the earbud\nPPG signal. The trained TCN network is transferred here, with\nonly the final layer changed to predict various HRV parameters\ninstead of peaks. In our current work, we considered five\ndifferent features, including the mean heart rate (HR) and\nthe following HRV features: root mean square of successive\ndifferences between normal heartbeats (RMSSD), proportion\nof the number of pairs of successive NN (R-R) intervals\nthat differ by more than 50 ms (pNN50), standard deviation\nof normal to normal R-R intervals (SDNN), and normalized\nhigh frequency power (HFnorm). We fine-tuned the pre-trained\nmodel separately for each of these HRV parameters. As we\nconsidered features that are continuous values, this required\nchanging only the loss function from the pre-trained model.\nWhile our framework can be extended to predict any HRV-\nbased parameter, we chose these five as they were the most\npredictive of stress in our prior empirical analysis [16]. An\noverview of our framework is shown in Figure 2.\nIV. R ESULTS\nUsing the framework described in the previous section, we\ninitially trained the upstream model to predict the peaks of the\ninput Biopac signal. This upstream model was subsequently\nfine-tuned on earbud PPG signal to predict different features\nseparately: HR, RMSSD, pNN50, SDNN, and HFnorm. Ul-\ntimately, using these features, we predict whether the subject\nis feeling stress or not for each PPG signal window. We con-\nducted all our analysis using Leave-One-Subject-Out Cross-\nValidation (LOSOXV), where the data from a single subject\nwas held out for testing while the model was trained on the\nremaining subjects. This process was repeated until all subjects\nhad been tested.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nHeartPy Our\nframework\nOur framework\nwithout pre-training\nHRV\nfeature MAE MAE MAE\nHR 5.9 ± 2.6 10.8 ± 3 10 ± 3.28\nRMSSD 16.7 ± 2.4 11.2 ± 2.9 14.74 ± 7.9\npNN50 0.17 ± 0.03 0.14 ± 0.02 0.15 ± 0.03\nSDNN 27.2 ± 8.1 17.3 ± 3.5 20.7 ± 7.9\nHFnorm 28.4 ± 5.8 23.2 ± 2.6 24.5 ± 4.3\nTABLE I: Comparison of HRV feature estimation\nA. Feature Estimation\nThe feature values predicted by our model are compared\nagainst HeartPy predictions. To the best of our', ' for subsequent analysis. The PPG-\nwindowed signals with rSQI < 0 are removed. Finally, we\nstandardized the PPG signals of each subject to remove any\nsubject-dependent information. Due to poor contact on one\nor more of the earbuds or Biopac finger sensor resulting in\nsignals too noisy to be usable, we excluded PPG data from\nfour subjects.\nIII. T RANSFER LEARNING BASED FRAMEWORK FOR\nEXTRACTING HRV FEATURES\nTo extract HRV based features from the denoised earbud\nPPG signal, we design a framework based on transfer learning\nand using Temporal convolution network (TCN) [9]. The\nframework consists of two steps: In the first step we pre-train\na TCN model on Biopac PPG to detect position of peaks.\nFor the latter step, we use this pre-trained TCN model for the\ndownstream task of predicting different HRV parameters. We\nexplain these steps in detail below.\nA. Pre-training using Biopac PPG\nThe growing availability of ubiquitous and unobtrusive\nwearable devices has made the collection of continuous sensor\ndata more convenient and accessible than ever. However,\nlabeling this data requires significant effort and time. As a\nresult, the amount of labeled sensor data available is minimal\nin comparison to the unlabeled data that can be collected. We\nfirst pre-train a model with the reference PPG signals captured\nfrom the Biopac finger sensor to detect peak positions of this\ninput signal. We chose this task because all the HRV-related\nfeatures extracted from the PPG signal are derived from the\nposition of peaks. The primary motivation for pre-training a\nmodel on the PPG data is to increase data efficiency given the\nrelative scarcity of available earbud PPG data, and to acquaint\nthe model with the PPG signal. We use HeartPy [10], [11], an\nopen-source python library for extracting ground truth peak\npositions. The idea of initiating models on hand-designed tasks\nbefore deploying them for different downstream purposes is\nnot uncommon. In computer vision, models are often pre-\ntrained with image segmentation tasks [12], [13] and in natural\nlanguage processing to generate language models like BERT\n[14] and GPT-3 [15], models are pre-trained with tasks of pre-\ndicting of next word in a sentence. We used TCN as our choice\nof model, while there are several choices for modeling contin-\nuous time-series data TCNs are a type of convolution network\nthat convolve over a time-domain by combining properties of\nConvolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN). They overcome the problem of exploding\nand vanishing gradients by using causal dilated convolution\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 18:50:53 UTC from IEEE Xplore.  Restrictions apply. \n\nBaseline-RecordingDot-trackingProgressive-Muscle relaxationCalm-MusicExciting-MusicReco-verySpeech-PrepSpeech\nFig. 1: Plot showing the duration and sequence of tasks that are in the study. Tasks that induce stress are marked in red, while\nthe blue ones represent non-stressful tasks. We removed breaks in-between tasks for the sake of visualization. We also show\nhow heart rate varies over the study for a sample individual subject.\nUnlabeled \nBiopac-PPG Peak Detection\nEarbud-PPG\nHeart Rate Variability (HRV) \nFeature\nFig. 2: Transfer learning framework to predict HRV features\ninstead of standard convolutions and using a receptive field\nthat is exponential in network depth.\nB. Downstream HRV parameters estimation\nThe pre-trained TCN model from the previous step is fine-\ntuned']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,,,,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,Diagnostic,,,,"[9, 2, 16]","[' to calculate the true positive rate (same as\nsensitivity) and the false-positive rate (1 – speciﬁcity). ROC\ncurve was the plot of the true positive rate versus the false-\npositive rate.\nClinical validation study\nWe tested our mobile app against dermatologists’ diagnosis in\nthree different clinical settings, namely, at a tertiary care centre\nData \ncollection\nAlgorithm \nfinetuning & \nvalidation\nApp \ndevelopment\nClinical \nvalidation of \nApp\nPublic database of annotated images for \n40 skin diseases + normal skin + nonspecific\nN = 9,203\nPrivate data classified by board certified \ndermatologists\nN = 8,205\nCNN training images N =12,350\nCNN testing images N =3,068\n(1990 nonspecific images not included)\nComparison of model diagnosis against \npredetermined classification\n15,418 images used for training of model to \ngenerate smartphone app\nApp tested by physicians and compared with \ndermatologists’ diagnosis. N = 5,014 \nDetermination of app’s overall sensitivity and \ndisease-specific sensitivity, specificity, PPV, \nNPV and AUC values\nApp testing on rural primary-care patients\nN = 932\nApp testing on urban, private clinic patients\nN = 383\nApp testing on urban tertiary hospital patients\nN = 3,699\nFigure 1 This ﬂowchart depicts the different steps in data collection, algorithm generation, algorithm testing, app generation and app\nvalidation in clinical studies.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 539\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosaceaMelasma\nAcne\nVitiligo/Leucoderma\nTinea pedis\nAnogenital wartsKeratoacanthoma\nMolluscum contagiosumHidradenitis suppurativa\nHerpes zosterTinea capitis\nImpetigo and Pyodermas\nAlopecia\nMilia\nBasal cell carcinoma\nMelanoma\nMelanocytic nevi/Mole\nIchthyosis\nPityriasis r\nosea\nActin\nic keratosis\nTinea cruris  corporis or faciei\nKeloid/Hype\nrtrophic scar\nSeborrheic\n keratosis\nFixed d\nrug eruption\nLichen sclerosus et atrophicus\nCh\nicken pox\nTinea ma\nnuum\nLichen planus\nUrticaria\nPityriasis ve\nrsicolorPsoriasis\nPe\nmphigus\nCandid\niasis\nBowens disease\nViral warts\nSquamous cell c\narcino\nma\nBullous pe\nmphigoidEcze\nma\nDis\ncoid lupus erythematosus\nPercentage\nSensitivity\nSpecificity\nPPV\nNPV\nSensitivity and specificity of five fold validation data(a)\n(b)\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosacea\nKeloid/Hypertrophic scar\nFixed drug eruption\nHerpes zoster\nVitiligo/Leucoderma\nIchthyosis\nHidradenitis suppurativa\nMelasma\nKeratoacanthoma\n', ' validation studies of AI-\nbased algorithms related to dermatology in clinical settings.\nMaterial and methods\nThe study aimed to develop a convolutional neural network\n(CNN)-based algorithm trained with clinical images of 40 differ-\nent skin diseases. A user-friendly, smartphone app was also gen-\nerated, and a clinical validation study on 5014 patients was done\nby physicians in urban, rural primary care and tertiary care set-\ntings. The app’s analysis of a single image of the lesion was com-\npared to the consensus diagnosis made by two board-certiﬁed\ndermatologists. Only patients, for whom the treating board-cer-\ntiﬁed dermatologist was conﬁdent of the diagnosis and the diag-\nnosis was cross-veriﬁed by another board-certiﬁed\ndermatologist, were included. If there was no consensus then the\npatient was excluded from the study.\nGeneration of the CNN-based algorithm\nThe CNN architecture used was a modiﬁed version of Densenet-\n161 with optimized augmentation and inference pipelines.\n16\nThese optimizations were derived through rigorous experiments\nand were attuned to clinical skin images. All training and testing\nimages were annotated and segmented so a bounding box sur-\nrounded lesion of interest and irrelevant features such as rulers\nand surgical markings were discarded. The 17 718 raw images\nwere sourced from public databases (http://www.hellenicderma\ntlas.com/en and http://www.danderm.dk/atlas), after obtaining\npermission from them, as well as images from dermatologists in\nIndia. Of these, 310 images were discarded during the\npreprocessing stage due to poor resolution or multiple lesions.\nOut of the remaining 17 408 images, 1990 images belonged to\nthe non-speciﬁc category and 15 418 images were within the 40\nselected disease categories (Table 1). These images were split\ninto ﬁve equal parts (folds) and ﬁve iterations of training and\nvalidation were performed in a manner so that within each itera-\ntion, a different fold of the data was held-out for validation while\nthe remaining fourfolds were used for learning. The training\nimages were 12 350 and testing images were 3068. Since the\ndataset was imbalanced, a custom modiﬁed version of the\nweighted focal loss function was used to train the network. The\nweights for each class were calculated using the inverse sample\nsize method. Our test images were either in-distribution images\ni.e. images within 40 training classes or out-of-distribution sets\nthat we named as non-speciﬁc set.\nWe initially used Inception-v4, which was one of the most\npopular CNN models at the time. For 10 diseases, a sensitivity of\naround 85% was achieved, but for the 20-disease class model,\nthis fell to around 65%. We experimented with many parameters\nlike learning rate and choice of optimizer etc. with no signiﬁcant\nimprovements. It was felt that as the architectures became dee-\nper, there could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants', '/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2563.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",Consecutive patients with confident dermatologist diagnosis.,,,,"[11, 16, 18]","[' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15', '/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14', ' the reuse of the same\npublic datasets from white-skinned individuals for training, eval-\nuating or comparing models in nearly all studies.\n30 Therefore,\nwe used several thousand new patient images for training and all\ndistinct images for validation. To enable our app to make deci-\nsions on clinically relevant features, we also trained our model\non images of normal skin from individuals as a control.\nOne limitation of this study was the absence of integration of\nmedical history with image analysis by the app. The dermatolo-\ngists had the advantage of having access to relevant additional\ninformation as they had a face-to-face interaction with the\npatient, making it easier for them to arrive at a diagnosis. A few\nimportant relevant points in the analysis, such as the duration of\nthe lesion, sites(s) involved, symptoms, and evolution of the\nlesions can improve the accuracy. The image analysis does not\ntake into consideration the distribution of skin lesions, which is\nas important a clue for the diagnosis of certain diseases as is the\nmorphology. For example, pityriasis rosea and herpes zoster\nhave a patterned distribution. Future versions of such apps need\nto reﬁne the image-based diagnosis with automated analyses of\npatient metadata.\nIn conclusion, this is the ﬁrst study where an app designed to\ndetect 40 dermatological diseases was tested in actual clinical set-\ntings on patients with skin of colour. Our data suggest that the\nAI-driven app has high diagnostic accuracy compared to a der-\nmatologist, and is, therefore, a useful, point-of-care, clinical\ndecision support tool for dermatological diagnosis for a range of\ncommon skin conditions.\nReferences\n1 Hollestein LM, Nijsten T. An insight into the global burden of skin dis-\neases. J Invest Dermatol2014; 134: 1499–1501.\n2 Karimkhani C, Dellavalle RP, Coffeng LEet al. Global skin disease mor-\nbidity and mortality: an update from the global burden of disease study\n2013. JAMA Dermatol2017; 153: 406–412.\n3 Skin diseases to grow in India by 2015: Report, May 2014. (https://www.b\niospectrumindia.com/news/73/8437/skin-diseases-to-grow-in-india-by-\n2015-report.html)Accessed on April 10, 2020.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\n544 Pangti et al.\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Yoo JY, Rigel DS. Trends in dermatology: geographic density of US der-\nmatologists. Arch Dermatol2010; 146: 779.\n5 Ehrlich A, Kostecki J, Olkaba H. Trends in dermatology practices and the\nimplications for the workforce.J Am Acad Dermatol2017; 77: 746–752.\n6 Lim HW, Collins SAB, Resneck JS, Jret al. The burden of skin disease in\nthe United States.J Am Acad Dermatol2017; 76: 958–972.e2.\n7 Patro BK, Tripathy JP, De D, Sinha S, Singh A, Kanwar AJ. Diagnostic\nagreement between a primary care physician and a teledermatologist for\ncommon dermatological conditions in North India.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2566.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Consecutively.,,,,"[11, 16, 9]","[' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15', '/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14', ' to calculate the true positive rate (same as\nsensitivity) and the false-positive rate (1 – speciﬁcity). ROC\ncurve was the plot of the true positive rate versus the false-\npositive rate.\nClinical validation study\nWe tested our mobile app against dermatologists’ diagnosis in\nthree different clinical settings, namely, at a tertiary care centre\nData \ncollection\nAlgorithm \nfinetuning & \nvalidation\nApp \ndevelopment\nClinical \nvalidation of \nApp\nPublic database of annotated images for \n40 skin diseases + normal skin + nonspecific\nN = 9,203\nPrivate data classified by board certified \ndermatologists\nN = 8,205\nCNN training images N =12,350\nCNN testing images N =3,068\n(1990 nonspecific images not included)\nComparison of model diagnosis against \npredetermined classification\n15,418 images used for training of model to \ngenerate smartphone app\nApp tested by physicians and compared with \ndermatologists’ diagnosis. N = 5,014 \nDetermination of app’s overall sensitivity and \ndisease-specific sensitivity, specificity, PPV, \nNPV and AUC values\nApp testing on rural primary-care patients\nN = 932\nApp testing on urban, private clinic patients\nN = 383\nApp testing on urban tertiary hospital patients\nN = 3,699\nFigure 1 This ﬂowchart depicts the different steps in data collection, algorithm generation, algorithm testing, app generation and app\nvalidation in clinical studies.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 539\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosaceaMelasma\nAcne\nVitiligo/Leucoderma\nTinea pedis\nAnogenital wartsKeratoacanthoma\nMolluscum contagiosumHidradenitis suppurativa\nHerpes zosterTinea capitis\nImpetigo and Pyodermas\nAlopecia\nMilia\nBasal cell carcinoma\nMelanoma\nMelanocytic nevi/Mole\nIchthyosis\nPityriasis r\nosea\nActin\nic keratosis\nTinea cruris  corporis or faciei\nKeloid/Hype\nrtrophic scar\nSeborrheic\n keratosis\nFixed d\nrug eruption\nLichen sclerosus et atrophicus\nCh\nicken pox\nTinea ma\nnuum\nLichen planus\nUrticaria\nPityriasis ve\nrsicolorPsoriasis\nPe\nmphigus\nCandid\niasis\nBowens disease\nViral warts\nSquamous cell c\narcino\nma\nBullous pe\nmphigoidEcze\nma\nDis\ncoid lupus erythematosus\nPercentage\nSensitivity\nSpecificity\nPPV\nNPV\nSensitivity and specificity of five fold validation data(a)\n(b)\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosacea\nKeloid/Hypertrophic scar\nFixed drug eruption\nHerpes zoster\nVitiligo/Leucoderma\nIchthyosis\nHidradenitis suppurativa\nMelasma\nKeratoacanthoma\n']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2549.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q4,How was the dataset described in this study before predictive modeling was performed?,Unknown from this paper.,,,,"[14, 16, 11]","[' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', '/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14', ' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2540.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?",Split into five folds for cross-validation.,,,,"[14, 2, 16]","[' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' validation studies of AI-\nbased algorithms related to dermatology in clinical settings.\nMaterial and methods\nThe study aimed to develop a convolutional neural network\n(CNN)-based algorithm trained with clinical images of 40 differ-\nent skin diseases. A user-friendly, smartphone app was also gen-\nerated, and a clinical validation study on 5014 patients was done\nby physicians in urban, rural primary care and tertiary care set-\ntings. The app’s analysis of a single image of the lesion was com-\npared to the consensus diagnosis made by two board-certiﬁed\ndermatologists. Only patients, for whom the treating board-cer-\ntiﬁed dermatologist was conﬁdent of the diagnosis and the diag-\nnosis was cross-veriﬁed by another board-certiﬁed\ndermatologist, were included. If there was no consensus then the\npatient was excluded from the study.\nGeneration of the CNN-based algorithm\nThe CNN architecture used was a modiﬁed version of Densenet-\n161 with optimized augmentation and inference pipelines.\n16\nThese optimizations were derived through rigorous experiments\nand were attuned to clinical skin images. All training and testing\nimages were annotated and segmented so a bounding box sur-\nrounded lesion of interest and irrelevant features such as rulers\nand surgical markings were discarded. The 17 718 raw images\nwere sourced from public databases (http://www.hellenicderma\ntlas.com/en and http://www.danderm.dk/atlas), after obtaining\npermission from them, as well as images from dermatologists in\nIndia. Of these, 310 images were discarded during the\npreprocessing stage due to poor resolution or multiple lesions.\nOut of the remaining 17 408 images, 1990 images belonged to\nthe non-speciﬁc category and 15 418 images were within the 40\nselected disease categories (Table 1). These images were split\ninto ﬁve equal parts (folds) and ﬁve iterations of training and\nvalidation were performed in a manner so that within each itera-\ntion, a different fold of the data was held-out for validation while\nthe remaining fourfolds were used for learning. The training\nimages were 12 350 and testing images were 3068. Since the\ndataset was imbalanced, a custom modiﬁed version of the\nweighted focal loss function was used to train the network. The\nweights for each class were calculated using the inverse sample\nsize method. Our test images were either in-distribution images\ni.e. images within 40 training classes or out-of-distribution sets\nthat we named as non-speciﬁc set.\nWe initially used Inception-v4, which was one of the most\npopular CNN models at the time. For 10 diseases, a sensitivity of\naround 85% was achieved, but for the 20-disease class model,\nthis fell to around 65%. We experimented with many parameters\nlike learning rate and choice of optimizer etc. with no signiﬁcant\nimprovements. It was felt that as the architectures became dee-\nper, there could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants', '/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2561.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q6,What preprocessing techniques on the included variables/features were applied in this study?,"Image crop, color balance, flips, rotations, Gaussian normalization.",,,,"[14, 15, 3]","[' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation, mainly for skin cancers. However, only\na few researchers have used clinical/macroscopic images for mul-\nti-class skin disease identiﬁcation, representative of a real-world\nclinical scenario. In a study by Estevaet al., CNN performedat\npar with dermatologists for classiﬁcation of 9 skin disease cate-\ngories, skin cancers and their precursors, using clinical images\nwith an accuracy of 55.4%.\n22 Mendes et al. generated a CNN\nclassiﬁcation model for 12 skin cancers and related benign and\npremalignant skin conditions and achieved a total accuracy of\n78%.\n23 Han et al. reported a mean AUC of 0.91 and top-1 accu-\nracy between~55–57% on the same set of 12 diseases and, inter-\nestingly, found a substantial difference in speciﬁcities of skin\ncancer between an Asian skin dataset and Caucasian skin data-\nset.\n24 More recently, Liuet al. developed a deep learning system\n76.55\n90.51\n80.33\n94.33\n74.9\n89.22\n75.07\n89.62\n0\n25\n50\n75\n100\nUrban private\npractice\nRural Tertiary\nCombined\nClinical setting\nAccuracy\nTop_1_accuracy\nTop_3_accuracy\n(a)\n(b)\nFigure 3 (a) Overall Top-1 and Top-3 accuracies from clinical vali-\ndation of app in a private urban clinicN = 383), rural hospital\n(N = 932), tertiary hospital (N = 3699) and combined data\n(N = 5014). (b) Disease-speciﬁc Top-1 sensitivities, Top-3 sensitiv-\nities and Top-1 speciﬁcities. The length of the dotted line depicts\nan increase in sensitivity betweenﬁrst and third predictions of the\napp. The size of the icons depicts the number of patients for that\ndisease class.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\n542 Pangti et al.\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100.', ' could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants such as Densenet-\n169, Densenet-121, Densenet-161 and densenet-201, but selected\nDensenet-161 as it achieved the best balance of sensitivity versus\nspeciﬁcity for the 40 disease classes. In future, as the disease list\nexpands and as newer CNN architectures evolve, this search for\nan optimal model will need to be revisited.\nOur optimizations included three stages. In the preprocessing\nstage, via iterative experiments, we identiﬁed a set of speciﬁc\nimage processing and augmentation algorithms that improve the\noverall model performance. We determined that for skin lesions\nrandom image crop selection within a targeted area range, slight\ncolour balance adjustment, image ﬂips (vertical and horizontal)\nand 90° rotations lead to an improvement in results. Few algo-\nrithms like heavy colour balance adjustment, random rotations\nlead to no improvements or even slight degradation in perfor-\nmance. We also did several experiments with normalization\nalgorithms and selected the best performing Gaussian normal-\nization algorithm on our dataset. In the postprocessing stage for\nprediction, we used several copies of the same model to generate\nthe ﬁnal prediction. The raw outputs from each of the model\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 537\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\ncopies were then combined to generate ﬁnal predictions. We\nadditionally made custom changes in the loss function. Focal\nloss is used for multi-class classiﬁcation whereas log loss for bin-\nary classiﬁcation. We implemented a new loss function that led\nto better results from either of them individually. Our custom\nloss function is a hybrid implementation of focal loss and log\nloss.\n18,19 We also accounted for class imbalances. The mathemat-\nical expression of the loss function used by us is described\nbelow.\nFLðp\ntÞ¼/C0 αt ð1 /C0 ptÞγlogðptÞ\nLLðpÞ¼/C0 ð ylogðpÞþð 1 /C0 yÞlogð1 /C0 pÞÞ\nFCLðptÞ¼/C0 αtðytð1 /C0 ptÞγlogðptÞþð 1 /C0 ytÞpγ\nt logð1 /C0 ptÞÞ\nFL = Focal loss; LL = Log Loss; FCL = Our loss function\nimplementation.\nTable 1 shows number of images and diagnostic parameters fromﬁvefold algorithm']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2544.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q7,How is missing data handled in this study?,Unknown from this paper,,,,"[4, 16, 11]","[' imbalances. The mathemat-\nical expression of the loss function used by us is described\nbelow.\nFLðp\ntÞ¼/C0 αt ð1 /C0 ptÞγlogðptÞ\nLLðpÞ¼/C0 ð ylogðpÞþð 1 /C0 yÞlogð1 /C0 pÞÞ\nFCLðptÞ¼/C0 αtðytð1 /C0 ptÞγlogðptÞþð 1 /C0 ytÞpγ\nt logð1 /C0 ptÞÞ\nFL = Focal loss; LL = Log Loss; FCL = Our loss function\nimplementation.\nTable 1 shows number of images and diagnostic parameters fromﬁvefold algorithmic validation of 41 skin conditions\nS.S Disease class N, Private N, Public AUC-model Sensitivity Speci ﬁcity PPV NPV\n1 Acne 337 240 0.98 86.23 /C6 3.26 99.56 /C6 0.13 86.32 /C6 4.22 99.46 /C6 0.19\n2 Actinic keratosis 249 372 0.97 76.49 /C6 3.25 99.04 /C6 0.22 82.83 /C6 3.72 98.85 /C6 0.19\n3 Alopecia 109 152 0.97 78.38 /C6 6.25 99.74 /C6 0.09 86.60 /C6 5.39 99.65 /C6 0.08\n4 Anogenital warts 86 96 0.95 80.22 /C6 1.96 99.92 /C6 0.04 85.69 /C6 6.51 99.73 /C6 0.10\n5 Basal cell carcinoma 365 506 0.97 77.77 /C6 3.58 99.10 /C6 0.13 83.89 /C6 1.09 98.50 /C6 0.27\n6 Bowen ’s disease 130 179 0.92 61.31 /C6 10.78 99.60 /C6 0.14 79.31 /C6 8.58 99.06 /C6 0.11\n7 Bullous pemphigoid 54 92 0.91 58.22 /C6 13.60 99.90 /C6 0.05 82.86 /C6 8.45 99.60 /C6 0.14\n8 Candidiasis 118 187 0.94 65.24 /C6 6.39 99.54 /C6 0.13 77.47 /C6 4.65 99.35 /C6 0.11\n9 Chicken pox 77 108 0.94 72.43 /C6 6.99 99.77 /C6 0.07 80.94 /C6 9.67 99.59 /C6 0.12\n10 Discoid lupus erythematosus 85 134 0.89 49.78 /C6 6.70 99.83 /C6 0.07 73.06 /C6 6.84 99.36 /C6 0.07\n11 Eczema 560 336 0.92 57.52 /C6 3.37 99.00 /', '/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14', ' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,,,,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[19, 21, 20]","[' Trends in dermatology: geographic density of US der-\nmatologists. Arch Dermatol2010; 146: 779.\n5 Ehrlich A, Kostecki J, Olkaba H. Trends in dermatology practices and the\nimplications for the workforce.J Am Acad Dermatol2017; 77: 746–752.\n6 Lim HW, Collins SAB, Resneck JS, Jret al. The burden of skin disease in\nthe United States.J Am Acad Dermatol2017; 76: 958–972.e2.\n7 Patro BK, Tripathy JP, De D, Sinha S, Singh A, Kanwar AJ. Diagnostic\nagreement between a primary care physician and a teledermatologist for\ncommon dermatological conditions in North India.Indian Dermatol\nOnline J2015; 6:2 1–26.\n8 Bahelah SO, Bahelah R, Bahelah M, Albatineh AN. Primary care physi-\ncians’ knowledge and self-perception of competency in dermatology: An\nevaluation study from Yemen.Cogent Med2015; 2: 1119948.\n9 Ramsay DL, Fox AB. The ability of primary care physicians to recognize\nthe common dermatoses.Arch Dermatol1981; 117: 620–622.\n10 Tran H, Chen K, Lim AC, Jabbour J, Shumack S. Assessing diagnostic\nskill in dermatology: a comparison between general practitioners and der-\nmatologists. Australas J Dermatol2005; 46: 230–234.\n11 Pickett K, Loveman E, Kalita N, Frampton GK, Jones J. Educational inter-\nventions to improve quality of life in people with chronic inﬂammatory\nskin diseases: systematic reviews of clinical effectiveness and cost-effec-\ntiveness. Health Technol Assess2015; 19: 1-176,v-vi.\n12 Gordon WJ, Landman A, Zhang H, Bates DW. Beyond validation: getting\nhealth apps into clinical practice.NPJ Digit Med2020; 3: 14.\n13 Newzoo’s Global Mobile Market Report: Insights into the World’s 3.2\nBillion Smartphone Users, the Devices They use & the Mobile Games\nThey Play. (https://newzoo.com/insights/articles/newzoos-global-mobile-\nmarket-report-insights-into-the-worlds-3-2-billion-smartphone-users-\nthe-devices-they-use-the-mobile-games-they-play/)(Accessed on April 10,\n2020).\n14 Sanders SF, Terwiesch M, Gordon WJ, Stern AD. How artiﬁcial intelli-\ngence is changing health care delivery.N Engl J Med Catalyst2019.\n(https://catalyst.nejm.org/health-care-ai-systems-changing-delivery/)\n(Accessed: April 10, 2020).\n15 Chuchu N, Takwoingi Y, Dinnes Jet al. Cochrane Skin Cancer Diagnostic\nTest Accuracy Group. Smartphone applications for triaging adults with\nskin lesions that are suspicious for melanoma.Cochrane Database Syst\nRev 2018; 12: CD013192.\n16 Huang G, Liu Z, MaatenLvd WKQ.Densely connected convolutional net-\nworks 2017 IEEE conference on computer vision and pattern recognition\n(CVPR), Honolulu, HI, 2017;2261–9. (http://ieeexplore.ieee.org/stamp/\nstamp.jsp?tp=&arnumber=8099726&isnumber=8099483) (Accessed on\nApril 10, 2020).\n17 Huang G, Liu Z, van der Maaten L, Weinberger KQ.Densely connected\nconvolutional networks. August 201', '917.\n28 Al Hasan M, Fitzgerald SM, Saoudian M, Krishnaswamy G. Dermatology\nfor the practicing allergist: Tinea pedis and its complications.Clin Mol\nAllergy 2004; 2:5 .\n29 Verma SB, Vasani R. Male genital dermatophytosis– clinical features and\nthe effects of the misuse of topical steroids and steroid combinations– an\nalarming problem in India.Mycoses 2016; 59: 606–614.\n30 Bissoto A, Fornacialli M, Valle E, Avila S.(De)Constructing bias on skin\nlesions datasets. April 2019 (arXiv:1904.08818) (Accessed on April 2020).\nSupporting information\nAdditional Supporting Information may be found in the online\nversion of this article:\nSupplementary Material\nSupplementary Figure 1. ROC curve of Machine and clinical val-\nidation studies of individual disease classes.\nSupplementary Methods: Formulae of the metrics used in the\nstudy.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 545\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n', ' triaging adults with\nskin lesions that are suspicious for melanoma.Cochrane Database Syst\nRev 2018; 12: CD013192.\n16 Huang G, Liu Z, MaatenLvd WKQ.Densely connected convolutional net-\nworks 2017 IEEE conference on computer vision and pattern recognition\n(CVPR), Honolulu, HI, 2017;2261–9. (http://ieeexplore.ieee.org/stamp/\nstamp.jsp?tp=&arnumber=8099726&isnumber=8099483) (Accessed on\nApril 10, 2020).\n17 Huang G, Liu Z, van der Maaten L, Weinberger KQ.Densely connected\nconvolutional networks. August 2016(arXiv:1608.06993[cs.CV]).\n18 Lin T-Y, Goyal P, Girshick R, He K, Doll´ar P.Focal Loss for Dense Object\nDetection. August 2017 (arXiv:1708.02002 [cs.CV]).\n19 Log loss. February 2017 (http://wiki.fast.ai/index.php/Log_Loss).\nAccessed on: May 13, 2020.\n20 Pedregosa F, Varoquaux G, Gramfort Aet al. Scikit-learn: machine learn-\ning in Python.\nJ Mach Learn Res2011; 12: 2825–2830.\n21 Topol R. Deep Medicine: how artiﬁcial intelligence can make healthcare\nhuman again Ch, 5th edn, Basic Books, New York, 2019.\n22 Esteva A, Kuprel B, Novoa RAet al. Dermatologist-level classiﬁcation of\nskin cancer with deep neural networks.Nature 2017; 542: 115–118.\n23 Mendes DB, da Silva NC.Skin lesions classiﬁcation using convolutional\nneural networks in clinical images, December 2018(arXiv:1812.02316\n[cs.CV])(Last accessed: April 10, 2020).\n24 Han SS, Kim MS, Lim W, Park GH, Park I, Chang SE. Classiﬁcation of\nthe clinical images for benign and malignant cutaneous tumors using a\ndeep learning algorithm.J Invest Dermatol2018; 138: 1529–1538.\n25 Liu Y, Jain A, Eng Cet al.A deep learning system for differential diagnosis\nof skin diseases, September 2019(arXiv:1909.05382) (Accessed on April\n2020).\n26 Gorouhi F, Davari P, Fazel N. Cutaneous and mucosal lichen planus: a\ncomprehensive review of clinical subtypes, risk factors, diagnosis, and\nprognosis. Scien World J2014; 2014: 742826.\n27 Siegfried EC, Hebert AA. Diagnosis of atopic dermatitis: mimics, over-\nlaps, and complications.J Clin Med2015; 4: 884–917.\n28 Al Hasan M, Fitzgerald SM, Saoudian M, Krishnaswamy G. Dermatology\nfor the practicing allergist: Tinea pedis and its complications.Clin Mol\nAllergy 2004; 2:5 .\n29 Verma SB, Vasani R. Male genital dermatophytosis– clinical features and\nthe effects of the misuse of topical steroids and steroid combinations– an\nalarming problem in India.Mycoses 2016; 59: 606–614.\n30 Bissoto A, Fornacialli M, Valle E, Avila S.(De)Constructing bias on skin\nlesions datasets. April 2019 (arXiv:1904.08818) (Accessed on April 202']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2065.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q9,Which prediction models were used in this study?,"CNN-based algorithm, DenseNet-161",,,,"[14, 2, 0]","[' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' validation studies of AI-\nbased algorithms related to dermatology in clinical settings.\nMaterial and methods\nThe study aimed to develop a convolutional neural network\n(CNN)-based algorithm trained with clinical images of 40 differ-\nent skin diseases. A user-friendly, smartphone app was also gen-\nerated, and a clinical validation study on 5014 patients was done\nby physicians in urban, rural primary care and tertiary care set-\ntings. The app’s analysis of a single image of the lesion was com-\npared to the consensus diagnosis made by two board-certiﬁed\ndermatologists. Only patients, for whom the treating board-cer-\ntiﬁed dermatologist was conﬁdent of the diagnosis and the diag-\nnosis was cross-veriﬁed by another board-certiﬁed\ndermatologist, were included. If there was no consensus then the\npatient was excluded from the study.\nGeneration of the CNN-based algorithm\nThe CNN architecture used was a modiﬁed version of Densenet-\n161 with optimized augmentation and inference pipelines.\n16\nThese optimizations were derived through rigorous experiments\nand were attuned to clinical skin images. All training and testing\nimages were annotated and segmented so a bounding box sur-\nrounded lesion of interest and irrelevant features such as rulers\nand surgical markings were discarded. The 17 718 raw images\nwere sourced from public databases (http://www.hellenicderma\ntlas.com/en and http://www.danderm.dk/atlas), after obtaining\npermission from them, as well as images from dermatologists in\nIndia. Of these, 310 images were discarded during the\npreprocessing stage due to poor resolution or multiple lesions.\nOut of the remaining 17 408 images, 1990 images belonged to\nthe non-speciﬁc category and 15 418 images were within the 40\nselected disease categories (Table 1). These images were split\ninto ﬁve equal parts (folds) and ﬁve iterations of training and\nvalidation were performed in a manner so that within each itera-\ntion, a different fold of the data was held-out for validation while\nthe remaining fourfolds were used for learning. The training\nimages were 12 350 and testing images were 3068. Since the\ndataset was imbalanced, a custom modiﬁed version of the\nweighted focal loss function was used to train the network. The\nweights for each class were calculated using the inverse sample\nsize method. Our test images were either in-distribution images\ni.e. images within 40 training classes or out-of-distribution sets\nthat we named as non-speciﬁc set.\nWe initially used Inception-v4, which was one of the most\npopular CNN models at the time. For 10 diseases, a sensitivity of\naround 85% was achieved, but for the 20-disease class model,\nthis fell to around 65%. We experimented with many parameters\nlike learning rate and choice of optimizer etc. with no signiﬁcant\nimprovements. It was felt that as the architectures became dee-\nper, there could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants', 'ORIGINAL ARTICLE\nA machine learning-based, decision support, mobile phone\napplication for diagnosis of common dermatological\ndiseases\nR. Pangti,1 J. Mathur,2 V. Chouhan,2 S. Kumar,2 L. Rajput,1 S. Shah,1 A. Gupta,3 A. Dixit,1\nD. Dholakia,4,5 S. Gupta,6 S. Gupta,1 M. George,7 V.K. Sharma,1 S. Gupta1,*\n1Department of Dermatology and Venereology, All India Institute of Medical Science, New Delhi, India\n2Nurithm Labs Private Limited, Noida, India\n3Skin Aid Clinic, Cross Point Mall, Gurugram, India\n4Genomics and Molecular Medicine Unit, Academy of Scientiﬁc and Innovative Research, New Delhi, India\n5Academy of Scientiﬁc and Innovative Research, Ghaziabad, Uttar Pradesh, India\n6Maharishi Markandeshwar Institute of Medical Sciences and Research, Mullana, Ambala, India\n7Sahrudya Hospital, Alappuzha, India\n*Correspondence: S. Gupta. E-mail: someshgupta@hotmail.com\nAbstract\nBackground The integration of machine learning algorithms in decision support tools for physicians is gaining popular-\nity. These tools can tackle the disparities in healthcare access as the technology can be implemented on smartphones.\nWe present theﬁrst, large-scale study on patients with skin of colour, in which the feasibility of a novel mobile health\napplication (mHealth app) was investigated in actual clinical workﬂows.\nObjective To develop a mHealth app to diagnose 40 common skin diseases and test it in clinical settings.\nMethods A convolutional neural network-based algorithm was trained with clinical images of 40 skin diseases. A\nsmartphone app was generated and validated on 5014 patients, attending rural and urban outpatient dermatology\ndepartments in India. The results of this mHealth app were compared against the dermatologists’ diagnoses.\nResults The machine–learning model, in an in silico validation study, demonstrated an overall top-1 accuracy of\n76.93 /C6 0.88% and mean area-under-curve of 0.95/C6 0.02 on a set of clinical images. In the clinical study, on patients\nwith skin of colour, the app achieved an overall top-1 accuracy of 75.07% (95% CI= 73.75–76.36), top-3 accuracy of\n89.62% (95% CI= 88.67–90.52) and mean area-under-curve of 0.90/C6 0.07.\nConclusion This study underscores the utility of artiﬁcial intelligence-driven smartphone applications as a point-of-\ncare, clinical decision support tool for dermatological diagnosis for a wide spectrum of skin diseases in patients of the\nskin of colour.\nReceived: 11 April 2020; revised: 22 June 2020; Accepted: 5 August 2020\nConﬂict of interests\nJyoti Mathur and Sharad Kumar are direct beneﬁciaries of any proﬁts acquired by the smartphone application developed\nat Nurithm Labs Private Limited and Vikas Chouhan is a salaried employee at Nurithm Labs Private Limited.\nFunding sources\nNone.\nIntroduction\nSkin diseases are the fourth leading cause of non-fatal disease\nburden across 188 countries in both low and high-income coun-\ntries.\n1,2 There is less than one dermatologist per 100 000 people\nin India and less than three dermatologists per 100 000 of the\npopulation in the United States and the ratios are expected to\ndecline further.\n3,4 In a survey by the American Academy of Der-\nmatology, 33%']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2548.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,"DenseNet-161 selected; preprocessing, augmentation, custom loss function.",,,,"[3, 14, 18]","[' could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants such as Densenet-\n169, Densenet-121, Densenet-161 and densenet-201, but selected\nDensenet-161 as it achieved the best balance of sensitivity versus\nspeciﬁcity for the 40 disease classes. In future, as the disease list\nexpands and as newer CNN architectures evolve, this search for\nan optimal model will need to be revisited.\nOur optimizations included three stages. In the preprocessing\nstage, via iterative experiments, we identiﬁed a set of speciﬁc\nimage processing and augmentation algorithms that improve the\noverall model performance. We determined that for skin lesions\nrandom image crop selection within a targeted area range, slight\ncolour balance adjustment, image ﬂips (vertical and horizontal)\nand 90° rotations lead to an improvement in results. Few algo-\nrithms like heavy colour balance adjustment, random rotations\nlead to no improvements or even slight degradation in perfor-\nmance. We also did several experiments with normalization\nalgorithms and selected the best performing Gaussian normal-\nization algorithm on our dataset. In the postprocessing stage for\nprediction, we used several copies of the same model to generate\nthe ﬁnal prediction. The raw outputs from each of the model\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 537\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\ncopies were then combined to generate ﬁnal predictions. We\nadditionally made custom changes in the loss function. Focal\nloss is used for multi-class classiﬁcation whereas log loss for bin-\nary classiﬁcation. We implemented a new loss function that led\nto better results from either of them individually. Our custom\nloss function is a hybrid implementation of focal loss and log\nloss.\n18,19 We also accounted for class imbalances. The mathemat-\nical expression of the loss function used by us is described\nbelow.\nFLðp\ntÞ¼/C0 αt ð1 /C0 ptÞγlogðptÞ\nLLðpÞ¼/C0 ð ylogðpÞþð 1 /C0 yÞlogð1 /C0 pÞÞ\nFCLðptÞ¼/C0 αtðytð1 /C0 ptÞγlogðptÞþð 1 /C0 ytÞpγ\nt logð1 /C0 ptÞÞ\nFL = Focal loss; LL = Log Loss; FCL = Our loss function\nimplementation.\nTable 1 shows number of images and diagnostic parameters fromﬁvefold algorithm', ' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' the reuse of the same\npublic datasets from white-skinned individuals for training, eval-\nuating or comparing models in nearly all studies.\n30 Therefore,\nwe used several thousand new patient images for training and all\ndistinct images for validation. To enable our app to make deci-\nsions on clinically relevant features, we also trained our model\non images of normal skin from individuals as a control.\nOne limitation of this study was the absence of integration of\nmedical history with image analysis by the app. The dermatolo-\ngists had the advantage of having access to relevant additional\ninformation as they had a face-to-face interaction with the\npatient, making it easier for them to arrive at a diagnosis. A few\nimportant relevant points in the analysis, such as the duration of\nthe lesion, sites(s) involved, symptoms, and evolution of the\nlesions can improve the accuracy. The image analysis does not\ntake into consideration the distribution of skin lesions, which is\nas important a clue for the diagnosis of certain diseases as is the\nmorphology. For example, pityriasis rosea and herpes zoster\nhave a patterned distribution. Future versions of such apps need\nto reﬁne the image-based diagnosis with automated analyses of\npatient metadata.\nIn conclusion, this is the ﬁrst study where an app designed to\ndetect 40 dermatological diseases was tested in actual clinical set-\ntings on patients with skin of colour. Our data suggest that the\nAI-driven app has high diagnostic accuracy compared to a der-\nmatologist, and is, therefore, a useful, point-of-care, clinical\ndecision support tool for dermatological diagnosis for a range of\ncommon skin conditions.\nReferences\n1 Hollestein LM, Nijsten T. An insight into the global burden of skin dis-\neases. J Invest Dermatol2014; 134: 1499–1501.\n2 Karimkhani C, Dellavalle RP, Coffeng LEet al. Global skin disease mor-\nbidity and mortality: an update from the global burden of disease study\n2013. JAMA Dermatol2017; 153: 406–412.\n3 Skin diseases to grow in India by 2015: Report, May 2014. (https://www.b\niospectrumindia.com/news/73/8437/skin-diseases-to-grow-in-india-by-\n2015-report.html)Accessed on April 10, 2020.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\n544 Pangti et al.\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Yoo JY, Rigel DS. Trends in dermatology: geographic density of US der-\nmatologists. Arch Dermatol2010; 146: 779.\n5 Ehrlich A, Kostecki J, Olkaba H. Trends in dermatology practices and the\nimplications for the workforce.J Am Acad Dermatol2017; 77: 746–752.\n6 Lim HW, Collins SAB, Resneck JS, Jret al. The burden of skin disease in\nthe United States.J Am Acad Dermatol2017; 76: 958–972.e2.\n7 Patro BK, Tripathy JP, De D, Sinha S, Singh A, Kanwar AJ. Diagnostic\nagreement between a primary care physician and a teledermatologist for\ncommon dermatological conditions in North India.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q11,How was data augmentation or generation used in this study?,Used new patient images and normal skin controls.,,,,"[18, 20, 9]","[' the reuse of the same\npublic datasets from white-skinned individuals for training, eval-\nuating or comparing models in nearly all studies.\n30 Therefore,\nwe used several thousand new patient images for training and all\ndistinct images for validation. To enable our app to make deci-\nsions on clinically relevant features, we also trained our model\non images of normal skin from individuals as a control.\nOne limitation of this study was the absence of integration of\nmedical history with image analysis by the app. The dermatolo-\ngists had the advantage of having access to relevant additional\ninformation as they had a face-to-face interaction with the\npatient, making it easier for them to arrive at a diagnosis. A few\nimportant relevant points in the analysis, such as the duration of\nthe lesion, sites(s) involved, symptoms, and evolution of the\nlesions can improve the accuracy. The image analysis does not\ntake into consideration the distribution of skin lesions, which is\nas important a clue for the diagnosis of certain diseases as is the\nmorphology. For example, pityriasis rosea and herpes zoster\nhave a patterned distribution. Future versions of such apps need\nto reﬁne the image-based diagnosis with automated analyses of\npatient metadata.\nIn conclusion, this is the ﬁrst study where an app designed to\ndetect 40 dermatological diseases was tested in actual clinical set-\ntings on patients with skin of colour. Our data suggest that the\nAI-driven app has high diagnostic accuracy compared to a der-\nmatologist, and is, therefore, a useful, point-of-care, clinical\ndecision support tool for dermatological diagnosis for a range of\ncommon skin conditions.\nReferences\n1 Hollestein LM, Nijsten T. An insight into the global burden of skin dis-\neases. J Invest Dermatol2014; 134: 1499–1501.\n2 Karimkhani C, Dellavalle RP, Coffeng LEet al. Global skin disease mor-\nbidity and mortality: an update from the global burden of disease study\n2013. JAMA Dermatol2017; 153: 406–412.\n3 Skin diseases to grow in India by 2015: Report, May 2014. (https://www.b\niospectrumindia.com/news/73/8437/skin-diseases-to-grow-in-india-by-\n2015-report.html)Accessed on April 10, 2020.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\n544 Pangti et al.\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n4 Yoo JY, Rigel DS. Trends in dermatology: geographic density of US der-\nmatologists. Arch Dermatol2010; 146: 779.\n5 Ehrlich A, Kostecki J, Olkaba H. Trends in dermatology practices and the\nimplications for the workforce.J Am Acad Dermatol2017; 77: 746–752.\n6 Lim HW, Collins SAB, Resneck JS, Jret al. The burden of skin disease in\nthe United States.J Am Acad Dermatol2017; 76: 958–972.e2.\n7 Patro BK, Tripathy JP, De D, Sinha S, Singh A, Kanwar AJ. Diagnostic\nagreement between a primary care physician and a teledermatologist for\ncommon dermatological conditions in North India.', ' triaging adults with\nskin lesions that are suspicious for melanoma.Cochrane Database Syst\nRev 2018; 12: CD013192.\n16 Huang G, Liu Z, MaatenLvd WKQ.Densely connected convolutional net-\nworks 2017 IEEE conference on computer vision and pattern recognition\n(CVPR), Honolulu, HI, 2017;2261–9. (http://ieeexplore.ieee.org/stamp/\nstamp.jsp?tp=&arnumber=8099726&isnumber=8099483) (Accessed on\nApril 10, 2020).\n17 Huang G, Liu Z, van der Maaten L, Weinberger KQ.Densely connected\nconvolutional networks. August 2016(arXiv:1608.06993[cs.CV]).\n18 Lin T-Y, Goyal P, Girshick R, He K, Doll´ar P.Focal Loss for Dense Object\nDetection. August 2017 (arXiv:1708.02002 [cs.CV]).\n19 Log loss. February 2017 (http://wiki.fast.ai/index.php/Log_Loss).\nAccessed on: May 13, 2020.\n20 Pedregosa F, Varoquaux G, Gramfort Aet al. Scikit-learn: machine learn-\ning in Python.\nJ Mach Learn Res2011; 12: 2825–2830.\n21 Topol R. Deep Medicine: how artiﬁcial intelligence can make healthcare\nhuman again Ch, 5th edn, Basic Books, New York, 2019.\n22 Esteva A, Kuprel B, Novoa RAet al. Dermatologist-level classiﬁcation of\nskin cancer with deep neural networks.Nature 2017; 542: 115–118.\n23 Mendes DB, da Silva NC.Skin lesions classiﬁcation using convolutional\nneural networks in clinical images, December 2018(arXiv:1812.02316\n[cs.CV])(Last accessed: April 10, 2020).\n24 Han SS, Kim MS, Lim W, Park GH, Park I, Chang SE. Classiﬁcation of\nthe clinical images for benign and malignant cutaneous tumors using a\ndeep learning algorithm.J Invest Dermatol2018; 138: 1529–1538.\n25 Liu Y, Jain A, Eng Cet al.A deep learning system for differential diagnosis\nof skin diseases, September 2019(arXiv:1909.05382) (Accessed on April\n2020).\n26 Gorouhi F, Davari P, Fazel N. Cutaneous and mucosal lichen planus: a\ncomprehensive review of clinical subtypes, risk factors, diagnosis, and\nprognosis. Scien World J2014; 2014: 742826.\n27 Siegfried EC, Hebert AA. Diagnosis of atopic dermatitis: mimics, over-\nlaps, and complications.J Clin Med2015; 4: 884–917.\n28 Al Hasan M, Fitzgerald SM, Saoudian M, Krishnaswamy G. Dermatology\nfor the practicing allergist: Tinea pedis and its complications.Clin Mol\nAllergy 2004; 2:5 .\n29 Verma SB, Vasani R. Male genital dermatophytosis– clinical features and\nthe effects of the misuse of topical steroids and steroid combinations– an\nalarming problem in India.Mycoses 2016; 59: 606–614.\n30 Bissoto A, Fornacialli M, Valle E, Avila S.(De)Constructing bias on skin\nlesions datasets. April 2019 (arXiv:1904.08818) (Accessed on April 202', ' to calculate the true positive rate (same as\nsensitivity) and the false-positive rate (1 – speciﬁcity). ROC\ncurve was the plot of the true positive rate versus the false-\npositive rate.\nClinical validation study\nWe tested our mobile app against dermatologists’ diagnosis in\nthree different clinical settings, namely, at a tertiary care centre\nData \ncollection\nAlgorithm \nfinetuning & \nvalidation\nApp \ndevelopment\nClinical \nvalidation of \nApp\nPublic database of annotated images for \n40 skin diseases + normal skin + nonspecific\nN = 9,203\nPrivate data classified by board certified \ndermatologists\nN = 8,205\nCNN training images N =12,350\nCNN testing images N =3,068\n(1990 nonspecific images not included)\nComparison of model diagnosis against \npredetermined classification\n15,418 images used for training of model to \ngenerate smartphone app\nApp tested by physicians and compared with \ndermatologists’ diagnosis. N = 5,014 \nDetermination of app’s overall sensitivity and \ndisease-specific sensitivity, specificity, PPV, \nNPV and AUC values\nApp testing on rural primary-care patients\nN = 932\nApp testing on urban, private clinic patients\nN = 383\nApp testing on urban tertiary hospital patients\nN = 3,699\nFigure 1 This ﬂowchart depicts the different steps in data collection, algorithm generation, algorithm testing, app generation and app\nvalidation in clinical studies.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 539\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosaceaMelasma\nAcne\nVitiligo/Leucoderma\nTinea pedis\nAnogenital wartsKeratoacanthoma\nMolluscum contagiosumHidradenitis suppurativa\nHerpes zosterTinea capitis\nImpetigo and Pyodermas\nAlopecia\nMilia\nBasal cell carcinoma\nMelanoma\nMelanocytic nevi/Mole\nIchthyosis\nPityriasis r\nosea\nActin\nic keratosis\nTinea cruris  corporis or faciei\nKeloid/Hype\nrtrophic scar\nSeborrheic\n keratosis\nFixed d\nrug eruption\nLichen sclerosus et atrophicus\nCh\nicken pox\nTinea ma\nnuum\nLichen planus\nUrticaria\nPityriasis ve\nrsicolorPsoriasis\nPe\nmphigus\nCandid\niasis\nBowens disease\nViral warts\nSquamous cell c\narcino\nma\nBullous pe\nmphigoidEcze\nma\nDis\ncoid lupus erythematosus\nPercentage\nSensitivity\nSpecificity\nPPV\nNPV\nSensitivity and specificity of five fold validation data(a)\n(b)\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosacea\nKeloid/Hypertrophic scar\nFixed drug eruption\nHerpes zoster\nVitiligo/Leucoderma\nIchthyosis\nHidradenitis suppurativa\nMelasma\nKeratoacanthoma\n']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2531.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared to dermatologists' consensus diagnosis.",,,,"[2, 14, 9]","[' validation studies of AI-\nbased algorithms related to dermatology in clinical settings.\nMaterial and methods\nThe study aimed to develop a convolutional neural network\n(CNN)-based algorithm trained with clinical images of 40 differ-\nent skin diseases. A user-friendly, smartphone app was also gen-\nerated, and a clinical validation study on 5014 patients was done\nby physicians in urban, rural primary care and tertiary care set-\ntings. The app’s analysis of a single image of the lesion was com-\npared to the consensus diagnosis made by two board-certiﬁed\ndermatologists. Only patients, for whom the treating board-cer-\ntiﬁed dermatologist was conﬁdent of the diagnosis and the diag-\nnosis was cross-veriﬁed by another board-certiﬁed\ndermatologist, were included. If there was no consensus then the\npatient was excluded from the study.\nGeneration of the CNN-based algorithm\nThe CNN architecture used was a modiﬁed version of Densenet-\n161 with optimized augmentation and inference pipelines.\n16\nThese optimizations were derived through rigorous experiments\nand were attuned to clinical skin images. All training and testing\nimages were annotated and segmented so a bounding box sur-\nrounded lesion of interest and irrelevant features such as rulers\nand surgical markings were discarded. The 17 718 raw images\nwere sourced from public databases (http://www.hellenicderma\ntlas.com/en and http://www.danderm.dk/atlas), after obtaining\npermission from them, as well as images from dermatologists in\nIndia. Of these, 310 images were discarded during the\npreprocessing stage due to poor resolution or multiple lesions.\nOut of the remaining 17 408 images, 1990 images belonged to\nthe non-speciﬁc category and 15 418 images were within the 40\nselected disease categories (Table 1). These images were split\ninto ﬁve equal parts (folds) and ﬁve iterations of training and\nvalidation were performed in a manner so that within each itera-\ntion, a different fold of the data was held-out for validation while\nthe remaining fourfolds were used for learning. The training\nimages were 12 350 and testing images were 3068. Since the\ndataset was imbalanced, a custom modiﬁed version of the\nweighted focal loss function was used to train the network. The\nweights for each class were calculated using the inverse sample\nsize method. Our test images were either in-distribution images\ni.e. images within 40 training classes or out-of-distribution sets\nthat we named as non-speciﬁc set.\nWe initially used Inception-v4, which was one of the most\npopular CNN models at the time. For 10 diseases, a sensitivity of\naround 85% was achieved, but for the 20-disease class model,\nthis fell to around 65%. We experimented with many parameters\nlike learning rate and choice of optimizer etc. with no signiﬁcant\nimprovements. It was felt that as the architectures became dee-\nper, there could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants', ' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' to calculate the true positive rate (same as\nsensitivity) and the false-positive rate (1 – speciﬁcity). ROC\ncurve was the plot of the true positive rate versus the false-\npositive rate.\nClinical validation study\nWe tested our mobile app against dermatologists’ diagnosis in\nthree different clinical settings, namely, at a tertiary care centre\nData \ncollection\nAlgorithm \nfinetuning & \nvalidation\nApp \ndevelopment\nClinical \nvalidation of \nApp\nPublic database of annotated images for \n40 skin diseases + normal skin + nonspecific\nN = 9,203\nPrivate data classified by board certified \ndermatologists\nN = 8,205\nCNN training images N =12,350\nCNN testing images N =3,068\n(1990 nonspecific images not included)\nComparison of model diagnosis against \npredetermined classification\n15,418 images used for training of model to \ngenerate smartphone app\nApp tested by physicians and compared with \ndermatologists’ diagnosis. N = 5,014 \nDetermination of app’s overall sensitivity and \ndisease-specific sensitivity, specificity, PPV, \nNPV and AUC values\nApp testing on rural primary-care patients\nN = 932\nApp testing on urban, private clinic patients\nN = 383\nApp testing on urban tertiary hospital patients\nN = 3,699\nFigure 1 This ﬂowchart depicts the different steps in data collection, algorithm generation, algorithm testing, app generation and app\nvalidation in clinical studies.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 539\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosaceaMelasma\nAcne\nVitiligo/Leucoderma\nTinea pedis\nAnogenital wartsKeratoacanthoma\nMolluscum contagiosumHidradenitis suppurativa\nHerpes zosterTinea capitis\nImpetigo and Pyodermas\nAlopecia\nMilia\nBasal cell carcinoma\nMelanoma\nMelanocytic nevi/Mole\nIchthyosis\nPityriasis r\nosea\nActin\nic keratosis\nTinea cruris  corporis or faciei\nKeloid/Hype\nrtrophic scar\nSeborrheic\n keratosis\nFixed d\nrug eruption\nLichen sclerosus et atrophicus\nCh\nicken pox\nTinea ma\nnuum\nLichen planus\nUrticaria\nPityriasis ve\nrsicolorPsoriasis\nPe\nmphigus\nCandid\niasis\nBowens disease\nViral warts\nSquamous cell c\narcino\nma\nBullous pe\nmphigoidEcze\nma\nDis\ncoid lupus erythematosus\nPercentage\nSensitivity\nSpecificity\nPPV\nNPV\nSensitivity and specificity of five fold validation data(a)\n(b)\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosacea\nKeloid/Hypertrophic scar\nFixed drug eruption\nHerpes zoster\nVitiligo/Leucoderma\nIchthyosis\nHidradenitis suppurativa\nMelasma\nKeratoacanthoma\n']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2552.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q13,Which type of explainability techniques are used?,Unknown from this paper,,,,"[3, 14, 4]","[' could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants such as Densenet-\n169, Densenet-121, Densenet-161 and densenet-201, but selected\nDensenet-161 as it achieved the best balance of sensitivity versus\nspeciﬁcity for the 40 disease classes. In future, as the disease list\nexpands and as newer CNN architectures evolve, this search for\nan optimal model will need to be revisited.\nOur optimizations included three stages. In the preprocessing\nstage, via iterative experiments, we identiﬁed a set of speciﬁc\nimage processing and augmentation algorithms that improve the\noverall model performance. We determined that for skin lesions\nrandom image crop selection within a targeted area range, slight\ncolour balance adjustment, image ﬂips (vertical and horizontal)\nand 90° rotations lead to an improvement in results. Few algo-\nrithms like heavy colour balance adjustment, random rotations\nlead to no improvements or even slight degradation in perfor-\nmance. We also did several experiments with normalization\nalgorithms and selected the best performing Gaussian normal-\nization algorithm on our dataset. In the postprocessing stage for\nprediction, we used several copies of the same model to generate\nthe ﬁnal prediction. The raw outputs from each of the model\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 537\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\ncopies were then combined to generate ﬁnal predictions. We\nadditionally made custom changes in the loss function. Focal\nloss is used for multi-class classiﬁcation whereas log loss for bin-\nary classiﬁcation. We implemented a new loss function that led\nto better results from either of them individually. Our custom\nloss function is a hybrid implementation of focal loss and log\nloss.\n18,19 We also accounted for class imbalances. The mathemat-\nical expression of the loss function used by us is described\nbelow.\nFLðp\ntÞ¼/C0 αt ð1 /C0 ptÞγlogðptÞ\nLLðpÞ¼/C0 ð ylogðpÞþð 1 /C0 yÞlogð1 /C0 pÞÞ\nFCLðptÞ¼/C0 αtðytð1 /C0 ptÞγlogðptÞþð 1 /C0 ytÞpγ\nt logð1 /C0 ptÞÞ\nFL = Focal loss; LL = Log Loss; FCL = Our loss function\nimplementation.\nTable 1 shows number of images and diagnostic parameters fromﬁvefold algorithm', ' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' imbalances. The mathemat-\nical expression of the loss function used by us is described\nbelow.\nFLðp\ntÞ¼/C0 αt ð1 /C0 ptÞγlogðptÞ\nLLðpÞ¼/C0 ð ylogðpÞþð 1 /C0 yÞlogð1 /C0 pÞÞ\nFCLðptÞ¼/C0 αtðytð1 /C0 ptÞγlogðptÞþð 1 /C0 ytÞpγ\nt logð1 /C0 ptÞÞ\nFL = Focal loss; LL = Log Loss; FCL = Our loss function\nimplementation.\nTable 1 shows number of images and diagnostic parameters fromﬁvefold algorithmic validation of 41 skin conditions\nS.S Disease class N, Private N, Public AUC-model Sensitivity Speci ﬁcity PPV NPV\n1 Acne 337 240 0.98 86.23 /C6 3.26 99.56 /C6 0.13 86.32 /C6 4.22 99.46 /C6 0.19\n2 Actinic keratosis 249 372 0.97 76.49 /C6 3.25 99.04 /C6 0.22 82.83 /C6 3.72 98.85 /C6 0.19\n3 Alopecia 109 152 0.97 78.38 /C6 6.25 99.74 /C6 0.09 86.60 /C6 5.39 99.65 /C6 0.08\n4 Anogenital warts 86 96 0.95 80.22 /C6 1.96 99.92 /C6 0.04 85.69 /C6 6.51 99.73 /C6 0.10\n5 Basal cell carcinoma 365 506 0.97 77.77 /C6 3.58 99.10 /C6 0.13 83.89 /C6 1.09 98.50 /C6 0.27\n6 Bowen ’s disease 130 179 0.92 61.31 /C6 10.78 99.60 /C6 0.14 79.31 /C6 8.58 99.06 /C6 0.11\n7 Bullous pemphigoid 54 92 0.91 58.22 /C6 13.60 99.90 /C6 0.05 82.86 /C6 8.45 99.60 /C6 0.14\n8 Candidiasis 118 187 0.94 65.24 /C6 6.39 99.54 /C6 0.13 77.47 /C6 4.65 99.35 /C6 0.11\n9 Chicken pox 77 108 0.94 72.43 /C6 6.99 99.77 /C6 0.07 80.94 /C6 9.67 99.59 /C6 0.12\n10 Discoid lupus erythematosus 85 134 0.89 49.78 /C6 6.70 99.83 /C6 0.07 73.06 /C6 6.84 99.36 /C6 0.07\n11 Eczema 560 336 0.92 57.52 /C6 3.37 99.00 /']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2530.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,"accuracy, AUC, sensitivity, specificity, PPV, NPV",,,,"[14, 9, 2]","[' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' to calculate the true positive rate (same as\nsensitivity) and the false-positive rate (1 – speciﬁcity). ROC\ncurve was the plot of the true positive rate versus the false-\npositive rate.\nClinical validation study\nWe tested our mobile app against dermatologists’ diagnosis in\nthree different clinical settings, namely, at a tertiary care centre\nData \ncollection\nAlgorithm \nfinetuning & \nvalidation\nApp \ndevelopment\nClinical \nvalidation of \nApp\nPublic database of annotated images for \n40 skin diseases + normal skin + nonspecific\nN = 9,203\nPrivate data classified by board certified \ndermatologists\nN = 8,205\nCNN training images N =12,350\nCNN testing images N =3,068\n(1990 nonspecific images not included)\nComparison of model diagnosis against \npredetermined classification\n15,418 images used for training of model to \ngenerate smartphone app\nApp tested by physicians and compared with \ndermatologists’ diagnosis. N = 5,014 \nDetermination of app’s overall sensitivity and \ndisease-specific sensitivity, specificity, PPV, \nNPV and AUC values\nApp testing on rural primary-care patients\nN = 932\nApp testing on urban, private clinic patients\nN = 383\nApp testing on urban tertiary hospital patients\nN = 3,699\nFigure 1 This ﬂowchart depicts the different steps in data collection, algorithm generation, algorithm testing, app generation and app\nvalidation in clinical studies.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 539\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosaceaMelasma\nAcne\nVitiligo/Leucoderma\nTinea pedis\nAnogenital wartsKeratoacanthoma\nMolluscum contagiosumHidradenitis suppurativa\nHerpes zosterTinea capitis\nImpetigo and Pyodermas\nAlopecia\nMilia\nBasal cell carcinoma\nMelanoma\nMelanocytic nevi/Mole\nIchthyosis\nPityriasis r\nosea\nActin\nic keratosis\nTinea cruris  corporis or faciei\nKeloid/Hype\nrtrophic scar\nSeborrheic\n keratosis\nFixed d\nrug eruption\nLichen sclerosus et atrophicus\nCh\nicken pox\nTinea ma\nnuum\nLichen planus\nUrticaria\nPityriasis ve\nrsicolorPsoriasis\nPe\nmphigus\nCandid\niasis\nBowens disease\nViral warts\nSquamous cell c\narcino\nma\nBullous pe\nmphigoidEcze\nma\nDis\ncoid lupus erythematosus\nPercentage\nSensitivity\nSpecificity\nPPV\nNPV\nSensitivity and specificity of five fold validation data(a)\n(b)\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosacea\nKeloid/Hypertrophic scar\nFixed drug eruption\nHerpes zoster\nVitiligo/Leucoderma\nIchthyosis\nHidradenitis suppurativa\nMelasma\nKeratoacanthoma\n', ' validation studies of AI-\nbased algorithms related to dermatology in clinical settings.\nMaterial and methods\nThe study aimed to develop a convolutional neural network\n(CNN)-based algorithm trained with clinical images of 40 differ-\nent skin diseases. A user-friendly, smartphone app was also gen-\nerated, and a clinical validation study on 5014 patients was done\nby physicians in urban, rural primary care and tertiary care set-\ntings. The app’s analysis of a single image of the lesion was com-\npared to the consensus diagnosis made by two board-certiﬁed\ndermatologists. Only patients, for whom the treating board-cer-\ntiﬁed dermatologist was conﬁdent of the diagnosis and the diag-\nnosis was cross-veriﬁed by another board-certiﬁed\ndermatologist, were included. If there was no consensus then the\npatient was excluded from the study.\nGeneration of the CNN-based algorithm\nThe CNN architecture used was a modiﬁed version of Densenet-\n161 with optimized augmentation and inference pipelines.\n16\nThese optimizations were derived through rigorous experiments\nand were attuned to clinical skin images. All training and testing\nimages were annotated and segmented so a bounding box sur-\nrounded lesion of interest and irrelevant features such as rulers\nand surgical markings were discarded. The 17 718 raw images\nwere sourced from public databases (http://www.hellenicderma\ntlas.com/en and http://www.danderm.dk/atlas), after obtaining\npermission from them, as well as images from dermatologists in\nIndia. Of these, 310 images were discarded during the\npreprocessing stage due to poor resolution or multiple lesions.\nOut of the remaining 17 408 images, 1990 images belonged to\nthe non-speciﬁc category and 15 418 images were within the 40\nselected disease categories (Table 1). These images were split\ninto ﬁve equal parts (folds) and ﬁve iterations of training and\nvalidation were performed in a manner so that within each itera-\ntion, a different fold of the data was held-out for validation while\nthe remaining fourfolds were used for learning. The training\nimages were 12 350 and testing images were 3068. Since the\ndataset was imbalanced, a custom modiﬁed version of the\nweighted focal loss function was used to train the network. The\nweights for each class were calculated using the inverse sample\nsize method. Our test images were either in-distribution images\ni.e. images within 40 training classes or out-of-distribution sets\nthat we named as non-speciﬁc set.\nWe initially used Inception-v4, which was one of the most\npopular CNN models at the time. For 10 diseases, a sensitivity of\naround 85% was achieved, but for the 20-disease class model,\nthis fell to around 65%. We experimented with many parameters\nlike learning rate and choice of optimizer etc. with no signiﬁcant\nimprovements. It was felt that as the architectures became dee-\nper, there could be problems related to over-ﬁtting. According\nto Huanget al.,convolutional networks can be substantially dee-\nper, more accurate and efﬁcient to train if they contain shorter\nconnections between layers close to the input and those close to\nthe output.\n17 Their implementation of this architecture is called\nDenseNet. In this architecture, to ensure maximum information\nﬂow between layers, each layer obtains additional inputs from all\npreceding layers and passes on its own feature-maps to all subse-\nquent layers.\n17 We found that the DenseNet architecture pro-\nvided better results than the other contemporary architectures\nsuch as Inception-Resnet-V2, Resnets-50, Resnet-101 and Res-\nnet-152. We tried several DenseNet variants']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2553.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,"Accuracy, AUC, sensitivity, specificity, PPV, NPV.",,,,"[14, 6, 11]","[' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' 78.96 /C6 8.07 99.31 /C6 0.09\n20 Lichen sclerosus 83 137 0.93 73.18 /C6 8.41 99.88 /C6 0.08 87.50 /C6 7.48 99.68 /C6 0.10\n21 Melanocytic nevi /Mole 153 134 0.95 77.02 /C6 8.37 99.73 /C6 0.11 84.78 /C6 6.35 99.65 /C6 0.12\n22 Melanoma 99 211 0.95 77.74 /C6 2.10 99.81 /C6 0.08 82.41 /C6 3.91 99.45 /C6 0.09\n23 Melasma 88 77 0.99 87.50 /C6 6.25 99.78 /C6 0.08 88.00 /C6 6.43 99.94 /C6 0.06\n24 Milia 52 60 0.94 77.86 /C6 12.92 99.95 /C6 0.02 85.43 /C6 10.1 99.75 /C6 0.10\n25 Molluscum contagiosum 48 234 0.97 79.75 /C6 7.72 99.81 /C6 0.08 85.41 /C6 4.63 99.58 /C6 0.13\n26 Pemphigus 99 137 0.94 66.51 /C6 1.96 99.76 /C6 0.07 80.81 /C6 4.33 99.59 /C6 0.07\n27 Pityriasis rosea 95 155 0.95 76.80 /C6 5.21 99.75 /C6 0.08 82.42 /C6 4.68 99.61 /C6 0.06\n28 Pityriasis versicolour 67 108 0.95 69.71 /C6 5.92 99.83 /C6 0.07 83.51 /C6 5.90 99.64 /C6 0.07\n29 Psoriasis 452 421 0.94 68.00 /C6 5.06 99.18 /C6 0.13 80.40 /C6 6.34 98.41 /C6 0.20\n30 Rosacea 184 255 0.98 90.17 /C6 4.40 99.62 /C6 0.13 92.44 /C6 2.47 99.70- /C6 0.06\n31 Seborrhoeic keratosis 211 238 0.97 74.39 /C6 3.92 99.39 /C6 0.17 86.66 /C6 3.35 99.47 /C6 0.16\n32 Squamous cell carcinoma 157 212 0.92 58.29 /C6 4.65 99.41 /C6 0.08 75.74 /C6 5.82 99.03 /C6 0.06\n33 Tinea capitis 4 153 0.96 79.', ' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2546.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?","AUC, accuracy, specificity, sensitivity, PPV, NPV.",,,,"[14, 9, 15]","[' of the CNN model for the diagnosis of 40 skin\ndiseases showed a top-1 overall accuracy of 76.93/C6 0.88% and\nAUC of 0.95/C6 0.02 (Table 1). High speciﬁcities and NPV were\nobserved for all disease classes (Table 1, Fig. 2a, and b). The sen-\nsitivities and PPV were more varied but 34/40 skin diseases\nshowed high PPVs in 80–99% range. AUC values and ROC plots\nfor different diseases in model validation and clinical study are\ngiven in Supplementary Figure S1.\nClinical validation\nThe app was tested on 5014 patients (3699 from tertiary care, 383\nfrom urban private practice, and 932 from rural primary care).\nOut of the 5014 patients, 760 patients’ images were in the nonspe-\nciﬁc (out-of-distribution) category. The mean AUC was\n0.90 /C6 0.07, top-1 overall accuracy 75.07% (CI= 73.75–76.36)\nand top-3 accuracy 89.62% (CI = 88.67–90.52) for combined\ndata from all centres (Table 2, Fig. 3a). The mean top-1 speciﬁcity\nwas 99.11% (CI= 99.06–99.15). Small differences were noted in\nthe overall accuracies of urban, rural primary and tertiary hospital\npatients, with the urban practice showing the highest and tertiary\ncare centres showing the lowest accuracy (Fig. 3a). We do not\npresent data for actinic keratosis, chickenpox, keratoacanthoma,\nmelanoma and tinea manuum because of less than 10 patients for\neach of these disease classes. The most signiﬁcant sensitivities were\nobserved for diagnosis of hidradenitis suppurativa, BCC, vitiligo,\nacne, alopecia, molluscum contagiosum, melasma, ichthyosis and\ntinea corporis, cruris or faciei. Bowen’s disease, bullous pem-\nphigoid, candidiasis, ﬁxed drug eruption, eczema, lichen sclero-\nsus, melanocytic naevus, rosacea and tinea pedis had lower\naccuracy across all sites (Figs 3b and 4a and b). The sensitivity of\ndiseases with larger sample sizes was generally greater except for\neczema and melanocytic nevi (Fig. 3b).\nThe top-3 sensitivity of most diseases, including eczema, was\nhigh (Figs 3b and 4b). Analyses of the confusion matrix showed\nthat many eczema lesions were misdiagnosed as psoriasis fol-\nlowed by lichen planus and tinea corporis, cruris or faciei (Fig. 3\na). Most of the skin cancers were detected in top-3 diagnoses as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation,', ' to calculate the true positive rate (same as\nsensitivity) and the false-positive rate (1 – speciﬁcity). ROC\ncurve was the plot of the true positive rate versus the false-\npositive rate.\nClinical validation study\nWe tested our mobile app against dermatologists’ diagnosis in\nthree different clinical settings, namely, at a tertiary care centre\nData \ncollection\nAlgorithm \nfinetuning & \nvalidation\nApp \ndevelopment\nClinical \nvalidation of \nApp\nPublic database of annotated images for \n40 skin diseases + normal skin + nonspecific\nN = 9,203\nPrivate data classified by board certified \ndermatologists\nN = 8,205\nCNN training images N =12,350\nCNN testing images N =3,068\n(1990 nonspecific images not included)\nComparison of model diagnosis against \npredetermined classification\n15,418 images used for training of model to \ngenerate smartphone app\nApp tested by physicians and compared with \ndermatologists’ diagnosis. N = 5,014 \nDetermination of app’s overall sensitivity and \ndisease-specific sensitivity, specificity, PPV, \nNPV and AUC values\nApp testing on rural primary-care patients\nN = 932\nApp testing on urban, private clinic patients\nN = 383\nApp testing on urban tertiary hospital patients\nN = 3,699\nFigure 1 This ﬂowchart depicts the different steps in data collection, algorithm generation, algorithm testing, app generation and app\nvalidation in clinical studies.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 539\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosaceaMelasma\nAcne\nVitiligo/Leucoderma\nTinea pedis\nAnogenital wartsKeratoacanthoma\nMolluscum contagiosumHidradenitis suppurativa\nHerpes zosterTinea capitis\nImpetigo and Pyodermas\nAlopecia\nMilia\nBasal cell carcinoma\nMelanoma\nMelanocytic nevi/Mole\nIchthyosis\nPityriasis r\nosea\nActin\nic keratosis\nTinea cruris  corporis or faciei\nKeloid/Hype\nrtrophic scar\nSeborrheic\n keratosis\nFixed d\nrug eruption\nLichen sclerosus et atrophicus\nCh\nicken pox\nTinea ma\nnuum\nLichen planus\nUrticaria\nPityriasis ve\nrsicolorPsoriasis\nPe\nmphigus\nCandid\niasis\nBowens disease\nViral warts\nSquamous cell c\narcino\nma\nBullous pe\nmphigoidEcze\nma\nDis\ncoid lupus erythematosus\nPercentage\nSensitivity\nSpecificity\nPPV\nNPV\nSensitivity and specificity of five fold validation data(a)\n(b)\n0\n25\n50\n75\n100\nNormal skin\nTinea unguium\nRosacea\nKeloid/Hypertrophic scar\nFixed drug eruption\nHerpes zoster\nVitiligo/Leucoderma\nIchthyosis\nHidradenitis suppurativa\nMelasma\nKeratoacanthoma\n', ' as\nshown in the confusion matrix (Fig. 4b).\nDiscussion\nThis cross-sectional study is the ﬁrst, large-scale, clinical valida-\ntion study of an AI-driven app involving a wide spectrum of\ncommon dermatological diseases on the skin of colour. More-\nover, this work shows the potential of an app to be successfully\nincorporated into clinical workﬂows by physicians using ordi-\nnary smartphones.\nNearly all previously published work on automated skin dis-\nease detection has focused onin silico algorithmic validation.\n21\nThere is also little published literature demonstrating high-efﬁ-\nciency neural networks for multi-disease classiﬁcation. AI-based\nalgorithms, particularly CNN, have been successfully applied in\nskin lesion classiﬁcation, mainly for skin cancers. However, only\na few researchers have used clinical/macroscopic images for mul-\nti-class skin disease identiﬁcation, representative of a real-world\nclinical scenario. In a study by Estevaet al., CNN performedat\npar with dermatologists for classiﬁcation of 9 skin disease cate-\ngories, skin cancers and their precursors, using clinical images\nwith an accuracy of 55.4%.\n22 Mendes et al. generated a CNN\nclassiﬁcation model for 12 skin cancers and related benign and\npremalignant skin conditions and achieved a total accuracy of\n78%.\n23 Han et al. reported a mean AUC of 0.91 and top-1 accu-\nracy between~55–57% on the same set of 12 diseases and, inter-\nestingly, found a substantial difference in speciﬁcities of skin\ncancer between an Asian skin dataset and Caucasian skin data-\nset.\n24 More recently, Liuet al. developed a deep learning system\n76.55\n90.51\n80.33\n94.33\n74.9\n89.22\n75.07\n89.62\n0\n25\n50\n75\n100\nUrban private\npractice\nRural Tertiary\nCombined\nClinical setting\nAccuracy\nTop_1_accuracy\nTop_3_accuracy\n(a)\n(b)\nFigure 3 (a) Overall Top-1 and Top-3 accuracies from clinical vali-\ndation of app in a private urban clinicN = 383), rural hospital\n(N = 932), tertiary hospital (N = 3699) and combined data\n(N = 5014). (b) Disease-speciﬁc Top-1 sensitivities, Top-3 sensitiv-\nities and Top-1 speciﬁcities. The length of the dotted line depicts\nan increase in sensitivity betweenﬁrst and third predictions of the\napp. The size of the icons depicts the number of patients for that\ndisease class.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\n542 Pangti et al.\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2541.0,free_text
A_machine_learning_based_decision_support_mobile_phone_application_for_diagnosis_of_common_dermatological_diseases,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[11, 16, 4]","[' diversity in the patient populations that\nconsult physicians in urban versus rural areas. The study was\napproved by the Ethics Committee of All India Institute of Med-\nical Sciences. We included consecutive patients presenting to a\nparticular investigator in the Outpatient Department, where the\ndermatologists were conﬁdent of the diagnosis, either by clinical\nexamination, laboratory investigation, and/or histopathological\nexamination. Dermatologists were blinded to the app results. A\nsingle representative image from each patient was tested on the\napp by a separate physician.\nResults\nThe overall scheme for this study is outlined in Figure 1.\nTable 2 shows th number of patients, AUC, Top‐1/Top‐3 sensitivity, speciﬁcity, positive predictive and negative predictive values for\ncombined clinical data from three different clinical settings.\nS.no. Disease class Number\nof patients\nAUC-\nclinic\nTop-1 sensitivity\n% (CI)\nTop-3\nsensitivity %\nTop-1\nspeciﬁcity %\nTop-1 PPV Top-1 NPV\n1 Acne 592 0.98 89.86 (87.15 –92.18) 97.97 98.19 86.92 98.64\n2 Alopecia 184 0.97 88.11 (82.55 –92.36) 96.22 99.28 82.32 99.54\n3 Anogenital warts 34 0.85 57.14 (39.35 –73.68) 71.43 99.70 57.14 99.70\n4 Basal cell carcinoma 123 0.98 93.55 (87.68 –97.17) 97.58 98.51 61.38 99.83\n5 Bowen ’s disease 11 0.77 45.45 (16.75 –76.62) 54.54 99.90 50.00 99.88\n6 Bullous pemphigoid 16 0.73 35.29 (14.21 –61.67) 47.06 100.00 100.00 99.78\n7 Candidiasis 16 0.81 41.18 (18.44 –67.10) 64.71 99.80 41.18 99.80\n8 Discoid lupus\nerythematosus\n47 0.90 60.42 (45.27 –74.23) 83.33 99.20 42.03 99.61\n9 Eczema 432 0.87 54.29 (49.46 –59.10) 81.90 97.58 67.83 95.78\n10 Fixed drug eruption 12 0.87 41.67 (15.16 –72.33) 75.00 99.70 25.00 99.86\n11 Herpes zoster 38 0.89 66.67 (49.78 –80.91) 79.49 99.60 56.52 99.74\n12 Hidradenitis suppurativa 32 0.97 93.94(79.78 –99.26) 93.94 99.62 62.00 99.96\n13 Ichthyosis 21 0.93 81.81 (59.72 –94.81) 86.36 99.78 62.07 99.92\n14 Impetigo and Pyodermas 109 0.89 57.27 (47.48 –66.66) 82.72 99.04 57.27 99.04\n15', '/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nFigure 4 (a) Confusion matrix from the top-1 prediction of clinical data by the app. The numbers in each row are normalized to 100. The\nactual patient numbers for each disease class are shown in parenthesis. The actual labels are depicted on the y-axis and predicted labels\nare on the x-axis. (b) Confusion matrix from the top-3 prediction of clinical data by the app. The numbers in each row are normalized to\n100. The actual patient numbers for each disease class are shown in parenthesis.\n© 2020 European Academy of Dermatology and VenereologyJEADV 2021, 35, 536–545\nAI-based mobile app for dermatology 543\n 14683083, 2021, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/jdv.16967 by Leiden University Libraries, Wiley Online Library on [23/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n\nto diagnose 26 common skin conditions from clinical images\nplus associated medical histories.25 During validation, Liuet al\nachieved a 71% top-1 accuracy and their algorithm surpassed\ndermatologists, PCPs and nurse practitioners in a clinical data-\nset.\n25 Our Top-1 accuracy was 77% and the mean AUC was 0.95\nfor algorithmic validation of 40 diseases.\nThere are no precedents for studies in which an AI-based, skin\ndisease diagnosis app has been tested in clinical settings. In our\nwork, a top-1 accuracy of 75% and a top-3 accuracy of 90% was\nachieved by the app for 35 skin diseases and was largely similar to\nour model validation results. Some differences in diagnostic per-\nformance (in terms of disease AUCs) between model and app\nwere noted and may be attributable to either fewer clinical valida-\ntion images for some diseases like Bowen’s disease, anogenital\nwarts, tinea pedis, rosacea, candidiasis, herpes zoster (Fig. 4a and\nSupplementary Figure S1) and/or differences in disease morphol-\nogy between white and pigmented skin. Eczema is a broad disease\ncategory, incorporating diseases with varied morphology such as\nlichen simplex chronicus, atopic dermatitis, nummular eczema\nand contact dermatitis. Training of the model with speciﬁc dis-\nease classes rather than a broad category of eczema may improve\nsensitivity. As shown in the confusion matrix, eczema was often\nconfused with psoriasis, lichen planus and tinea corporis, cruris\nor faciei (Figs 4a, b). To some extent, this mirrors the diagnostic\nconfusion faced by many physicians as well.\n2,25,26 A few melano-\ncytic nevi lesions were mistaken for BCC possibly because of their\nresemblance with pigmented BCC, which is more commonly seen\nin skin of colour. Besides, it was observed that diseases with dis-\ntinct morphology and predilection for certain sites (in a similar\nrecognizable background) such as melasma and tinea unguium\nhad better accuracies.\nIn this study, we found a high top-3 sensitivity for BCC\n(97.58%, n = 124) and SCC (92.86%,n = 14', ' imbalances. The mathemat-\nical expression of the loss function used by us is described\nbelow.\nFLðp\ntÞ¼/C0 αt ð1 /C0 ptÞγlogðptÞ\nLLðpÞ¼/C0 ð ylogðpÞþð 1 /C0 yÞlogð1 /C0 pÞÞ\nFCLðptÞ¼/C0 αtðytð1 /C0 ptÞγlogðptÞþð 1 /C0 ytÞpγ\nt logð1 /C0 ptÞÞ\nFL = Focal loss; LL = Log Loss; FCL = Our loss function\nimplementation.\nTable 1 shows number of images and diagnostic parameters fromﬁvefold algorithmic validation of 41 skin conditions\nS.S Disease class N, Private N, Public AUC-model Sensitivity Speci ﬁcity PPV NPV\n1 Acne 337 240 0.98 86.23 /C6 3.26 99.56 /C6 0.13 86.32 /C6 4.22 99.46 /C6 0.19\n2 Actinic keratosis 249 372 0.97 76.49 /C6 3.25 99.04 /C6 0.22 82.83 /C6 3.72 98.85 /C6 0.19\n3 Alopecia 109 152 0.97 78.38 /C6 6.25 99.74 /C6 0.09 86.60 /C6 5.39 99.65 /C6 0.08\n4 Anogenital warts 86 96 0.95 80.22 /C6 1.96 99.92 /C6 0.04 85.69 /C6 6.51 99.73 /C6 0.10\n5 Basal cell carcinoma 365 506 0.97 77.77 /C6 3.58 99.10 /C6 0.13 83.89 /C6 1.09 98.50 /C6 0.27\n6 Bowen ’s disease 130 179 0.92 61.31 /C6 10.78 99.60 /C6 0.14 79.31 /C6 8.58 99.06 /C6 0.11\n7 Bullous pemphigoid 54 92 0.91 58.22 /C6 13.60 99.90 /C6 0.05 82.86 /C6 8.45 99.60 /C6 0.14\n8 Candidiasis 118 187 0.94 65.24 /C6 6.39 99.54 /C6 0.13 77.47 /C6 4.65 99.35 /C6 0.11\n9 Chicken pox 77 108 0.94 72.43 /C6 6.99 99.77 /C6 0.07 80.94 /C6 9.67 99.59 /C6 0.12\n10 Discoid lupus erythematosus 85 134 0.89 49.78 /C6 6.70 99.83 /C6 0.07 73.06 /C6 6.84 99.36 /C6 0.07\n11 Eczema 560 336 0.92 57.52 /C6 3.37 99.00 /']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2540.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,Diagnostic,,,,"[10, 11, 3]","['iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3', ' future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart', ' such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices, including steps,\nheart rate, calories burned, and distance traveled. At the end\nof each week, they were asked to ﬁll out eight PROMIS short\nforms as a self-report assessment of their health status [4].\nA. Activity Data\nThe Fitbit Charge 2 (Fitbit Inc., San Francisco, CA, USA)\nis a popular commercially available activity tracker that can\nrecord a person’s daily activities and health indices like heart\nrate, steps, and sleep ( Table I ). Previous work has validated\nthe accuracy of heart rate monitoring speciﬁcally in the Fitbit\nCharge 2 [29]. The Fitbit hardware and its computational algo-\nrithms for calculating step counts and physical activity have been\nT ABLE I\nSUMMARY OF 17 T YPES OF FEA TURECOLLECTED FROM FITBIT PER DAY\n∗means that feature was eliminated for model input because it was highly sparse or redun-\ndant.\nvalidated using other Fitbit devices [30], [31]. The Fitbit Charge\n2 estimates activity using metabolic equivalents (METs), which\nare calculated based on heart rate and distance traveled [32].\nHeart rate during activity is also provided, however it has been\nshown to be inaccurate during activity [33]. Data quality was\nassured by verifying that there were no extreme outliers based\non subject-speciﬁc inter-quartile range [34]. We aggregated the\ndata for each day to compensate for noise and redundancy. Af-\nter data preprocessing, tracker distance was eliminated because\nit was identical to total distance, and logged activity distance\nand sedentary active distance were also deleted because of high\nsparsity. As a result, there were 14 features per day for each\npatient in our model.\nB. Patient-Reported Outcome Measures\nPatient-Reported Outcomes Measurement Information Sys-\ntems (PROMIS) questionnaires are a library of instruments\ndeveloped and validated to measure many domains of phys-\nical and mental health [15]. This analysis uses data from\neight PROMIS instruments: Global Physical Health and Global\nMental Health, which are two composite scores from the Global-\n10 short form [35]; Fatigue-Short Form 4a; Physical Function-\nShort Form 10a; Emotional Distress-Anxiety-Short Form 6a;\nDepression-Short Form 4a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2572.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",Patients with stable ischemic heart disease.,,,,"[11, 12, 0]","[' future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart', ' activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart failure self-management: A pilot study,” JMIR mHealth\nuHealth, vol. 3, no. 2, Apr. 2015, Art. no. e33.\n[12] T. M. Hale, K. Jethwani, M. S. Kandola, F. Saldana, and J. C. Kvedar, “A\nremote medication monitoring system for chronic heart failure patients to\nreduce readmissions: A two-arm randomized pilot study,” J. Med. Internet\nRes., vol. 18, no. 5, Apr. 2016, Art. no. e91.\n[13] P . A. Pilkonis, L. Y u, N. E. Dodds, K. L. Johnston, C. C. Maihoefer,\nand S. M. Lawrence, “V alidation of the depression item bank from the\npatient-reported outcomes measurement information system (PROMIS)\nin a three-month observational study,” J. Psychiatric Res. , vol. 56, no. 1,\npp. 112–119, 2014.\n[14] D. Cella et al. , “Initial adult health item banks and ﬁrst wave testing of the\npatient-reported outcomes measurement information system (PROMIS)\nnetwork: 2005-2008,” J. Clin. Epidemiol. , vol. 63, no. 11, pp. 1179–1194,\n2011.\n[15] H. Liu et al. , “Representativeness of the patient-reported outcomes mea-\nsurement information system internet panel,” J. Clin. Epidemiol. , vol. 63,\nno. 11, pp. 1169–1178, 2010.\n[16] B. L. Egleston, S. M. Miller, and N. J. Meropol, “The impact of misclas-\nsiﬁcation due to survey response fatigue on estimation and identiﬁability\nof treatment effects,” Statist. Med. , vol. 30, no. 30, pp. 3560–3572, 2011.\n[17] S. R. Porter, M. E. Whitcomb, and W. H. Weitzer, “Multiple surveys\nof students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen H', '878 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nA Machine Learning Approach to Classifying\nSelf-Reported Health Status in a Cohort of\nPatients With Heart Disease Using\nActivity T racker Data\nYiwen Meng , William Speier , Member, IEEE, Chrisandra Shufelt, Sandy Joung, Jennifer E Van Eyk,\nC. Noel Bairey Merz, Mayra Lopez, Brennan Spiegel, and Corey W. Arnold\nAbstract— Constructing statistical models using per-\nsonal sensor data could allow for tracking health status\nover time, thereby enabling the possibility of early inter-\nvention. The goal of this study was to use machine learning\nalgorithms to classify patient-reported outcomes (PROs)\nusing activity tracker data in a cohort of patients with stable\nischemic heart disease (SIHD). A population of 182 patients\nwith SIHD were monitored over a period of 12 weeks. Each\nsubject received a Fitbit Charge 2 device to record daily\nactivity data, and each subject completed eight Patient-\nReported Outcomes Measurement Information Systems\nshort form at the end of each week as a self-assessment\nof their health status. Two models were built to classify\nPRO scores using activity tracker data. The ﬁrst model\ntreated each week independently, whereas the second\nused a hidden Markov model (HMM) to take advantage\nof correlations between successive weeks. Retrospective\nanalysis compared the classiﬁcation accuracy of the two\nmodels and the importance of each feature. In the inde-\npendent model, a random forest classiﬁer achieved a mean\narea under curve (AUC) of 0.76 for classifying the physical\nManuscript received January 9, 2019; revised March 28, 2019 and\nMay 16, 2019; accepted May 29, 2019. Date of publication June 11,\n2019; date of current version March 6, 2020. This work was supported in\npart by the California Initiative to Advance Precision Medicine (CIAPM)\n(BS, NBM, and JVE), in part by the National Heart, Lung, and Blood In-\nstitute (NIH/NHLBI R56HL135425, R01HL141773 CWA; K23HL127262,\nCS), in part by the National Center for Research Resources (NIH/NCRR\nUL1RR033176), in part by the National Center for Advancing T ransla-\ntional Sciences (NCA TS) and UCLA Clinical T ranslational Science In-\nstitute (NIH/NCA TS UL1TR000124), in part by the Advanced Clinical\nBiosystems Research Institute (JVE), in part by the Erika Glazer En-\ndowed Chair in Women’s Heart Health (NBM and JVE), and in part by\nthe Barbra Streisand Women’s Cardiovascular Research and Education\nProgram. (Corresponding author: Corey W. Arnold.)\nY . Meng, W. Speier, and C. W. Arnold are with the Computational Inte-\ngrated Diagnostics Laboratory, Department of Bioengineering, Depart-\nment of Radiology, and Department of Pathology, University of California\nLos Angeles, Los Angeles, CA 90024 USA (e-mail: , lanyexiaosa@\nucla.edu; speier@ucla.edu; cwarnold@ucla.edu).\nC. Shufelt, S. Joung, and J. E. V . Eyk, and C. N. B. Merz are\nwith Barbra Streisand Women’s Heart Center, Smidt Heart Insti-\ntute, Los Angeles, CA 90048 USA (e-mail: , Chrisandra.Shufelt@\ncshs']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2574.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Unknown from this paper.,,,,"[10, 6, 2]","['iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3', ' transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENCE ALGORITHMS\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement over GBRT.\n†Signiﬁcant improvement over both GBRT and AdaBoost.\nclassiﬁed state,\nS (yt |yt−1 ,...,y 1 ,x t ,...,x 1 )= P (xt |yt )\n∗\n∑\nP( yt |yt−1 ) ∗S( yt−1 |yt−2 ,..., xt−1 ,... ) (1)\nwhere the weekly PRO score was treated as state yt , with obser-\nvation of features xt . The emission probability, P (xt |yt ), com-\nputed the probability of the observed feature vector xt given\nstate yt , computed from the random forest classiﬁer and P (yt ):\nP (xt |yt ) ∝ P (yt |xt )\nP (yt ) (2)\nAt the ﬁrst-time step, the transition probability distribution is\nundeﬁned, so the state probability was:\nS (y1 |x1 ) ∝ P (x1 |y1 ) P (y1 ) (3)\nFor analysis, states were binarized according to the criteria\ndeﬁned above. Because dichotomizing PRO score values loses\nsome information and precision, a regression analysis was con-\nducted between the median value of HMM stages and actual\nscores for the HMM. This method of predicting PRO scores\nwas compared against multinomial logistic regression to evalu-\nate the accuracy of predicting PRO scores over time.\nIV . R ESULTS\nTable II shows the mean AUC for binary classiﬁcation of\nPRO scores for the seven PROMIS measures using GBRT, Ad-\naBoost and RF. The highest mean AUC was 0.75 using RF for\nclassifying Physical Function, while the lowest was 0.47 using\nAdaBoost for Depression. The results indicated that RF signif-\nicantly outperformed other models in classiﬁcation of Anxiety\nand Depression (p < 0.05), and it was also signiﬁcantly better\nthan GBRT for Global Physical Health and Mental Health (p =\n0.01 and p = 0.01, respectively). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps,', ' experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2560.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q4,How was the dataset described in this study before predictive modeling was performed?,Unknown from this paper,,,,"[10, 9, 5]","['iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3', ' used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach', '17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig. 2. Histogram of number of weeks of evaluable data for the 182\nsubjects used in the dataset.\nweeks of evaluable data for each patient ranged from two to 12\nweeks as shown in Fig. 2 .\nA. Independent Per Week Model by Machine Learning\nAlgorithms\nSince survey scores were generated per week, a naive ap-\nproach for using this data is to treat each week independently.\nT h el e f tp l o ti n Fig. 3 illustrates the idea of the independent\nmodel as an example for one subject with a number of weeks\nof evaluable data of 12 weeks. The features for each of the\nseven days were appended into a single feature vector, which\nwas then used as the input for binary classiﬁcation of each PRO\nscore. Ensemble methods like Adaboost, GBRT (gradient boost-\ning regression tree) and Random Forest (RF) are relatively ro-\nbust over unbalanced dataset and is capable of generating better\nclassiﬁcation accuracy than other types of machine learning al-\ngorithms [37]. Each of these methods was applied to the dataset\nusing ten-fold cross-validation across subjects in conjunction\nwith grid search to ﬁnd the optimal parameters for each model.\nT-tests were applied to validate the statistical signiﬁcance for\neach comparison of the result with different p values: 0.05, 0.01\nas for different levels of signiﬁcance. A sensitivity analysis was\ncompleted to investigate the model performance against missing\nvalues in feature vector by randomly withholding values from\none to six days within a week.\nB. Hidden Markov Model (HMM) With Forward Algorithm\nIn order to track changes in PRO responses over time, a model\nwas built to incorporate temporal correlations of PRO scores\nacross weeks. As shown in the right part of Figure 3 ,a nH M M\nwas used and formalized such that the state at each time point\ncorresponded to the PRO score for that week, with the features\ncollected for that week treated as observations. The transition\nmatrix was derived by counting the s state transitions from week\nto week. The original number of states for each PRO was found\nby number of unique responses, ranging from 15 to 36. In order\nto make the transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,,,,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?",Unknown from this paper.,,,,"[10, 9, 5]","['iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3', ' used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach', '17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig. 2. Histogram of number of weeks of evaluable data for the 182\nsubjects used in the dataset.\nweeks of evaluable data for each patient ranged from two to 12\nweeks as shown in Fig. 2 .\nA. Independent Per Week Model by Machine Learning\nAlgorithms\nSince survey scores were generated per week, a naive ap-\nproach for using this data is to treat each week independently.\nT h el e f tp l o ti n Fig. 3 illustrates the idea of the independent\nmodel as an example for one subject with a number of weeks\nof evaluable data of 12 weeks. The features for each of the\nseven days were appended into a single feature vector, which\nwas then used as the input for binary classiﬁcation of each PRO\nscore. Ensemble methods like Adaboost, GBRT (gradient boost-\ning regression tree) and Random Forest (RF) are relatively ro-\nbust over unbalanced dataset and is capable of generating better\nclassiﬁcation accuracy than other types of machine learning al-\ngorithms [37]. Each of these methods was applied to the dataset\nusing ten-fold cross-validation across subjects in conjunction\nwith grid search to ﬁnd the optimal parameters for each model.\nT-tests were applied to validate the statistical signiﬁcance for\neach comparison of the result with different p values: 0.05, 0.01\nas for different levels of signiﬁcance. A sensitivity analysis was\ncompleted to investigate the model performance against missing\nvalues in feature vector by randomly withholding values from\none to six days within a week.\nB. Hidden Markov Model (HMM) With Forward Algorithm\nIn order to track changes in PRO responses over time, a model\nwas built to incorporate temporal correlations of PRO scores\nacross weeks. As shown in the right part of Figure 3 ,a nH M M\nwas used and formalized such that the state at each time point\ncorresponded to the PRO score for that week, with the features\ncollected for that week treated as observations. The transition\nmatrix was derived by counting the s state transitions from week\nto week. The original number of states for each PRO was found\nby number of unique responses, ranging from 15 to 36. In order\nto make the transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2551.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q6,What preprocessing techniques on the included variables/features were applied in this study?,Unknown from this paper.,,,,"[7, 5, 10]","['). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly', '17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig. 2. Histogram of number of weeks of evaluable data for the 182\nsubjects used in the dataset.\nweeks of evaluable data for each patient ranged from two to 12\nweeks as shown in Fig. 2 .\nA. Independent Per Week Model by Machine Learning\nAlgorithms\nSince survey scores were generated per week, a naive ap-\nproach for using this data is to treat each week independently.\nT h el e f tp l o ti n Fig. 3 illustrates the idea of the independent\nmodel as an example for one subject with a number of weeks\nof evaluable data of 12 weeks. The features for each of the\nseven days were appended into a single feature vector, which\nwas then used as the input for binary classiﬁcation of each PRO\nscore. Ensemble methods like Adaboost, GBRT (gradient boost-\ning regression tree) and Random Forest (RF) are relatively ro-\nbust over unbalanced dataset and is capable of generating better\nclassiﬁcation accuracy than other types of machine learning al-\ngorithms [37]. Each of these methods was applied to the dataset\nusing ten-fold cross-validation across subjects in conjunction\nwith grid search to ﬁnd the optimal parameters for each model.\nT-tests were applied to validate the statistical signiﬁcance for\neach comparison of the result with different p values: 0.05, 0.01\nas for different levels of signiﬁcance. A sensitivity analysis was\ncompleted to investigate the model performance against missing\nvalues in feature vector by randomly withholding values from\none to six days within a week.\nB. Hidden Markov Model (HMM) With Forward Algorithm\nIn order to track changes in PRO responses over time, a model\nwas built to incorporate temporal correlations of PRO scores\nacross weeks. As shown in the right part of Figure 3 ,a nH M M\nwas used and formalized such that the state at each time point\ncorresponded to the PRO score for that week, with the features\ncollected for that week treated as observations. The transition\nmatrix was derived by counting the s state transitions from week\nto week. The original number of states for each PRO was found\nby number of unique responses, ranging from 15 to 36. In order\nto make the transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN', 'iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2540.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q7,How is missing data handled in this study?,Subjects with only one week eliminated.,,,,"[2, 5, 11]","[' experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices', '17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig. 2. Histogram of number of weeks of evaluable data for the 182\nsubjects used in the dataset.\nweeks of evaluable data for each patient ranged from two to 12\nweeks as shown in Fig. 2 .\nA. Independent Per Week Model by Machine Learning\nAlgorithms\nSince survey scores were generated per week, a naive ap-\nproach for using this data is to treat each week independently.\nT h el e f tp l o ti n Fig. 3 illustrates the idea of the independent\nmodel as an example for one subject with a number of weeks\nof evaluable data of 12 weeks. The features for each of the\nseven days were appended into a single feature vector, which\nwas then used as the input for binary classiﬁcation of each PRO\nscore. Ensemble methods like Adaboost, GBRT (gradient boost-\ning regression tree) and Random Forest (RF) are relatively ro-\nbust over unbalanced dataset and is capable of generating better\nclassiﬁcation accuracy than other types of machine learning al-\ngorithms [37]. Each of these methods was applied to the dataset\nusing ten-fold cross-validation across subjects in conjunction\nwith grid search to ﬁnd the optimal parameters for each model.\nT-tests were applied to validate the statistical signiﬁcance for\neach comparison of the result with different p values: 0.05, 0.01\nas for different levels of signiﬁcance. A sensitivity analysis was\ncompleted to investigate the model performance against missing\nvalues in feature vector by randomly withholding values from\none to six days within a week.\nB. Hidden Markov Model (HMM) With Forward Algorithm\nIn order to track changes in PRO responses over time, a model\nwas built to incorporate temporal correlations of PRO scores\nacross weeks. As shown in the right part of Figure 3 ,a nH M M\nwas used and formalized such that the state at each time point\ncorresponded to the PRO score for that week, with the features\ncollected for that week treated as observations. The transition\nmatrix was derived by counting the s state transitions from week\nto week. The original number of states for each PRO was found\nby number of unique responses, ranging from 15 to 36. In order\nto make the transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN', ' future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2564.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[11, 12, 15]","[' future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart', ' activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart failure self-management: A pilot study,” JMIR mHealth\nuHealth, vol. 3, no. 2, Apr. 2015, Art. no. e33.\n[12] T. M. Hale, K. Jethwani, M. S. Kandola, F. Saldana, and J. C. Kvedar, “A\nremote medication monitoring system for chronic heart failure patients to\nreduce readmissions: A two-arm randomized pilot study,” J. Med. Internet\nRes., vol. 18, no. 5, Apr. 2016, Art. no. e91.\n[13] P . A. Pilkonis, L. Y u, N. E. Dodds, K. L. Johnston, C. C. Maihoefer,\nand S. M. Lawrence, “V alidation of the depression item bank from the\npatient-reported outcomes measurement information system (PROMIS)\nin a three-month observational study,” J. Psychiatric Res. , vol. 56, no. 1,\npp. 112–119, 2014.\n[14] D. Cella et al. , “Initial adult health item banks and ﬁrst wave testing of the\npatient-reported outcomes measurement information system (PROMIS)\nnetwork: 2005-2008,” J. Clin. Epidemiol. , vol. 63, no. 11, pp. 1179–1194,\n2011.\n[15] H. Liu et al. , “Representativeness of the patient-reported outcomes mea-\nsurement information system internet panel,” J. Clin. Epidemiol. , vol. 63,\nno. 11, pp. 1169–1178, 2010.\n[16] B. L. Egleston, S. M. Miller, and N. J. Meropol, “The impact of misclas-\nsiﬁcation due to survey response fatigue on estimation and identiﬁability\nof treatment effects,” Statist. Med. , vol. 30, no. 30, pp. 3560–3572, 2011.\n[17] S. R. Porter, M. E. Whitcomb, and W. H. Weitzer, “Multiple surveys\nof students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen H', ' information system (PROMIS) gastrointestinal\nsymptom scales,” Am. J. Gastroenterol. , vol. 109, no. 11, pp. 1804–1814,\n2014.\n[37] J. O. Ogutu, H. P . Piepho, and T. Schulz-Streeck, “A comparison of random\nforests, boosting and support vector machines for genomic selection,”\nBMC Proc. , vol. 5, no. Suppl. 3, pp. 3–7, 2011.\n[38] R. J. Petrella, J. J. Koval, and D. A. Cunningham, “A self-paced step test\nto predict aerobic ﬁtness in older adults in the primary care clinic,” J. Am.\nGeriatrics Soc. , vol. 49, pp. 632–638, 2001.\n[39] M. Kachuee, M. M. Kiani, H. Mohammadzade, and M. Shabany, “Cuff-\nless blood pressure estimation algorithms for continuous health-care mon-\nitoring,” IEEE Trans. Biomed. Eng. , vol. 64, no. 4, pp. 859–869, Apr. 2017.\n[40] F. Fallo et al. , “Circadian blood pressure patterns and life stress,” Psy-\nchotherapy Psychosomatics , vol. 71, no. 6, pp. 350–356, 2002.\n[41] A. Stolcke and S. Omohundro, “Hidden Markov model induction by\nBayesian model merging,” Neural Inf. Process. Syst. , vol. 5, no. Ml,\npp. 11–18, 1993.\n[42] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzel, “Learning to diag-\nnose with LSTM recurrent neural networks,” in Proc Int. Conf. Learn.\nRepresentations, 2015, pp. 1–18, arXiv: 1511.03677(2015).\n[43] A. Rajkomar et al. , “Scalable and accurate deep learning for electronic\nhealth records,” npj Digit. Med. , vol. 1, 2018, Art. no. 18.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2294.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q9,Which prediction models were used in this study?,Hidden Markov Models,,,,"[9, 3, 15]","[' used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach', ' such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices, including steps,\nheart rate, calories burned, and distance traveled. At the end\nof each week, they were asked to ﬁll out eight PROMIS short\nforms as a self-report assessment of their health status [4].\nA. Activity Data\nThe Fitbit Charge 2 (Fitbit Inc., San Francisco, CA, USA)\nis a popular commercially available activity tracker that can\nrecord a person’s daily activities and health indices like heart\nrate, steps, and sleep ( Table I ). Previous work has validated\nthe accuracy of heart rate monitoring speciﬁcally in the Fitbit\nCharge 2 [29]. The Fitbit hardware and its computational algo-\nrithms for calculating step counts and physical activity have been\nT ABLE I\nSUMMARY OF 17 T YPES OF FEA TURECOLLECTED FROM FITBIT PER DAY\n∗means that feature was eliminated for model input because it was highly sparse or redun-\ndant.\nvalidated using other Fitbit devices [30], [31]. The Fitbit Charge\n2 estimates activity using metabolic equivalents (METs), which\nare calculated based on heart rate and distance traveled [32].\nHeart rate during activity is also provided, however it has been\nshown to be inaccurate during activity [33]. Data quality was\nassured by verifying that there were no extreme outliers based\non subject-speciﬁc inter-quartile range [34]. We aggregated the\ndata for each day to compensate for noise and redundancy. Af-\nter data preprocessing, tracker distance was eliminated because\nit was identical to total distance, and logged activity distance\nand sedentary active distance were also deleted because of high\nsparsity. As a result, there were 14 features per day for each\npatient in our model.\nB. Patient-Reported Outcome Measures\nPatient-Reported Outcomes Measurement Information Sys-\ntems (PROMIS) questionnaires are a library of instruments\ndeveloped and validated to measure many domains of phys-\nical and mental health [15]. This analysis uses data from\neight PROMIS instruments: Global Physical Health and Global\nMental Health, which are two composite scores from the Global-\n10 short form [35]; Fatigue-Short Form 4a; Physical Function-\nShort Form 10a; Emotional Distress-Anxiety-Short Form 6a;\nDepression-Short Form 4a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on', ' information system (PROMIS) gastrointestinal\nsymptom scales,” Am. J. Gastroenterol. , vol. 109, no. 11, pp. 1804–1814,\n2014.\n[37] J. O. Ogutu, H. P . Piepho, and T. Schulz-Streeck, “A comparison of random\nforests, boosting and support vector machines for genomic selection,”\nBMC Proc. , vol. 5, no. Suppl. 3, pp. 3–7, 2011.\n[38] R. J. Petrella, J. J. Koval, and D. A. Cunningham, “A self-paced step test\nto predict aerobic ﬁtness in older adults in the primary care clinic,” J. Am.\nGeriatrics Soc. , vol. 49, pp. 632–638, 2001.\n[39] M. Kachuee, M. M. Kiani, H. Mohammadzade, and M. Shabany, “Cuff-\nless blood pressure estimation algorithms for continuous health-care mon-\nitoring,” IEEE Trans. Biomed. Eng. , vol. 64, no. 4, pp. 859–869, Apr. 2017.\n[40] F. Fallo et al. , “Circadian blood pressure patterns and life stress,” Psy-\nchotherapy Psychosomatics , vol. 71, no. 6, pp. 350–356, 2002.\n[41] A. Stolcke and S. Omohundro, “Hidden Markov model induction by\nBayesian model merging,” Neural Inf. Process. Syst. , vol. 5, no. Ml,\npp. 11–18, 1993.\n[42] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzel, “Learning to diag-\nnose with LSTM recurrent neural networks,” in Proc Int. Conf. Learn.\nRepresentations, 2015, pp. 1–18, arXiv: 1511.03677(2015).\n[43] A. Rajkomar et al. , “Scalable and accurate deep learning for electronic\nhealth records,” npj Digit. Med. , vol. 1, 2018, Art. no. 18.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2275.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,Random forest selected; hyperparameter tuning not specified.,,,,"[11, 0, 6]","[' future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart', '878 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nA Machine Learning Approach to Classifying\nSelf-Reported Health Status in a Cohort of\nPatients With Heart Disease Using\nActivity T racker Data\nYiwen Meng , William Speier , Member, IEEE, Chrisandra Shufelt, Sandy Joung, Jennifer E Van Eyk,\nC. Noel Bairey Merz, Mayra Lopez, Brennan Spiegel, and Corey W. Arnold\nAbstract— Constructing statistical models using per-\nsonal sensor data could allow for tracking health status\nover time, thereby enabling the possibility of early inter-\nvention. The goal of this study was to use machine learning\nalgorithms to classify patient-reported outcomes (PROs)\nusing activity tracker data in a cohort of patients with stable\nischemic heart disease (SIHD). A population of 182 patients\nwith SIHD were monitored over a period of 12 weeks. Each\nsubject received a Fitbit Charge 2 device to record daily\nactivity data, and each subject completed eight Patient-\nReported Outcomes Measurement Information Systems\nshort form at the end of each week as a self-assessment\nof their health status. Two models were built to classify\nPRO scores using activity tracker data. The ﬁrst model\ntreated each week independently, whereas the second\nused a hidden Markov model (HMM) to take advantage\nof correlations between successive weeks. Retrospective\nanalysis compared the classiﬁcation accuracy of the two\nmodels and the importance of each feature. In the inde-\npendent model, a random forest classiﬁer achieved a mean\narea under curve (AUC) of 0.76 for classifying the physical\nManuscript received January 9, 2019; revised March 28, 2019 and\nMay 16, 2019; accepted May 29, 2019. Date of publication June 11,\n2019; date of current version March 6, 2020. This work was supported in\npart by the California Initiative to Advance Precision Medicine (CIAPM)\n(BS, NBM, and JVE), in part by the National Heart, Lung, and Blood In-\nstitute (NIH/NHLBI R56HL135425, R01HL141773 CWA; K23HL127262,\nCS), in part by the National Center for Research Resources (NIH/NCRR\nUL1RR033176), in part by the National Center for Advancing T ransla-\ntional Sciences (NCA TS) and UCLA Clinical T ranslational Science In-\nstitute (NIH/NCA TS UL1TR000124), in part by the Advanced Clinical\nBiosystems Research Institute (JVE), in part by the Erika Glazer En-\ndowed Chair in Women’s Heart Health (NBM and JVE), and in part by\nthe Barbra Streisand Women’s Cardiovascular Research and Education\nProgram. (Corresponding author: Corey W. Arnold.)\nY . Meng, W. Speier, and C. W. Arnold are with the Computational Inte-\ngrated Diagnostics Laboratory, Department of Bioengineering, Depart-\nment of Radiology, and Department of Pathology, University of California\nLos Angeles, Los Angeles, CA 90024 USA (e-mail: , lanyexiaosa@\nucla.edu; speier@ucla.edu; cwarnold@ucla.edu).\nC. Shufelt, S. Joung, and J. E. V . Eyk, and C. N. B. Merz are\nwith Barbra Streisand Women’s Heart Center, Smidt Heart Insti-\ntute, Los Angeles, CA 90048 USA (e-mail: , Chrisandra.Shufelt@\ncshs', ' transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENCE ALGORITHMS\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement over GBRT.\n†Signiﬁcant improvement over both GBRT and AdaBoost.\nclassiﬁed state,\nS (yt |yt−1 ,...,y 1 ,x t ,...,x 1 )= P (xt |yt )\n∗\n∑\nP( yt |yt−1 ) ∗S( yt−1 |yt−2 ,..., xt−1 ,... ) (1)\nwhere the weekly PRO score was treated as state yt , with obser-\nvation of features xt . The emission probability, P (xt |yt ), com-\nputed the probability of the observed feature vector xt given\nstate yt , computed from the random forest classiﬁer and P (yt ):\nP (xt |yt ) ∝ P (yt |xt )\nP (yt ) (2)\nAt the ﬁrst-time step, the transition probability distribution is\nundeﬁned, so the state probability was:\nS (y1 |x1 ) ∝ P (x1 |y1 ) P (y1 ) (3)\nFor analysis, states were binarized according to the criteria\ndeﬁned above. Because dichotomizing PRO score values loses\nsome information and precision, a regression analysis was con-\nducted between the median value of HMM stages and actual\nscores for the HMM. This method of predicting PRO scores\nwas compared against multinomial logistic regression to evalu-\nate the accuracy of predicting PRO scores over time.\nIV . R ESULTS\nTable II shows the mean AUC for binary classiﬁcation of\nPRO scores for the seven PROMIS measures using GBRT, Ad-\naBoost and RF. The highest mean AUC was 0.75 using RF for\nclassifying Physical Function, while the lowest was 0.47 using\nAdaBoost for Depression. The results indicated that RF signif-\nicantly outperformed other models in classiﬁcation of Anxiety\nand Depression (p < 0.05), and it was also signiﬁcantly better\nthan GBRT for Global Physical Health and Mental Health (p =\n0.01 and p = 0.01, respectively). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps,']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2570.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q11,How was data augmentation or generation used in this study?,Unknown from this paper.,,,,"[11, 10, 13]","[' future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3, pp. 462–466, 2016.\n[3] J. J. T. Black et al. , “A remote monitoring and telephone nurse coaching\nintervention to reduce readmissions among patients with heart failure:\nstudy protocol for the better effectiveness after transition-heart failure\n(BEA T-HF) randomized controlled trial,” Trials, vol. 15, no. 1, pp. 124–\n135, 2014.\n[4] W. Speier et al. , “Evaluating utility and compliance in a patient-based\neHealth study using continuous-time heart rate and activity trackers,” J.\nAm. Med. Inform. Assoc. , vol. 25, pp. 1386–1391, 2018.\n[5] J. Meyer and A. Hein, “Live long and prosper: Potentials of low-cost\nconsumer devices for the prevention of cardiovascular diseases,” J. Med.\nInternet Res. , vol. 15, no. 8, pp. 1–9, 2013.\n[6] M. Alharbi, N. Straiton, and R. Gallagher, “Harnessing the potential of\nwearable activity trackers for heart failure self-care,” Current Heart F ail-\nure Rep. , vol. 14, no. 1, pp. 23–29, Feb. 2017.\n[7] T. Ferguson, A. V . Rowlands, T. Olds, and C. Maher, “The validity of\nconsumer-level, activity monitors in healthy adults worn in free-living\nconditions: A cross-sectional study,” Int. J. Behav . Nutrition Phys. Activity ,\nvol. 12, no. 1, pp. 1–9, 2015.\n[8] N. C. Franklin, C. J. Lavie, and R. A. Arena, “Personal health technology:\nA new era in cardiovascular disease prevention,” P ostgrad. Med., vol. 127,\nno. 2, pp. 150–158, 2015.\n[9] C. Smith-spangler, A. L. Gienger, N. Lin, R. Lewis, C. D. Stave, and I.\nOlkin, “Clinician’s corner using pedometers to increase physical activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart', 'iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3', ' students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen Hsich et al. , “Identifying important risk factors for survival\nin systolic heart failure patients using random survival forests,” Circ.\nCardiovascular Qual. Outcomes , vol. 4, no. 1, pp. 39–45, 2014.\n[21] N. Limsopatham, C. Macdonald, and I. Ounis, “Learning to combine\nrepresentations for medical records search,” in Proc. 36th Int. ACM SIGIR\nConf. Res. Develop. Inf. Retrieval , 2013, pp. 833–836.\n[22] J. H. Morra, Zhuowen Tu, L. G. Apostolova, A. E. Green, A. W. Toga, and\nP . M. Thompson, “Comparison of AdaBoost and support vector machines\nfor detecting alzheimer’s disease through automated hippocampal seg-\nmentation,” IEEE Trans. Med. Imag. , vol. 29, no. 1, pp. 30–43, Jan. 2010.\n[23] H. Ishwaran, U. B. Kogalur, E. H. Blackstone, and M. S. Lauer, “Random\nsurvival forests,” Ann. Appl. Statist. , vol. 2, no. 3, pp. 841–860, 2008.\n[24] S. Shen et al. , “A Bayesian model for estimating multi-state disease pro-\ngression,” Comput. Biol. Med. , vol. 81, pp. 111–120, 2017.\n[25] B. Schuster-B ¨ockler and A. Bateman, “An introduction to hidden Markov\nmodels,” Current Protocols Bioinf. , vol. 18, pp. A.3A.1–A.3A.9, 2007.\n[26] E. Birney Clamp and R. M. Durbin, “Genewise and genomewise,” Genome\nRes., vol. 14, no. 4, pp. 988–995, 2004.\n[27] L. A. Cadmus-bertram, B. H. Marcus, R. E. Patterson, B. A. Parker, and B.\nL. Morey, “Physical activity intervention for women,” Am. J. Preventive\nMed., vol. 49, no. 3, pp. 414–418, 2015.\n[28] J. B. Wang et al. , “Wearable sensor/device (ﬁtbit one) and SMS text-\nmessaging prompts to increase physical activity in overweight and obese\nadults: A randomized controlled trial,” T elemedicine e-Health, vol. 21, no.\n10, pp. 18–23, 2014.\n[29] S. Benedetto, C. Caldato, E. Bazzan, D. C. Greenwood, V . Pensabene,\nand P . Actis, “Assessment of the ﬁtbit']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2558.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared to other methods.",,,,"[9, 7, 10]","[' used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to capture information about patient’s health status, in some\ncases PROs could be a preferable method. Internet access en-\nables PRO data collection to be done outside of clinic through\nweb or mobile apps, which provides convenience and reduces\ntime commitment for patients.\nAccording to Table III , steps and total distance have signiﬁ-\ncantly higher importance for classifying the majority of survey\nscores, while calories BMR signiﬁcantly contributes to mental\nhealth scores, like Anxiety and Depression. Their importance\nfactor may due to data quality, as previous studies [5]–[7] have\nvalidated the data accuracy for step counts, distance travelled,\nand energy expenditure for activity trackers, while other fea-\ntures have not been validated in scientiﬁc work. As Fitbits are\nnot sold as medical devices, many of their features are not val-\nidated or regulated like other medical devices. In our study, we\nfound inconsistency in sleep data and sleeping stages for sub-\njects. It was likely that Fitbit was taken off for charging during\nnights. Therefore, future studies should notice user not always\ncharge it during nights to collect sleep data. Moreover, the data\nelements that have been validated are generally only tested in\nspeciﬁc devices, rather than across all activity trackers, so it is\nnot clear how these validation results translate to other devices.\nFuture studies should be conducted to validate these features.\nAs indicated by the correlation between subject’s average\nPRO scores and the number of missing PRO values, patients\nwith moderate to severe health status were less likely to com-\nplete PRO questionnaires routinely, which may have introduced\nbias for data collection in this study. Future studies could try\nto provide incentives for continued participation, which may\nmitigate study attrition. Eight PROMIS instruments were used\nin this study, and some redundancy existed between the speciﬁc\nshort forms such as Fatigue or Anxiety to the general Global-10\nshort form. Our current approach treated each score indepen-\ndently without considering this overlap. A possible future study\ncould predict PRO scores simultaneously in a joint model such\nas Bayesian network, which considers the correlations between\nPRO scores.\nIn our dataset of patients with SIHD based on adjudicated\nclinical data, HMMs achieved signiﬁcantly higher classiﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach', '). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly', 'iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2528.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q13,Which type of explainability techniques are used?,Random forests and Hidden Markov Models.,,,,"[6, 2, 10]","[' transition matrix less sparse, we deﬁned 10 states for\nall types of health status based on the score distribution of each\nPRO. The Forward algorithm computed the probability across\nstates at time t, with the maximum probability representing the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 881\nFig. 3. Illustration of independent week model (left) and Hidden Markov Model (right). For HMM, feature in each week was observed while the\nstate of health status transits from week to week.\nT ABLE II\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENCE ALGORITHMS\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement over GBRT.\n†Signiﬁcant improvement over both GBRT and AdaBoost.\nclassiﬁed state,\nS (yt |yt−1 ,...,y 1 ,x t ,...,x 1 )= P (xt |yt )\n∗\n∑\nP( yt |yt−1 ) ∗S( yt−1 |yt−2 ,..., xt−1 ,... ) (1)\nwhere the weekly PRO score was treated as state yt , with obser-\nvation of features xt . The emission probability, P (xt |yt ), com-\nputed the probability of the observed feature vector xt given\nstate yt , computed from the random forest classiﬁer and P (yt ):\nP (xt |yt ) ∝ P (yt |xt )\nP (yt ) (2)\nAt the ﬁrst-time step, the transition probability distribution is\nundeﬁned, so the state probability was:\nS (y1 |x1 ) ∝ P (x1 |y1 ) P (y1 ) (3)\nFor analysis, states were binarized according to the criteria\ndeﬁned above. Because dichotomizing PRO score values loses\nsome information and precision, a regression analysis was con-\nducted between the median value of HMM stages and actual\nscores for the HMM. This method of predicting PRO scores\nwas compared against multinomial logistic regression to evalu-\nate the accuracy of predicting PRO scores over time.\nIV . R ESULTS\nTable II shows the mean AUC for binary classiﬁcation of\nPRO scores for the seven PROMIS measures using GBRT, Ad-\naBoost and RF. The highest mean AUC was 0.75 using RF for\nclassifying Physical Function, while the lowest was 0.47 using\nAdaBoost for Depression. The results indicated that RF signif-\nicantly outperformed other models in classiﬁcation of Anxiety\nand Depression (p < 0.05), and it was also signiﬁcantly better\nthan GBRT for Global Physical Health and Mental Health (p =\n0.01 and p = 0.01, respectively). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps,', ' experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices', 'iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,"AUC, classification accuracy, ROCAUC.",,,,"[7, 4, 10]","['). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly', 'a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n880 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nFig. 1. Distribution of normal and abnormal (moderate to severe) class\nfor each PRO measure.\nFor function (i.e., Global Physical Health, Global Mental\nHealth, and Physical Function), scores less than 40 are classi-\nﬁed as moderate to severe, meaning less functional ability than\nnormal. For this study, PRO scores were predicted in two ways:\nregression was used to predict PRO scores from patient activity\ntracker data, and classiﬁcation was used to determine whether\nsubjects’ PRO scores were above the threshold for at least mod-\nerate severity. The distributions of PRO scores are shown in\nFig. 1 . Because of a lack of moderate or severe cases for social\nisolation (<2%), this variable was eliminated for analysis in our\nmodel.\nIII. M ETHODS\nMissing data is a common concern when dealing with activity\ntracker data and can result from subjects either forgetting to wear\ntheir devices or removing them for charging. Patients were asked\nto ﬁll out eight PROMIS questionnaires at the end of each week\nfor a 12-week monitoring period. In total, 19.1 percent of weeks\nhad missing PRO data and 16.6 percent of weeks had missing\nvalues from the activity tracker in four or more days. If data was\navailable for at least four days in a week, missing values were\npermuted by using the average value of the rest of the week for\nsteps or resting heart rate. Weeks with missing survey scores, as\nwell as those without step and resting heart rate data for more\nthan three days, were removed from the analysis.\nA correlation analysis between subjects’ missing Fitbit data\nand their average Global Physical Health and Global Mental\nHealth scores shows a slight negative relationship ( −0.11 and\n−0.09, respectively) that was not statistically signiﬁcant (p =\n0.13 and p = 0.23, respectively). The correlation coefﬁcient be-\ntween number of missing PROs and the average global health\nscores are −0.17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig', 'iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2545.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,"AUC, feature importance, and sensitivity to missing data.",,,,"[7, 4, 8]","['). The RF model was selected\nfor the remaining analyses because its performance was equiv-\nalent to or better than the other methods for classifying all PRO\nscores. Additionally, it was notable that the AUC related to self-\nreported physical health PROs such as Global Physical Health,\nFatigue, and Physical Function were higher than those related\nto mental health such as Global Mental Health, Anxiety, and\nDepression.\nWe then looked at the importance factor of each feature con-\ntributing to the classiﬁcation in the RF model. Table III displays\nthe importance factor for the 14 feature types summed over\nseven days. Features that were signiﬁcantly higher (p < 0.05)\nthan the average value for each classiﬁcation were determined.\nSteps, total distance, calories, and calories BMR contributed to\nmost of the PRO scores. The importance factor of light active\ndistance was signiﬁcantly better than other features for classify-\ning Global Physical Health and Physical Function, which were\nboth related to a subject’s physical health. On the other hand,\nresting heart rate contributed signiﬁcantly more than other fea-\ntures for classiﬁcation of mental health PROs such as Anxiety\nand Depression, while its importance factor was not signiﬁcantly\nhigher than other features in classiﬁcation of PROs related to\nphysical health.\nThe analysis was repeated using the RF classiﬁer and only the\nsigniﬁcant features from Table III and are shown in Table IV .\nBecause some studies such as [38] only used steps data to assess\nuser’s health status, we also compared the model performance\nin the same manner. The result suggested that the RF model\ncan generate signiﬁcantly better classiﬁcation accuracy with the\nselected features than all features from Fitbit for all PROMIS\nshort form survey scores except for Global Mental Health (p =\n0.37), with the highest AUC of 0.76 for classiﬁcation of Physical\nFunction.\nFig. 4 illustrates the results of sensitivity analysis on missing\nfeature data on RF classiﬁcation by randomly censoring data\nfrom one day to six days per a week. The results show that\nROCAUC decreased monotonically as days were removed. For\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n882 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nT ABLE III\nIMPORT ANCEFACTOR OF EACH FEA TURE FOR CLASSIFYING VARIOUS HEALTH STAT U S\nV alue in parentheses is the standard deviation. Bold values are signiﬁcantly higher (p < 0.05) than the average value for a feature (1/14 = 0.0714).\nT ABLE IV\nMEAN AND STA N DA R DDEVIA TIONROCAUC OF DIFFERENT\nFEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly', 'a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n880 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nFig. 1. Distribution of normal and abnormal (moderate to severe) class\nfor each PRO measure.\nFor function (i.e., Global Physical Health, Global Mental\nHealth, and Physical Function), scores less than 40 are classi-\nﬁed as moderate to severe, meaning less functional ability than\nnormal. For this study, PRO scores were predicted in two ways:\nregression was used to predict PRO scores from patient activity\ntracker data, and classiﬁcation was used to determine whether\nsubjects’ PRO scores were above the threshold for at least mod-\nerate severity. The distributions of PRO scores are shown in\nFig. 1 . Because of a lack of moderate or severe cases for social\nisolation (<2%), this variable was eliminated for analysis in our\nmodel.\nIII. M ETHODS\nMissing data is a common concern when dealing with activity\ntracker data and can result from subjects either forgetting to wear\ntheir devices or removing them for charging. Patients were asked\nto ﬁll out eight PROMIS questionnaires at the end of each week\nfor a 12-week monitoring period. In total, 19.1 percent of weeks\nhad missing PRO data and 16.6 percent of weeks had missing\nvalues from the activity tracker in four or more days. If data was\navailable for at least four days in a week, missing values were\npermuted by using the average value of the rest of the week for\nsteps or resting heart rate. Weeks with missing survey scores, as\nwell as those without step and resting heart rate data for more\nthan three days, were removed from the analysis.\nA correlation analysis between subjects’ missing Fitbit data\nand their average Global Physical Health and Global Mental\nHealth scores shows a slight negative relationship ( −0.11 and\n−0.09, respectively) that was not statistically signiﬁcant (p =\n0.13 and p = 0.23, respectively). The correlation coefﬁcient be-\ntween number of missing PROs and the average global health\nscores are −0.17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig', 'FEA TURESELECTION STRA TEGY\nBold values are the highest for a given PRO.\n∗Signiﬁcant improvement from Selected Feature over All Feature.\n†Signiﬁcant improvement from All Feature over Steps Only.\nGlobal Physical Health, the value at missing four days drops\nsigniﬁcantly compared to no missing data (p = 0.03), while\nthe difference at missing three days was not signiﬁcant (p =\n0.11). This was why that cutoff was chosen for inclusion in our\nanalysis.\nTable V displayed the comparison of means and standard\ndeviations of the AUC for each PRO measure using the inde-\npendent model and HMM. AUCs derived using the HMM were\nsigniﬁcantly higher than those from the independent model in\nall domains other than Fatigue and Sleep Disturbance. Depres-\nsion achieved the highest increase from 0.57 to 0.61. We also\ncompared the R 2 value of the regression analysis between HMM\nand multinomial logistic regression. The value were 0.079 and\n0.1526 from HMM in Global Physical Health and Physical\nFunction. They were signiﬁcantly better than the values achieved\nFig. 4. Plot of ROCAUC for each type of PRO after randomly withhold\nfeature values from one day to six days within a week.\nT ABLE V\nMEAN AND STA N DA R DDEVIA TION OF AUC VALUES BETWEEN THE\nINDEPENDENT WEEK MODEL AND THE HIDDEN MARKOV MODEL\nBold values are the highest AUC for a given PRO.\n∗Signiﬁcant improvement over the independent model.\nby the multinomial logit model (0.0016 and 0.0026, respec-\ntively; p < 0.001 for both). This result suggested that HMM\ncould also track the minor change of PRO score with higher\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 883\nprecision over time than baseline models like multinomial lo-\ngistic regression.\nV. D ISCUSSION\nIn general, the AUCs related to classifying physical health\nwere relatively higher than mental health PROs, such as Global\nMental Health, Anxiety and Depression. This result makes in-\ntuitive sense, as collected data, such as steps, total distance, and\ncalorie expenditure, are more directly related to physical health\nthan mental health. It might therefore be useful to develop hard-\nware to record data more related to mental health for future stud-\nies. For instance, there has been effort to develop non-invasive\nand continuous blood pressure tracking [39] using wearable de-\nvices, which may improve the performance of classifying men-\ntal health [40]. Also, only Anxiety and Depression measured by\nPROMIS instruments were used in this study, which lacks pre-\ncision as mental health is a broad and complicated ﬁeld. More\nthorough evaluations of subjects’ mental states could provide\nmore descriptive labels for training machine learning models,\nwhich could further improve performance in predicting mental\nhealth status.\nOur highest AUC was 0.79 from classiﬁcation of Physical\nFunction, which demonstrated the correlation between data col-\nlected from Fitbit and PROs. However, the AUC values also\nindicated that PROs cannot be completely determined by activ-\nity tracker data alone, suggesting that PROs, particularly those\npertaining to mental health such as depression, contain addi-\ntional information that was not captured in the tracking devices.\nWhile the current study demonstrates the use of activity track-\ners to']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2540.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?",Unknown from this paper.,,,,"[10, 4, 2]","['iﬁcation\naccuracy than treating weeks independently because they took\nadvantage of correlations in subjects’ survey scores from week\nto week. In our data-driven approach, the model states were de-\ntermined based on the distribution of PRO scores in the clinical\nstudy [41]. Score bins for the states were deﬁned to limit the\nsparsity during training and make the number or states consistent\nacross all of the PROMIS PROs tested. However, this may not be\nthe optimal way to deﬁne the number of states for clinical rep-\nresentation of health status. Future studies could conduct some\nanalysis to ﬁnd out the optimum number of states, which may\nfurther increase the classiﬁcation accuracy. In addition, another\nfuture direction would approach this as a regression problem to\npredict actual PRO scores with high precision over time.\nSequential deep learning models, such as recurrent neural net-\nworks (RNNs) and long-short-term-memory (LSTM) networks\nhave also demonstrated strong performance when dealing with\nsequential data [42], [43]. Therefore, these techniques may hold\npotential for applications to sensor data to classify or predict\nhealth status. However, such methods generally require a large\namount of training data, which was not available in the cur-\nrent study. In future studies, deep learning methods could be\nexplored if a sufﬁciently large data set were collected.\nWhile activity trackers are able to produce patient informa-\ntion within seconds or minutes, the sampling periods for PROs\nlike PROMIS [13] are on the order of weeks, requiring down-\nsampling of the Fitbit data for comparison. Given that the PROs\nmeasured in this study are unlikely to vary signiﬁcantly from day\nto day, this temporal resolution is appropriate for the application\nof PRO prediction. However, predicting more acute events might\nrequire more temporal resolution, which could be addressed by\nusing the activity tracker data at a ﬁner time scale. Long term\nfollow-up with patients including recordings of clinical events\nsuch as rehospitalizations could also allow us to evaluate the\neffect of mHealth monitoring on clinical outcome, an important\nstep in determining the efﬁcacy of such an intervention.\nVI. C ONCLUSION\nA temporal machine learning model can be used to classify\nself-reported physical health in patients with SIHD using phys-\niological indices measured by activity trackers. By constructing\nan HMM with feature selection and an RF classiﬁer, the result-\ning model can achieve an AUC of 0.79 for classifying Physical\nFunction. Our result indicates data generated from activity track-\ners may be used in a machine learning framework to classify\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n884 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nvalidated self-reported health status variables. These techniques\ncould play a future role in larger frameworks for remotely moni-\ntoring a patient’s health state in a clinically meaningful manner.\nREFERENCES\n[ 1 ] M .K .O n g et al. , “Effectiveness of remote patient monitoring after dis-\ncharge of hospitalized patients with heart failure the better effectiveness\nafter transition-heart failure (BEA T-HF) randomized clinical trial,” JAMA\nInternal Med. , vol. 176, no. 3, pp. 310–318, 2016.\n[2] R. J. Shaw et al. , “Mobile health devices: Will patients actually use\nthem?,” J .A m .M e d .I n f o r m .A s s o c ., vol. 23, no. 3', 'a; Social Isolation-Short Form 4a; and\nSleep Disturbance-Short Form 4a. Each questionnaire either\nasks about current health or has a recall period of the previous\nseven days, so they are appropriate for weekly administration.\nThe T metric method was used to standardize scores for each\ntype to a mean of 50 and a standard deviation of 10, with a\nrange between 0 and 100 [15], [36]. Symptom (i.e., Fatigue,\nAnxiety, Depression, Social Isolation, and Sleep Disturbance)\nscores of 60 or higher are one standard deviation above the av-\nerage, which is deﬁned as moderate to severe symptom severity.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\n880 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 24, NO. 3, MARCH 2020\nFig. 1. Distribution of normal and abnormal (moderate to severe) class\nfor each PRO measure.\nFor function (i.e., Global Physical Health, Global Mental\nHealth, and Physical Function), scores less than 40 are classi-\nﬁed as moderate to severe, meaning less functional ability than\nnormal. For this study, PRO scores were predicted in two ways:\nregression was used to predict PRO scores from patient activity\ntracker data, and classiﬁcation was used to determine whether\nsubjects’ PRO scores were above the threshold for at least mod-\nerate severity. The distributions of PRO scores are shown in\nFig. 1 . Because of a lack of moderate or severe cases for social\nisolation (<2%), this variable was eliminated for analysis in our\nmodel.\nIII. M ETHODS\nMissing data is a common concern when dealing with activity\ntracker data and can result from subjects either forgetting to wear\ntheir devices or removing them for charging. Patients were asked\nto ﬁll out eight PROMIS questionnaires at the end of each week\nfor a 12-week monitoring period. In total, 19.1 percent of weeks\nhad missing PRO data and 16.6 percent of weeks had missing\nvalues from the activity tracker in four or more days. If data was\navailable for at least four days in a week, missing values were\npermuted by using the average value of the rest of the week for\nsteps or resting heart rate. Weeks with missing survey scores, as\nwell as those without step and resting heart rate data for more\nthan three days, were removed from the analysis.\nA correlation analysis between subjects’ missing Fitbit data\nand their average Global Physical Health and Global Mental\nHealth scores shows a slight negative relationship ( −0.11 and\n−0.09, respectively) that was not statistically signiﬁcant (p =\n0.13 and p = 0.23, respectively). The correlation coefﬁcient be-\ntween number of missing PROs and the average global health\nscores are −0.17 (p = 0.018) to −0.14 (p = 0.048), respec-\ntively, indicating that the missing PROs are signiﬁcantly related\nto patient health. Another correlation analysis was performed\nbetween subject’s age and number of missing values with\nr2 <0.001, which demonstrates no trend of more missing values\nfor elder subjects. Finally, subjects with only one week of data\nwere eliminated in order to ensure the continuity of transition\nof states from week to week when building the HMM model.\nAfter adopting this data preprocessing approach and using the\nclassiﬁcation criteria above, a total number of 182 subjects with\na total of 1,640 weeks were collected, where the number of\nFig', ' experience of their own\nhealth and to provide valid and reliable data [13]–[15]. How-\never, response fatigue is a common problem that can result in\nmissing data due to an incomplete response from the subject,\nresulting in misclassiﬁcation [16], [17]. Response fatigue can be\ncommon when an administered survey is too long, or when a sur-\nvey is short, but administered too frequently. Because of these\ndrawbacks, data collected from a less invasive method could\npotentially provide more reliable estimates of patient health sta-\ntus over time. Previous studies have shown high compliance\nin activity trackers, indicating that they may be more reliable\nmethods for tracking continuous patient data [4], [18].\n2168-2194 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 13:07:47 UTC from IEEE Xplore.  Restrictions apply. \n\nMENG et al.: MACHINE LEARNING APPROACH TO CLASSIFYING SELF-REPORTED HEALTH STATUS 879\nIn this study, we explore the use of machine learning methods\nto classify PRO scores over time [14]. Machine learning algo-\nrithms have been widely used in biomedical research for tasks\nsuch as disease detection [19] and outcome prediction [20].\nThese methods traditionally use a set of demographic variables\nand baseline data as a feature vector to make a classiﬁcation\nusing a machine learning algorithm such as gradient boosting\nregression tree (GBRT) [21], AdaBoost [22], or random forests\n(RF) [20], [23]. However, traditional machine learning meth-\nods are effective for making single decisions, but do not allow\nfor adjusting as more information is learned. Temporal mod-\nels are appropriate in the case of sequential observations where\nthe value of the outcome may need to be adjusted over time. In\nparticular, hidden Markov models (HMMs) are well-established\ntemporal models that use sequential data to predict events such\nas patient state changes, such as estimating mean sojourn time\nof lung cancer patients using screening images [24], detecting\nhomologous protein sequences [25], and gene ﬁnding [26].\nThe goal of this study was to investigate the feasibility of using\nmachine learning models to classify PRO scores based on data\ncollected using one type of activity tracker, the Fitbit Charge 2.\nIn this study, we tested this goal within a population of patients\nwith stable ischemic heart disease (SIHD). The rest of this article\ndescribes an approach for data preprocessing and constructing\na model that treats weeks independently, as well as an HMM\nthat takes temporal information into account. Performance of\nthe classiﬁcation algorithms is then evaluated for each PRO\nmeasure and feature importance in classiﬁcation is analyzed.\nFinally, we provide a discussion and analysis of the results and\nsuggest future directions for implementing such a classiﬁer in a\npatient surveillance application.\nII. D ATADESCRIPTION\nA set of 200 patients with SIHD were recruited for a feasibil-\nity study conducted by Cedars-Sinai Medical Center from 2017\nto 2018 to predict surrogate markers of major adverse cardiac\nevents (MACE), including myocardial infarction, arrhythmia,\nand hospitalization due to heart failure, using biometrics, wear-\nable sensors, patient-reported surveys, and other biochemical\nmarkers. This study population size is similar to several pre-\nvious studies that used activity trackers for patient monitoring\n[27], [28]. The desired monitoring period was 12 weeks for\neach subject, during which time subjects wore personal activity\ntrackers to record their physiological indices']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2560.0,free_text
A_Machine_Learning_Approach_to_Classifying_Self_Reported_Health_Status_in_a_Cohort_of_Patients_With_Heart_Disease_Using_Activity_Tracker_Data,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[14, 12, 13]","['L. Morey, “Physical activity intervention for women,” Am. J. Preventive\nMed., vol. 49, no. 3, pp. 414–418, 2015.\n[28] J. B. Wang et al. , “Wearable sensor/device (ﬁtbit one) and SMS text-\nmessaging prompts to increase physical activity in overweight and obese\nadults: A randomized controlled trial,” T elemedicine e-Health, vol. 21, no.\n10, pp. 18–23, 2014.\n[29] S. Benedetto, C. Caldato, E. Bazzan, D. C. Greenwood, V . Pensabene,\nand P . Actis, “Assessment of the ﬁtbit charge 2 for monitoring heart rate,”\nPLoS One , vol. 13, no. 2, 2018, Art. no. e0192691.\n[30] M. A. Tully, C. McBride, L. Heron, and R. F. Hunter, “The validation of\nFibit ZipTM physical activity monitor as a measure of free-living physical\nactivity,” BMC Res Notes , vol. 7, 2014, Art. no. 952.\n[31] K. M. Diaz et al. , “Fitbit: An accurate and reliable device for wireless\nphysical activity tracking,” Int. J. Cardiol. , vol. 185, pp. 138–140, 2015.\n[32] R. K. Reddy et al. , “Accuracy of wrist-worn activity monitors during com-\nmon daily physical activities and types of structured exercise: Evaluation\nstudy,” JMIR Mhealth Uhealth , vol. 6, 2018, Art. no. e10338.\n[33] E. Jo, K. Lewis, D. Directo, M. J. Kim, and B. A. Dolezal, “V alidation of\nbiofeedback wearables for photoplethysmographic heart rate tracking,” J.\nSports Sci. Med. , vol. 15, pp. 540–547, 2016.\n[34] A. Ghasemi and S. Zahediasl, “Normality tests for statistical analysis:\nA guide for non-statisticians,” Int. J. Endocrinol. Metab. , vol. 10, no. 2,\npp. 486–489, 2012.\n[35] R. D. Hays, J. B. Bjorner, D. A. Revicki, K. L. Spritzer, and D. Cella,\n“Development of physical and mental health summary scores from the\npatient-reported outcomes measurement information system (PROMIS)\nglobal items,” Qual. Life Res. , vol. 18, no. 7, pp. 873–880, 2009.\n[36] B. M. R. Spiegel et al. , “Development of the NIH patient-reported\noutcomes measurement information system (PROMIS) gastrointestinal\nsymptom scales,” Am. J. Gastroenterol. , vol. 109, no. 11, pp. 1804–1814,\n2014.\n[37] J. O. Ogutu, H. P . Piepho, and T. Schulz-Streeck, “A comparison of random\nforests, boosting and support vector machines for genomic selection,”\nBMC Proc. , vol. 5, no. Suppl. 3, pp. 3–7, 2011.\n[38] R. J. Petrella, J. J. Koval, and D. A. Cunningham, “A self-paced step test\nto predict aerobic ﬁtness in older adults in the primary', ' activity a\nsystematic review,” Clin. Corner , vol. 298, no. 19, pp. 2296–2304, 2014.\n[10] S. L. Shuger et al. , “Electronic feedback in a diet- and physical activity-\nbased lifestyle intervention for weight loss: A randomized controlled trial,”\nInt. J. Behav . Nutrition Phys. Activity , vol. 8, no. 1, May 2011, Art. no. 41.\n[11] S. Zan, S. Agboola, S. A. Moore, K. A. Parks, J. C. Kvedar, and K.\nJethwani, “Patient engagement with a mobile web-based telemonitoring\nsystem for heart failure self-management: A pilot study,” JMIR mHealth\nuHealth, vol. 3, no. 2, Apr. 2015, Art. no. e33.\n[12] T. M. Hale, K. Jethwani, M. S. Kandola, F. Saldana, and J. C. Kvedar, “A\nremote medication monitoring system for chronic heart failure patients to\nreduce readmissions: A two-arm randomized pilot study,” J. Med. Internet\nRes., vol. 18, no. 5, Apr. 2016, Art. no. e91.\n[13] P . A. Pilkonis, L. Y u, N. E. Dodds, K. L. Johnston, C. C. Maihoefer,\nand S. M. Lawrence, “V alidation of the depression item bank from the\npatient-reported outcomes measurement information system (PROMIS)\nin a three-month observational study,” J. Psychiatric Res. , vol. 56, no. 1,\npp. 112–119, 2014.\n[14] D. Cella et al. , “Initial adult health item banks and ﬁrst wave testing of the\npatient-reported outcomes measurement information system (PROMIS)\nnetwork: 2005-2008,” J. Clin. Epidemiol. , vol. 63, no. 11, pp. 1179–1194,\n2011.\n[15] H. Liu et al. , “Representativeness of the patient-reported outcomes mea-\nsurement information system internet panel,” J. Clin. Epidemiol. , vol. 63,\nno. 11, pp. 1169–1178, 2010.\n[16] B. L. Egleston, S. M. Miller, and N. J. Meropol, “The impact of misclas-\nsiﬁcation due to survey response fatigue on estimation and identiﬁability\nof treatment effects,” Statist. Med. , vol. 30, no. 30, pp. 3560–3572, 2011.\n[17] S. R. Porter, M. E. Whitcomb, and W. H. Weitzer, “Multiple surveys\nof students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen H', ' students and survey fatigue,” New Dir . Inst. Res. , vol. 2004, no. 121,\npp. 63–73, 2004.\n[18] S. Hermsen, J. Moons, P . Kerkhof, C. Wiekens, and M. De Groot, “De-\nterminants for sustained use of an activity tracker: Observational study,”\nJMIR mHealth uHealth , vol. 5, no. 10, 2017, Art. no. e164.\n[19] R. C. Deo, “Machine learning in medicine,” Circulation, vol. 132, no. 20,\npp. 1920–1930, 2015.\n[20] M. Eileen Hsich et al. , “Identifying important risk factors for survival\nin systolic heart failure patients using random survival forests,” Circ.\nCardiovascular Qual. Outcomes , vol. 4, no. 1, pp. 39–45, 2014.\n[21] N. Limsopatham, C. Macdonald, and I. Ounis, “Learning to combine\nrepresentations for medical records search,” in Proc. 36th Int. ACM SIGIR\nConf. Res. Develop. Inf. Retrieval , 2013, pp. 833–836.\n[22] J. H. Morra, Zhuowen Tu, L. G. Apostolova, A. E. Green, A. W. Toga, and\nP . M. Thompson, “Comparison of AdaBoost and support vector machines\nfor detecting alzheimer’s disease through automated hippocampal seg-\nmentation,” IEEE Trans. Med. Imag. , vol. 29, no. 1, pp. 30–43, Jan. 2010.\n[23] H. Ishwaran, U. B. Kogalur, E. H. Blackstone, and M. S. Lauer, “Random\nsurvival forests,” Ann. Appl. Statist. , vol. 2, no. 3, pp. 841–860, 2008.\n[24] S. Shen et al. , “A Bayesian model for estimating multi-state disease pro-\ngression,” Comput. Biol. Med. , vol. 81, pp. 111–120, 2017.\n[25] B. Schuster-B ¨ockler and A. Bateman, “An introduction to hidden Markov\nmodels,” Current Protocols Bioinf. , vol. 18, pp. A.3A.1–A.3A.9, 2007.\n[26] E. Birney Clamp and R. M. Durbin, “Genewise and genomewise,” Genome\nRes., vol. 14, no. 4, pp. 988–995, 2004.\n[27] L. A. Cadmus-bertram, B. H. Marcus, R. E. Patterson, B. A. Parker, and B.\nL. Morey, “Physical activity intervention for women,” Am. J. Preventive\nMed., vol. 49, no. 3, pp. 414–418, 2015.\n[28] J. B. Wang et al. , “Wearable sensor/device (ﬁtbit one) and SMS text-\nmessaging prompts to increase physical activity in overweight and obese\nadults: A randomized controlled trial,” T elemedicine e-Health, vol. 21, no.\n10, pp. 18–23, 2014.\n[29] S. Benedetto, C. Caldato, E. Bazzan, D. C. Greenwood, V . Pensabene,\nand P . Actis, “Assessment of the ﬁtbit']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2542.0,free_text
1_s2.0_S2666693623000737_main,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,diagnostic,,,,"[5, 0, 3]","[' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', 'Data-efﬁcient machine learning methods in the\nME-TIME study: Rationale and design of a longitudinal\nstudy to detect atrialﬁbrillation and heart failure from\nwearables\nArman Naseri, MSc,*† David Tax, PhD,† Pim van der Harst, MD, PhD,‡\nMarcel Reinders, PhD,† Ivo van der Bilt, MD, PhD*‡\nFrom the*Department of Cardiology, Haga Teaching Hospital, The Hague, The Netherlands,†Pattern\nRecognition and Bioinformatics, Delft University of Technology, Delft, The Netherlands, and\n‡Department of Cardiology, University Medical Center Utrecht, Utrecht, The Netherlands.\nBACKGROUND Smartwatches enable continuous and noninvasive\ntime series monitoring of cardiovascular biomarkers like heart\nrate (from photoplethysmograms), step counter, skin temperature,\net cetera; as such, they have promise in assisting in early detection\nand prevention of cardiovascular disease. Although these bio-\nmarkers may not be directly useful to physicians, a machine learning\n(ML) model could ﬁnd clinically relevant patterns. Unfortunately,\nML models typically need supervised (ie, annotated) data, and la-\nbeling of large amounts of continuous data is very labor intensive.\nTherefore, ML methods that are data efﬁcient, ie, needing a low\nnumber of labels, are required to detect potential clinical value in\npatterns found in wearable data.\nOBJECTIVE The primary study objective of the ME-TIME (Machine\nLearning Enabled Time Series Analysis in Medicine) study is to\ndesign an ML model that can detect atrialﬁbrillation (AF) and heart\nfailure (HF) from wearable data in a data-ef ﬁcient manner. To\nachieve this, self-supervised and weakly supervised learning tech-\nniques are used.\nMETHODS Two hundred subjects (100 reference, 50 AF, and 50 HF)\nare being invited to participate in wearing a Fitbitﬁtness tracker for\n3 months. Interested volunteers are sent a questionnaire to deter-\nmine their health, in particular cardiovascular health. Volunteers\nwithout any (history of) serious illness are assigned to the reference\ngroup. Participants with AF and HF are recruited in the Haga teach-\ning hospital in The Hague, The Netherlands.\nRESULTS Enrollment commenced on May 1, 2022, and as of the\ntime of this report, 62 subjects have been included in the study. Pre-\nliminary analysis of the data reveals signiﬁcant inter-subject vari-\nability. Notably, we identi ﬁed heart rate recovery curves and\ntime-delayed correlations between heart rate and step count as po-\ntential strong indicators for heart disease.\nCONCLUSION Using self-supervised and multiple-instance\nlearning techniques, we hypothesize that patterns speciﬁct oA F\nand HF can be found in continuous data obtained from smart-\nwatches.\nKEYWORDS Wearables; mHealth; Atrial ﬁbrillation; Heart failure;\nSmartwatch; Arti ﬁcial intelligence; Machine learning; Multiple-\ninstance learning; Self-supervised learning\n(Cardiovascular Digital Health Journal 2023;4:165–172)\n© 2023\nHeart Rhythm Society. This is an open access article under the CC\nBY license (http://creativecommons.org/licenses/by/4.0/).\nIntroduction\nCardiovascular disease is one of the leading causes of mortal-\nity globally1 and cardiovascular healthcare accounts for a\nlarge portion of global healthcare costs and expenses. Early\ndetection and prevention will decrease the burden of cardio-\nvascular disease and will therefore decrease mortality,\nmorbidity, and costs. Cardiovascular monitoring using big\ndata from wearables and machine learning can drastically in-\ncrease the availability and ef ﬁciency of cardiovascular\nhealthcare globally, at the fraction of the costs of conven-\ntional medical-grade devices.\nElectrocardi', '\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,free_text
1_s2.0_S2666693623000737_main,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",Unknown from this paper.,,,,"[4, 7, 5]","[' of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains', ', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,free_text
1_s2.0_S2666693623000737_main,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Unknown from this paper.,,,,"[4, 5, 7]","[' of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', ', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2549.0,free_text
1_s2.0_S2666693623000737_main,Q4,How was the dataset described in this study before predictive modeling was performed?,Data from 22 subjects with large inter-subject variability.,,,,"[7, 5, 6]","[', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', ' and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2550.0,free_text
1_s2.0_S2666693623000737_main,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?","20% external test set, internal test from final 20% data.",,,,"[5, 6, 7]","[' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', ' and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation', ', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2561.0,free_text
1_s2.0_S2666693623000737_main,Q6,What preprocessing techniques on the included variables/features were applied in this study?,Unknown from this paper.,,,,"[7, 5, 9]","[', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', '-correlation function\nbetween the heart rate window and its corresponding step\ncounter window is indicative of heart disease. To calculate\nthe correlation, we consider varying window sizes and time\ndifferences (lags) between heart rate and steps. The computed\ncross-correlation matrix for the healthy group, along with the\nAF and HF patient groups, as shown inFigure 7, shows that\nthe heart rate is correlated with the step counter with 1-minute\ndelay.\nDiscussion\nBy building a suitable infrastructure with Cloud technology,\nbig data acquired in the study is used to develop data-efﬁcient\nmodels using methods from multiple instance and self-\nsupervised learning.\nWe aim to examine the inﬂuence of inter-subject vari-\nability on predicting cardiovascular disease and will explore\npotential methods to mitigate these variabilities.\n13,14 We\nexpect that patterns indicative of cardiovascular disease\nbecome apparent within a timeframe of minutes, hours, or\nmore, considering that consumer-grade wearables have a\nslower sampling rate compared to the gold standard. We\nhave shown 1 example of such a pattern: the acceleration-\ndeceleration curve. Preselecting windows based on such\nTable 2 Confusion matrix of per-week healthy vs atrialﬁbrillation\nclassiﬁcation of the MILES model with peak aligned curves\nconcatenated with step counter data, with true and predicted labels\nshown vertically and horizontally, respectively\nTrue/Predicted label AF Healthy\nAF 11 14\nHealthy 7 33\nAF 5 atrial ﬁbrillation.\nFigure 7 Cross-correlation matrices between windowed heart rate data and\nnumber of steps for healthy and the persistent atrialﬁbrillation (AF) group.\nRows are window sizes and columns lag between heart rate and step counter.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 171\n\npatterns furthermore mitigates searching through substantial\namounts of data that may not provide much information\nabout cardiovascular disease. Inspecting the cross-\ncorrelation for several combinations of window size and\nlag show a different proﬁle for healthy and AF group individ-\nuals, showing that it is meaningful to analyze step counter\nand heart rate together. Big data for heart disease detection\nrequires substantial labeling efforts from physicians.\nUsing self-supervised learning and MIL, a model can be\ntrained with much fewer labels. Ourﬁndings demonstrate\nthis by employing MILES to achieve high speciﬁcity, which\ncan aid in ruling out heart disease in individuals experiencing\nsymptoms similar to heart disease but without the condition\n(ie, false-positives).\nConclusion\nThe ongoing ME-TIME study is a longitudinal observational\nstudy that uses machine learning with time series data from\nconsumer-grade smartwatches to detect atrial ﬁbrillation\nand heart failure. This will contribute to cost-effective cardio-\nvascular monitoring of outpatients, thereby reducing exacer-\nbation of cardiovascular disease and effectively increasing\ncapacity of global cardiovascular healthcare.\nFunding Sources\nThis research did not receive any speciﬁc grant from funding\nagencies in the public, commercial, or not-for-proﬁt sectors.\nDisclosures\nThe authors declare no conﬂict of interest.\nAuthorship\nAll authors attest they meet the current ICMJE criteria for\nauthorship.\nPatient Consent\nInformed consent was obtained from all subjects involved in\nthe study.\nInstitutional Review Board Statement\nThe study was conducted in accordance with the Declaration\nof Helsinki, and approved by the Institutional Review Board\n(or Ethics Committee) of METC-LDD (protocol code\nNL73708.058.20) for studies involving humans.\nReferences\n1. Roth GA, Mensah GA, Johnson CO, et al. Global burden of cardiovascular dis-\neases and risk factors, 1990–2019: update from the GBD 2019 study. J']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2538.0,free_text
1_s2.0_S2666693623000737_main,Q7,How is missing data handled in this study?,"Linear interpolation for short gaps, exclusion for longer gaps.",,,,"[4, 5, 7]","[' of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', ', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2547.0,free_text
1_s2.0_S2666693623000737_main,Q8,How are outliers handled in this study?,"Artifacts removed, linear interpolation applied.",,,,"[4, 6, 7]","[' of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains', ' and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation', ', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2544.0,free_text
1_s2.0_S2666693623000737_main,Q9,Which prediction models were used in this study?,Multiple-instance learning models.,,,,"[5, 0, 6]","[' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', 'Data-efﬁcient machine learning methods in the\nME-TIME study: Rationale and design of a longitudinal\nstudy to detect atrialﬁbrillation and heart failure from\nwearables\nArman Naseri, MSc,*† David Tax, PhD,† Pim van der Harst, MD, PhD,‡\nMarcel Reinders, PhD,† Ivo van der Bilt, MD, PhD*‡\nFrom the*Department of Cardiology, Haga Teaching Hospital, The Hague, The Netherlands,†Pattern\nRecognition and Bioinformatics, Delft University of Technology, Delft, The Netherlands, and\n‡Department of Cardiology, University Medical Center Utrecht, Utrecht, The Netherlands.\nBACKGROUND Smartwatches enable continuous and noninvasive\ntime series monitoring of cardiovascular biomarkers like heart\nrate (from photoplethysmograms), step counter, skin temperature,\net cetera; as such, they have promise in assisting in early detection\nand prevention of cardiovascular disease. Although these bio-\nmarkers may not be directly useful to physicians, a machine learning\n(ML) model could ﬁnd clinically relevant patterns. Unfortunately,\nML models typically need supervised (ie, annotated) data, and la-\nbeling of large amounts of continuous data is very labor intensive.\nTherefore, ML methods that are data efﬁcient, ie, needing a low\nnumber of labels, are required to detect potential clinical value in\npatterns found in wearable data.\nOBJECTIVE The primary study objective of the ME-TIME (Machine\nLearning Enabled Time Series Analysis in Medicine) study is to\ndesign an ML model that can detect atrialﬁbrillation (AF) and heart\nfailure (HF) from wearable data in a data-ef ﬁcient manner. To\nachieve this, self-supervised and weakly supervised learning tech-\nniques are used.\nMETHODS Two hundred subjects (100 reference, 50 AF, and 50 HF)\nare being invited to participate in wearing a Fitbitﬁtness tracker for\n3 months. Interested volunteers are sent a questionnaire to deter-\nmine their health, in particular cardiovascular health. Volunteers\nwithout any (history of) serious illness are assigned to the reference\ngroup. Participants with AF and HF are recruited in the Haga teach-\ning hospital in The Hague, The Netherlands.\nRESULTS Enrollment commenced on May 1, 2022, and as of the\ntime of this report, 62 subjects have been included in the study. Pre-\nliminary analysis of the data reveals signiﬁcant inter-subject vari-\nability. Notably, we identi ﬁed heart rate recovery curves and\ntime-delayed correlations between heart rate and step count as po-\ntential strong indicators for heart disease.\nCONCLUSION Using self-supervised and multiple-instance\nlearning techniques, we hypothesize that patterns speciﬁct oA F\nand HF can be found in continuous data obtained from smart-\nwatches.\nKEYWORDS Wearables; mHealth; Atrial ﬁbrillation; Heart failure;\nSmartwatch; Arti ﬁcial intelligence; Machine learning; Multiple-\ninstance learning; Self-supervised learning\n(Cardiovascular Digital Health Journal 2023;4:165–172)\n© 2023\nHeart Rhythm Society. This is an open access article under the CC\nBY license (http://creativecommons.org/licenses/by/4.0/).\nIntroduction\nCardiovascular disease is one of the leading causes of mortal-\nity globally1 and cardiovascular healthcare accounts for a\nlarge portion of global healthcare costs and expenses. Early\ndetection and prevention will decrease the burden of cardio-\nvascular disease and will therefore decrease mortality,\nmorbidity, and costs. Cardiovascular monitoring using big\ndata from wearables and machine learning can drastically in-\ncrease the availability and ef ﬁciency of cardiovascular\nhealthcare globally, at the fraction of the costs of conven-\ntional medical-grade devices.\nElectrocardi', ' and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2535.0,free_text
1_s2.0_S2666693623000737_main,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,Hyperparameter tuning for threshold determination.,,,,"[7, 5, 8]","[', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', ' a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\ndisease (n515), respectively. The mean and standard deviation are shown per time point (5 second intervals) for both groups.\n170 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\ntesting, a subject can signiﬁcantly deviate from the subjects\non which the model was trained.\nHeart rate peak alignment in acceleration-\ndeceleration curves indicate difference between 7\nAF patients and 15 healthy controls\nNext, we explored the heart rate recovery curves after activity\n(acceleration-deceleration curves).12 First a peak is detected,\nwhereafter the start (onset) and end (recovery) points associ-\nated to that peak are determined by the minimum heart rate\nvalue 5 minutes before and 15 minutes after the peak. The\ncurves are preprocessed by aligning the peaks on the time\naxis. Additionally, for every subject, the amplitude of the\ncurves is rescaled by the average peak value across all curves\nfor that individual.Figure 6shows the curves for light activ-\nity, deﬁned by a maximum of 20 steps in the 5 minutes pre-\nceding the peak and fewer than 10 steps in the 15 minutes\nafter the peak. There are 2 noticeable differences in heart\nrate patterns between persistent AF patients (in red) and\nhealthy participants (in blue). The standard deviation for\nAF patients is considerably smaller than that of healthy indi-\nviduals, and their heart rate recovery is slower, as observed at\nthe 6-minute mark. These distinctions could potentially serve\nas clinical indicators for atrialﬁbrillation.\nMIL can detect healthy cardiovascular outcomes\nThe peak aligned acceleration-deceleration curves are\nconcatenated with their corresponding step counter data\nand grouped per week to form bags. The MILES (Multiple-\nInstance Learning via Embedded Instance Selection) model\nis then used to classify every week as healthy or AF. The re-\nsults inTable 2show that even though the sensitivity is low,\nthe speciﬁcity is decent. This shows potential in avoiding un-\nnecessary visits to a cardiologist for patients who have symp-\ntoms that are wrongly suspected to be related to heart\nproblems.\nStep counter and heart rate are correlated with a\ntime delay\nNext, we examined whether the cross-correlation function\nbetween the heart rate window and its corresponding step\ncounter window is indicative of heart disease. To calculate\nthe correlation, we consider varying window sizes and time\ndifferences (lags) between heart rate and steps. The computed\ncross-correlation matrix for the healthy group, along with the\nAF and HF patient groups, as shown inFigure 7, shows that\nthe heart rate is correlated with the step counter with 1-minute\ndelay.\nDiscussion\nBy building a suitable infrastructure with Cloud technology,\nbig data acquired in the study is used to develop data-efﬁcient\nmodels using methods from multiple instance and self-\nsupervised learning.\nWe aim to examine the inﬂuence of inter-subject vari-\nability on predicting cardiovascular disease and will explore\n']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2540.0,free_text
1_s2.0_S2666693623000737_main,Q11,How was data augmentation or generation used in this study?,Unknown from this paper.,,,,"[3, 7, 5]","['\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are', ', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2549.0,free_text
1_s2.0_S2666693623000737_main,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared to baseline.",,,,"[6, 5, 7]","[' and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', ', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2545.0,free_text
1_s2.0_S2666693623000737_main,Q13,Which type of explainability techniques are used?,Unknown from this paper.,,,,"[4, 8, 2]","[' of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are removed, if more than 12 consecutive\nconstant values (equivalent to 1 minute of heart rate samples)\nare detected. This 1-minute threshold was chosen based on\nvisual inspection, which revealed that heart rate patterns typi-\ncally occur in the order of minutes, often spanning 10–20 mi-\nnutes. For sequences with fewer than 12 consecutive missing\nvalues, linear interpolation is applied. From the cleaned time\nseries, smaller segments, denoted as windows, are extracted\nand employed as input for a machine learning model. This\nprocess involves a sliding window and windows containing\ntime gaps are excluded. Windows have 2 design consider-\nations: the window size, which determines the number of\nsamples within a window and deﬁnes its dimensionality,\nand the stride, which establishes the step size dictating the\nshift between windows.\nAlthough the cardiovascular condition of each subject is\nknown, it is unknown in which speciﬁc windows these con-\nditions manifest themselves. This is owing to the paroxysmal\nnature of atrialﬁbrillation and the variable symptoms of heart\nfailure, which can be inﬂuenced by factors like medication\nadjustments, dietary changes, and the disease’s progressive\ncourse. In other words, the subject label is known, but the in-\ndividual window labels are unknown. This is visually repre-\nsented inFigure 2a, where the subject label is depicted by the\nblue/red colors and the unknown window labels are indicated\nby black dotted lines.\nPlanned machine learning approach\nOur planned machine learning approach is tailored to operate\nin this setting through a 2-stage process. To learn informative\npatterns/features directly from the input data, despite the lack\nof labeled windows, the ﬁrst stage involves using self-\nsupervised learning. A commonly used self-supervised\nlearning technique involves compressing the input windows\nto a lower-dimensional representation and then reconstruct-\ning the original input from this compact representation, as de-\npicted in Figure 2b.\n7,8 Instead of reconstruction, another\ntechnique is to forecast future time points of the input\ndata.9 The second stage involves multiple-instance learning\n(MIL). MIL, depicted inFigure 3and more elaborately ex-\nplained inBox 1, is suitable for data where a single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains', ' a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\ndisease (n515), respectively. The mean and standard deviation are shown per time point (5 second intervals) for both groups.\n170 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\ntesting, a subject can signiﬁcantly deviate from the subjects\non which the model was trained.\nHeart rate peak alignment in acceleration-\ndeceleration curves indicate difference between 7\nAF patients and 15 healthy controls\nNext, we explored the heart rate recovery curves after activity\n(acceleration-deceleration curves).12 First a peak is detected,\nwhereafter the start (onset) and end (recovery) points associ-\nated to that peak are determined by the minimum heart rate\nvalue 5 minutes before and 15 minutes after the peak. The\ncurves are preprocessed by aligning the peaks on the time\naxis. Additionally, for every subject, the amplitude of the\ncurves is rescaled by the average peak value across all curves\nfor that individual.Figure 6shows the curves for light activ-\nity, deﬁned by a maximum of 20 steps in the 5 minutes pre-\nceding the peak and fewer than 10 steps in the 15 minutes\nafter the peak. There are 2 noticeable differences in heart\nrate patterns between persistent AF patients (in red) and\nhealthy participants (in blue). The standard deviation for\nAF patients is considerably smaller than that of healthy indi-\nviduals, and their heart rate recovery is slower, as observed at\nthe 6-minute mark. These distinctions could potentially serve\nas clinical indicators for atrialﬁbrillation.\nMIL can detect healthy cardiovascular outcomes\nThe peak aligned acceleration-deceleration curves are\nconcatenated with their corresponding step counter data\nand grouped per week to form bags. The MILES (Multiple-\nInstance Learning via Embedded Instance Selection) model\nis then used to classify every week as healthy or AF. The re-\nsults inTable 2show that even though the sensitivity is low,\nthe speciﬁcity is decent. This shows potential in avoiding un-\nnecessary visits to a cardiologist for patients who have symp-\ntoms that are wrongly suspected to be related to heart\nproblems.\nStep counter and heart rate are correlated with a\ntime delay\nNext, we examined whether the cross-correlation function\nbetween the heart rate window and its corresponding step\ncounter window is indicative of heart disease. To calculate\nthe correlation, we consider varying window sizes and time\ndifferences (lags) between heart rate and steps. The computed\ncross-correlation matrix for the healthy group, along with the\nAF and HF patient groups, as shown inFigure 7, shows that\nthe heart rate is correlated with the step counter with 1-minute\ndelay.\nDiscussion\nBy building a suitable infrastructure with Cloud technology,\nbig data acquired in the study is used to develop data-efﬁcient\nmodels using methods from multiple instance and self-\nsupervised learning.\nWe aim to examine the inﬂuence of inter-subject vari-\nability on predicting cardiovascular disease and will explore\n', ' learning is used, where each observation of the data has\na class label. This must be done with ECGs, since photople-\nthysmography or derived signals are difﬁcult to interpret by a\nclinician. This so-called labeling or annotating of signals by\nphysicians is infeasible for the large amounts of (continuous)\ndata required, and therefore semi-automated\n4,5 and fully\nautomated2,3 ECG labeling systems6 have been developed.\nHowever, these still require a lot of manual labor from contin-\nuously monitored users.\nTherefore, the objective of the ME-TIME is (early) detec-\ntion and prevention of heart disease by leveraging time series\ndata from smartwatches, a cloud-based infrastructure, and\nmachine learning algorithms speciﬁcally designed to func-\ntion effectively with minimal labeling efforts.\nMethods\nStudy design and data collection\nME-TIME (registered at ClinicalTrials.gov; ID:\nNCT05802563) is designed as an observational cohort study\nconsisting of 3 data subject groups, as depicted inFigure 1.\nThe ﬁrst group consists of patients with systolic heart failure\n(HF group); the second group consists of patients with docu-\nmented atrial ﬁbrillation (AF group); and the third group,\nserving as a reference, consists of healthy volunteers. The\nrationale for creating distinct AF and HF groups comes\nfrom their unique pathophysiological characteristics. Conse-\nquently, heart rate patterns that are indicative of these dis-\neases might also be different. The HF group consists of 50\nstudy participants with systolic heart failure, deﬁned as a\nleft ventricular ejection fraction,35% without documented\natrial ﬁbrillation. The AF group consists of 50 patients with\ndocumented atrialﬁbrillation (paroxysmal, persistent, or per-\nmanent) without systolic heart failure. Ejection fractions will\nbe assessed from echocardiograms that are made within 1\nyear of inclusion, and if this is not available an echocardio-\ngram will be performed. The reference group consists of\n100 participants without any prior medical history and\nwithout medication use. Potential study subjects that meet\n2 31\nRef AF HF\n4 5\nFigure 1 Data analysis pipeline for the ME-TIME study. Included participants (image 1) are given a smartwatch (image 2), which is connected to our data\nacquisition and storage platform (image 3). The resulting data are then preprocessed (image 4) and put into the data-efﬁcient machine learning model (image\n5). AF5 atrial ﬁbrillation group; HF5 heart failure group; Ref5 reference group.\nAB C\nFigure 2 Pipeline for the study’s proposed approach.a: Segmentation of the time series of each subject (1 healthy and 2 atrialﬁbrillation) using a sliding win-\ndow. Only the label of the entire subject is available, instead of each individual window.b: The windows are inputs to an autoencoder and are compressed to a\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\n']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2523.0,free_text
1_s2.0_S2666693623000737_main,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,Unknown from this paper.,,,,"[5, 3, 6]","[' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if', '\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are', ' and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if\nnone of the subject’s individual samples are associated\nwith heart disease, indicating that the subject is not\naffected by it. Conversely, a bag label is positive if a\ncertain amount of the subject’s samples is linked to heart\ndisease.\nLearning The algorithm then learns a model based on\nthe bag-level labels only. The goal of the MIL algorithm is\nto learn a model that can correctly predict the bag-level\nlabels given the instances in each bag. By training on\nthe bag-level labels, the MIL algorithm can capture pat-\nterns and relationships within the data that help identify\nthe presence or absence of heart disease. Note that the\ndecision boundary produced by the model inFigure 3is\nnot ideally suited for classifying individual windows,\nwhich is expected, as it did not use this information.\nHowever, if a sufﬁcient number of windows are classiﬁed\naccurately, the correct bag label can still be predicted.\nThis is accomplished during training by aggregating these\naccurate classi ﬁcations using methods like majority\nvoting, or by setting a threshold for the minimum number\nof positively predicted windows needed to assign a pos-\nitive bag label.\nPrediction Once the model is trained, it can predict the\nlabel of a new bag by examining the instances in the bag.\nIf the model predicts that a certain percentage of instances\nin the bag are positive deﬁned by the threshold, the bag is\nclassiﬁed as positive (heart disease) and negative\n(healthy) otherwise. By analyzing the presence or absence\nof positive instances within the bag, the MIL algorithm\ncan make predictions on a bag level, providing insights\ninto the subject’s condition.\n168 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nsubjects previously encountered by the model. This segment\nof data was excluded during the cross-validation phase and\nserves as a baseline, as it minimizes the inﬂuence of inter-\nsubject variability, providing a reliable reference for compar-\nison.\nParameters not directly learned by the machine learning\nmodel, such as window parameters, are termed hyperpara-\nmeters. Since optimal values are typically unknown in\nadvance, multiple options are examined during LPSOCV,\nand the best-performing one, with the best average perfor-\nmance over all folds, is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2554.0,free_text
1_s2.0_S2666693623000737_main,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,Unknown from this paper.,,,,"[7, 3, 5]","[', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd', '\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2553.0,free_text
1_s2.0_S2666693623000737_main,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?",Leave-p-subjects-out cross-validation with stratification.,,,,"[10, 3, 5]","[' commercial, or not-for-proﬁt sectors.\nDisclosures\nThe authors declare no conﬂict of interest.\nAuthorship\nAll authors attest they meet the current ICMJE criteria for\nauthorship.\nPatient Consent\nInformed consent was obtained from all subjects involved in\nthe study.\nInstitutional Review Board Statement\nThe study was conducted in accordance with the Declaration\nof Helsinki, and approved by the Institutional Review Board\n(or Ethics Committee) of METC-LDD (protocol code\nNL73708.058.20) for studies involving humans.\nReferences\n1. Roth GA, Mensah GA, Johnson CO, et al. Global burden of cardiovascular dis-\neases and risk factors, 1990–2019: update from the GBD 2019 study. J Am\nColl Cardiol 2020;76:2982–3021.\n2. Tison GH, Sanchez JM, Ballinger B, et al. Passive detection of atrialﬁbrillation\nusing a commercially available smartwatch. JAMA Cardiol 2018;3:409–416.\n3. Wasserlauf J, You C, Patel R, Valys A, Albert D, Passman R. Smartwatch perfor-\nmance for the detection and quantiﬁcation of atrialﬁbrillation. Circ Arrhythm\nElectrophysiol 2019;12:e006834.\n4. Zhu L, Nathan V, Kuang J, et al. Atrialﬁbrillation detection and atrialﬁbrillation\nburden estimation via wearables. IEEE J Biomed Health Inform 2021;\n26:2063–2074.\n5. Lubitz SA, Faranesh AZ, Selvaggi C, et al. Detection of atrialﬁbrillation in a large\npopulation using wearable devices: the Fitbit heart study. Circulation 2022;\n146:1415–1424.\n6. Hall A, Mitchell ARJ, Wood L, Holland C. Effectiveness of a single lead Alive-\nCor electrocardiogram application for the screening of atrialﬁbrillation: a system-\natic review. Medicine (Baltimore) 2020;99:e21388.\n7. Torres-Soto J, Ashley EA. Multi-task deep learning for cardiac rhythm detection\nin wearable devices. NPJ Digit Med 2020;3:116.\n8. Mienye ID, Sun Y, Wang Z. Improved sparse autoencoder based artiﬁcial neural\nnetwork approach for prediction of heart disease. Informatics in Medicine Un-\nlocked 2020;18:100307.\n9. Spathis D, Perez-Pozuelo I, Brage S, Wareham NJ, Mascolo C. Self-supervised\ntransfer learning of physiological representations from free-living wearable\ndata. In: Proceedings of the Conference on Health. Inference, and Learning;\n2021. p. 69–78.\n10. Quer G, Gouda P, Galarnyk M, Topol EJ, Steinhubl SR. Inter-and intraindividual\nvariability in daily resting heart rate and its associations with age, sex, sleep, BMI,\nand time of year: retrospective, longitudinal cohort study of 92,457 adults. PLoS\nOne 2020;15:e0227709.\n11. McInnes L, Healy J, Melville J. Umap: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint arXiv:1802.03426 2018.\n12. Coote JH. Recovery of heart rate following intense dynamic exercise. Exp Physiol\n2010;95:431–440.\n13. Gyawali PK, Horacek BM, Sapp JL, Wang L. Sequential factorized autoencoder\nfor localizing the origin of ventricular activation from 12-lead electrocardiograms.\nIEEE Trans Biomed Eng', '\nsmaller (2-dimensional for illustrative purposes) representation.c: The compressed representation is used to train a multiple-instance classiﬁer that can distinguish\nbetween healthy, atrialﬁbrillation, or heart failure). AF5 atrial ﬁbrillation.\n166 Cardiovascular Digital Health Journal, Vol 4, No 6, December 2023\n\nany of the following criteria will be excluded from participa-\ntion in this study: age,18 years, age.85 years, recent pul-\nmonary venous antrum isolation (,1 year), kidney or liver\nfailure, known systemic active in ﬂammatory disease,\nimpaired mental state, inability to use aﬁtness tracker or mo-\nbile phone, impaired cognition, and inability to understand\nthe study protocol.\nPatients will be asked by their treating physician if they\nmay be approached by an investigator to inform them about\nthe study and potential participation. Healthy participants are\nrecruited through local advertising. Anyone that is interested\nwill then receive an information brochure and informed con-\nsent form. At least 2 days after the patient’s receipt of the\nbrochure, the research team will call the patient to schedule\nan appointment. During this visit, the patient submits the\nsigned consent form and will undergo an ECG and blood\npressure measurement that will be analyzed by an experi-\nenced cardiologist (I.B.). Participants can use their own Fitbit\nand are otherwise provided with a Fitbit Inspire 2 or Fitbit\nCharge 5 smartwatch. The device type is assigned to a partic-\nipant at random to prevent device sampling bias. This will\nalso help to investigate the effect of device type on the perfor-\nmance of theﬁnal model. A Fitbit account will be created for\nall participants which will be connected to a custom-built\ndata platform using the Google Cloud Platform. Our platform\nfeatures a data portal for research staff to easily register or de-\nregister participants by authorizing a connection to their Fit-\nbit data. Data are extracted daily from Fitbits until the\nobservation period ends and can be analyzed either in the\ncloud or locally.\nAll participants will be asked toﬁll out a survey regarding\ntheir health. All participants are monitored for a period of 3\nmonths. After written consent from the 200 subjects, heart\nrate, step counter, and sleep time series data are extracted\nfrom the data platform. Clinical metadata such as age, height,\nweight, blood pressure at baseline, health survey, and medi-\ncation use are saved in the Castor (Ciwit BV, Amsterdam,\nThe Netherlands) electronic database.\nData privacy\nAfter performing a thorough data protection impact assess-\nment, the local hospital security information and privacy of-\nﬁcers granted permission to perform the study. This was also\nvalidated by the ethics review board. The data protection\nimpact assessment describes a data management plan con-\nforming to the European General Data Protection Regulation.\nTo protect the data privacy of the participants, all data are\npseudonymized. Only the researchers have access to the\nsensor data, and only the Principal Investigator has access\nto personal information of participants (ie, names, contact in-\nformation, etc). They have all signed processing agreements.\nSecond, the Google servers storing the data are only located\nwithin the Netherlands; hence the data does not leave the\ncountry, therefore conforming to Dutch law. This is done\nto have a clear data infrastructure both legally and technically\nto explain to participants.\nData characteristics and preparation\nThe dataﬁrst undergoes a process involving resampling and\nartifact removal. In our experience with Fitbit smartwatches,\nthe heart rate is nonuniformly sampled, with a prevalent rate\nof 0.2 Hz. Therefore, the heart rate is resampled to once per 5\nseconds. The step counter is sampled once per minute.\nArtifacts involving samples with numerous consecutive\nconstant values are', ' single prediction\nis made collectively on a group of samples (known as a\n“bag”) instead of predicting on individual samples (known\nas “instances”). MIL techniques align well with our time se-\nries data, where during the training phase only 1 label related\nto the subject (bag label) is known while the individual labels\nof the compressed windows (instance labels) remain un-\nknown. In the testing phase the primary objective is to predict\n1 clinical outcome for each subject. The key concept in MIL\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 167\n\nis that each bag of instances is labeled as positive (heart dis-\nease) if it contains a certain amount of positive instances and\nnegative (healthy) if it contains no positive instances (only\nnegative instances). Although traditional MIL approaches\noften classify a bag as positive even with just 1 positive\ninstance, we aim to minimize false-positives by setting a\nthreshold on the number of positive instances required to la-\nbel a bag as positive. This threshold will be determined\nthrough hyperparameter tuning. Thus, instead of learning a\nmodel that predicts the cardiovascular outcome of individual\ninstances, we are learning a model that predicts the outcome\nof a bag of instances.\nAlgorithm validation\nIn our speciﬁc setting where the model encounters data from\npreviously unseen subjects without any prior knowledge, we\nuse leave-p-subjects-out cross-validation (LPSOCV). This\napproach, shown inFigure 4, ensures a more accurate reﬂec-\ntion of real-world situations. LPSOCV involves multiple iter-\nations, or folds, during which data from distinct subjects are\nused for training and validation purposes (ie, the model is\nvalidated on data from subjects that the model is not trained\non), mitigating observation bias.\nMachine learning models are sensitive to the distribution\nof classes. To mitigate potential bias owing to different class\ndistributions in each fold, we incorporate stratiﬁcation during\nthe cross-validation process. This ensures that the ratio of\nnonarrhythmic, atrial ﬁbrillation, and heart failure subjects\nremains approximately consistent and that the inﬂuence of\ninconsistent class distribution across different folds is mini-\nmized.\nHowever, Fitbit time series data exhibit inter-subject vari-\nability resulting from individuals’ distinct physical attri-\nbutes,\n10 making the development of a universally effective\nmodel for “new” subjects challenging. In order to examine\nthe effect of inter-subject variability, we will assess the model\non 2 distinct test sets. Theﬁrst is an external test set that con-\nsists of subjects not previously encountered, randomly\nselected to make up 20% of the total subjects, with an equal\nnumber from each class. This allows us to evaluate the\nmodel’s generalization capabilities. The second test set is\nan internal one, encompassing theﬁnal 20% of data from\nFigure 3 Illustration of multiple-instance learning. The red and blue lines\nindicate bags of heart disease patients and reference subjects. Even though\nthe labels for each instance are not known, for the sake of this example,\nthe plus and minus signs depict time windows where heart disease is present\nor absent, respectively. The decision boundary is depicted by a dotted circle,\nwhere instances within the circle are classiﬁed as heart disease, and instances\noutside the circle are classiﬁed as healthy.\nBox 1. A simple multiple-instance learning\nexample\nMIL is elaborated with an example in 3 steps.\nInitial setup Under traditional supervised learning,\neach window must be annotated to train a machine\nlearning model. However, in our MIL setting (Figure 3),\nonly the bag label is known for the entire set of windows\nrelated to a subject. A bag label is considered negative if']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2538.0,free_text
1_s2.0_S2666693623000737_main,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[10, 1, 7]","[' commercial, or not-for-proﬁt sectors.\nDisclosures\nThe authors declare no conﬂict of interest.\nAuthorship\nAll authors attest they meet the current ICMJE criteria for\nauthorship.\nPatient Consent\nInformed consent was obtained from all subjects involved in\nthe study.\nInstitutional Review Board Statement\nThe study was conducted in accordance with the Declaration\nof Helsinki, and approved by the Institutional Review Board\n(or Ethics Committee) of METC-LDD (protocol code\nNL73708.058.20) for studies involving humans.\nReferences\n1. Roth GA, Mensah GA, Johnson CO, et al. Global burden of cardiovascular dis-\neases and risk factors, 1990–2019: update from the GBD 2019 study. J Am\nColl Cardiol 2020;76:2982–3021.\n2. Tison GH, Sanchez JM, Ballinger B, et al. Passive detection of atrialﬁbrillation\nusing a commercially available smartwatch. JAMA Cardiol 2018;3:409–416.\n3. Wasserlauf J, You C, Patel R, Valys A, Albert D, Passman R. Smartwatch perfor-\nmance for the detection and quantiﬁcation of atrialﬁbrillation. Circ Arrhythm\nElectrophysiol 2019;12:e006834.\n4. Zhu L, Nathan V, Kuang J, et al. Atrialﬁbrillation detection and atrialﬁbrillation\nburden estimation via wearables. IEEE J Biomed Health Inform 2021;\n26:2063–2074.\n5. Lubitz SA, Faranesh AZ, Selvaggi C, et al. Detection of atrialﬁbrillation in a large\npopulation using wearable devices: the Fitbit heart study. Circulation 2022;\n146:1415–1424.\n6. Hall A, Mitchell ARJ, Wood L, Holland C. Effectiveness of a single lead Alive-\nCor electrocardiogram application for the screening of atrialﬁbrillation: a system-\natic review. Medicine (Baltimore) 2020;99:e21388.\n7. Torres-Soto J, Ashley EA. Multi-task deep learning for cardiac rhythm detection\nin wearable devices. NPJ Digit Med 2020;3:116.\n8. Mienye ID, Sun Y, Wang Z. Improved sparse autoencoder based artiﬁcial neural\nnetwork approach for prediction of heart disease. Informatics in Medicine Un-\nlocked 2020;18:100307.\n9. Spathis D, Perez-Pozuelo I, Brage S, Wareham NJ, Mascolo C. Self-supervised\ntransfer learning of physiological representations from free-living wearable\ndata. In: Proceedings of the Conference on Health. Inference, and Learning;\n2021. p. 69–78.\n10. Quer G, Gouda P, Galarnyk M, Topol EJ, Steinhubl SR. Inter-and intraindividual\nvariability in daily resting heart rate and its associations with age, sex, sleep, BMI,\nand time of year: retrospective, longitudinal cohort study of 92,457 adults. PLoS\nOne 2020;15:e0227709.\n11. McInnes L, Healy J, Melville J. Umap: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint arXiv:1802.03426 2018.\n12. Coote JH. Recovery of heart rate following intense dynamic exercise. Exp Physiol\n2010;95:431–440.\n13. Gyawali PK, Horacek BM, Sapp JL, Wang L. Sequential factorized autoencoder\nfor localizing the origin of ventricular activation from 12-lead electrocardiograms.\nIEEE Trans Biomed Eng', ' Digital Health Journal 2023;4:165–172)\n© 2023\nHeart Rhythm Society. This is an open access article under the CC\nBY license (http://creativecommons.org/licenses/by/4.0/).\nIntroduction\nCardiovascular disease is one of the leading causes of mortal-\nity globally1 and cardiovascular healthcare accounts for a\nlarge portion of global healthcare costs and expenses. Early\ndetection and prevention will decrease the burden of cardio-\nvascular disease and will therefore decrease mortality,\nmorbidity, and costs. Cardiovascular monitoring using big\ndata from wearables and machine learning can drastically in-\ncrease the availability and ef ﬁciency of cardiovascular\nhealthcare globally, at the fraction of the costs of conven-\ntional medical-grade devices.\nElectrocardiogram (ECG) monitoring, such as Holters or\nimplantable loop recorders, are the gold standard for moni-\ntoring of outpatients with known or suspected arrhythmias.\nHowever, they are burdensome, can only be used for a\nlimited period of time, and are expensive. Implantable loop\nrecorders are invasive and have to be manually activated\nand analyzed in the hospital. This severely limits the use of\nthese devices for long-term home monitoring of patients,\nand they have suboptimal patient comfort. For patients with\nchronic cardiovascular diseases, such as atrial ﬁbrillation\nand heart failure, this implies frequent hospital visits and\nAddress reprint requests and correspondence:Mr Arman Naseri, Haga\nHospital, The Hague, Netherlands. E-mail address: a.naserijahfari@\nhagaziekenhuis.nl.\n2666-6936/© 2023 Heart Rhythm Society. This is an open access article under the CC BY license\n(http://creativecommons.org/licenses/by/4.0/).\nhttps://doi.org/10.1016/j.cvdhj.2023.09.001\n\nsometimes even hospital admissions (associated with higher\nmortality) that can be prevented by continuous and adequate\nhome monitoring.\nWith the widespread availability of reliable, consumer-\ngrade wearables such as smartwatches, continuous moni-\ntoring of, for example, heart rate with photoplethysmography\nand step counting with accelerometers is possible. This moni-\ntoring is easy, patient friendly, and cost effective. Combining\nthe power of large amounts of data (big data) and novel ma-\nchine learning techniques, these time series can be used to\ndetect and perhaps even predict cardiovascular disease, there-\nfore improving patient care. There are some caveats, howev-\ner, as not all wearables have the same characteristics and\nquality. Consequently, they have been used with moderate\nsuccess.\n2,3 They also provide less informative diagnostic sig-\nnals as compared to, for example, electrocardiography or\nother commonly used cardiologic diagnostic modalities.\nThe challenge but also the strength of machine learning\nmodels is that they learn by example and therefore large\namounts of data are needed for which the cardiovascular\noutcome (class label) has been determined. Typically, super-\nvised learning is used, where each observation of the data has\na class label. This must be done with ECGs, since photople-\nthysmography or derived signals are difﬁcult to interpret by a\nclinician. This so-called labeling or annotating of signals by\nphysicians is infeasible for the large amounts of (continuous)\ndata required, and therefore semi-automated\n4,5 and fully\nautomated2,3 ECG labeling systems6 have been developed.\nHowever, these still require a lot of manual labor from contin-\nuously monitored users.\nTherefore, the objective of the ME-TIME is (early) detec-\ntion and prevention of heart disease by leveraging time series\ndata from smartwatches, a cloud-based infrastructure, and\nmachine learning algorithms', ', is chosen for theﬁnal model; a process\nknown as hyperparameter tuning.\nResults\nPreliminary ﬁndings are discussed in the following sections.\nStudy characteristics\nSo far, 62 of the 200 envisioned subjects have been included\nand data from 22 subjects (15 healthy, 7 AF) have been ex-\ntracted successfully from the data platform for preliminary\nanalysis (Table 1).\nData show large inter-subject variability\nNonoverlapping 1-hour windows are used to segment heart\nrate and step counter time series data from 6 subjects. To\nvisualize this high-dimensional data, UMAP\n11 is employed\nInternal test\nExternal test\n1 123\n12 23\n123 3\n1\n2\n3\n4\nCross-validation\n(train/validation loop)\nFigure 4 Our leave-p-subjects-out cross-validation strategy consists of the following: On the left, cross-validation folds are illustrated using 3 subjects with\ncorresponding subject number. Green and blue represent training and validation subjects, respectively. On the right, a single fold is expanded and additionally\nillustrates the internal and external test sets. Each row corresponds to a subject, and the dotted squares within each row represent windows. The yellow external test\nblock encompasses entire subjects and their corresponding data points that have not been encountered by the model. Meanwhile, the dark yellow internal test\nblock consists of unobserved data points, representing theﬁnal 20% measurements of the time series from subjects already encountered during model develop-\nment.\nTable 1 Characteristics of preliminary study participants as of\nMay 2022\nCharacteristic Ref HF AF\nParticipants, n 25 15 22\nAge, y\n18–39 16 1 0\n40–54 3 3 2\n55–64 5 5 3\n651 16 1 7\nSex\nMale 12 12 15\nFemale 13 3 7\nBMI\n18.5–24.9 14 3 7\n25–29.9 6 6 8\n301 56 7\nDiabetes\nYes 0 5 4\nNo 25 10 18\nSmoking\nYes 0 6 5\nNo 25 9 17\nHypertension\nYes 2 8 15\nNo 23 7 7\nDevice\nCharge 5 23 8 12\nInspire 2 2 7 10\nAF 5atrial ﬁbrillation group; BMI5body mass index; HF5heart failure\ngroup; Ref5 reference group.\nNaseri et al ME-TIME: CVD Detection Using Smartwatches and AI 169\n\nto reduce the data to 2 dimensions while maintaining as much\nstructure as possible. The resulting embedding is displayed in\nFigure 5, where the distribution of the 2-dimensional UMAP\nsamples are illustrated per subject.\nWhen there is little overlap between subjects,ﬁnding a\nshared pattern among them becomes challenging, making it\ndifﬁcult for a model to learn. As a result, the performance\nof a machine learning model could be impacted as, during\nFigure 5 Visualization of heart rate windows (upper image) and step windows (lower image) of 6 subjects. Each window has data of 1 hour (720 time points for\nheart rate, 60 for steps, respectively). Each high-dimensional window is mapped to a 2-dimensional (2D) location using UMAP.11 The contour curves illustrate the\ndistribution of the 2D UMAP samples for each subject, with each color representing 1 of the 6 subjects.\nFigure 6 Acceleration-deceleration curves during light activity. Red and blue represent data from subjects with persistent atrialﬁbrillation (n57) and no heart\nd']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2519.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,Diagnostic,,,,"[5, 4, 7]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', 'ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.', ' and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C. Silva, F. D. M. Guedes and\nJ. J. P. C. Rodrigues, “A mobile healthcare solution for ambient assisted\nliving environments,” 2014 IEEE 16th International Conference on e-\nHealth Networking, Applications and Services (Healthcom), Natal, 2014,\npp. 170-175. doi: 10.1109/HealthCom.2014.7001836\n[4] J. M. Quero et al., “Health Care Applications Based on Mobile Phone\nCentric Smart Sensor Network,” 2007 29th Annual International Con-\nference of the IEEE Engineering in Medicine and Biology Society, Lyon,\n2007, pp. 6298-6301. doi: 10.1109/IEMBS.2007.4353795\n[5] Kvedar et al., “Connected health: A review of technologies and strategies\nto improve patient care with telemedicine and telehealth,”Health Affairs,\nvol. 33, pp. 194199, 2014.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n505\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\n[6] Swerdlow, D.R. et. al., “Robotic armassisted sonography: Review of\ntechnical developments and potential clinical applications,” Am. J.\nRoentgenol., vol. 208, pp. 733738, 2017.\n[7] J. Grundy, M. Abdelrazek and M. K. Curumsing, “Vision: Improved\nDevelopment of Mobile eHealth Applications,” 2018 IEEE/ACM 5th\nInternational Conference on Mobile Software Engineering and Systems\n(MOBILESoft), Gothenburg, Sweden, 2018, pp. 219-223.\n[8] H.-Y . Tang et al., “Miniaturizing ultrasonic system for portable health\ncare and ﬁtness,” IEEE Trans. Biomed. Circuits Syst., vol. 9, no. 6, pp.\n767776, Dec. 2015.\n[9] J. Kang et al., “A System-on-Chip Solution for Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2553.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",Unknown from this paper.,,,,"[7, 5, 9]","[' and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C. Silva, F. D. M. Guedes and\nJ. J. P. C. Rodrigues, “A mobile healthcare solution for ambient assisted\nliving environments,” 2014 IEEE 16th International Conference on e-\nHealth Networking, Applications and Services (Healthcom), Natal, 2014,\npp. 170-175. doi: 10.1109/HealthCom.2014.7001836\n[4] J. M. Quero et al., “Health Care Applications Based on Mobile Phone\nCentric Smart Sensor Network,” 2007 29th Annual International Con-\nference of the IEEE Engineering in Medicine and Biology Society, Lyon,\n2007, pp. 6298-6301. doi: 10.1109/IEMBS.2007.4353795\n[5] Kvedar et al., “Connected health: A review of technologies and strategies\nto improve patient care with telemedicine and telehealth,”Health Affairs,\nvol. 33, pp. 194199, 2014.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n505\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\n[6] Swerdlow, D.R. et. al., “Robotic armassisted sonography: Review of\ntechnical developments and potential clinical applications,” Am. J.\nRoentgenol., vol. 208, pp. 733738, 2017.\n[7] J. Grundy, M. Abdelrazek and M. K. Curumsing, “Vision: Improved\nDevelopment of Mobile eHealth Applications,” 2018 IEEE/ACM 5th\nInternational Conference on Mobile Software Engineering and Systems\n(MOBILESoft), Gothenburg, Sweden, 2018, pp. 219-223.\n[8] H.-Y . Tang et al., “Miniaturizing ultrasonic system for portable health\ncare and ﬁtness,” IEEE Trans. Biomed. Circuits Syst., vol. 9, no. 6, pp.\n767776, Dec. 2015.\n[9] J. Kang et al., “A System-on-Chip Solution for Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking', ' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2125.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Unknown from this paper.,,,,"[5, 2, 6]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', ' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2532.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q4,How was the dataset described in this study before predictive modeling was performed?,"58 ultrasound images, 36 normal and 22 abnormal.",,,,"[5, 4, 6]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', 'ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2547.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?",Unknown from this paper.,,,,"[5, 4, 6]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', 'ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2548.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q6,What preprocessing techniques on the included variables/features were applied in this study?,Cropping and resizing to 224x224 pixels.,,,,"[2, 6, 4]","[' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C', 'ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2541.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q7,How is missing data handled in this study?,Unknown from this paper.,,,,"[6, 2, 9]","[' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C', ' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2098.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[7, 9, 6]","[' and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C. Silva, F. D. M. Guedes and\nJ. J. P. C. Rodrigues, “A mobile healthcare solution for ambient assisted\nliving environments,” 2014 IEEE 16th International Conference on e-\nHealth Networking, Applications and Services (Healthcom), Natal, 2014,\npp. 170-175. doi: 10.1109/HealthCom.2014.7001836\n[4] J. M. Quero et al., “Health Care Applications Based on Mobile Phone\nCentric Smart Sensor Network,” 2007 29th Annual International Con-\nference of the IEEE Engineering in Medicine and Biology Society, Lyon,\n2007, pp. 6298-6301. doi: 10.1109/IEMBS.2007.4353795\n[5] Kvedar et al., “Connected health: A review of technologies and strategies\nto improve patient care with telemedicine and telehealth,”Health Affairs,\nvol. 33, pp. 194199, 2014.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n505\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\n[6] Swerdlow, D.R. et. al., “Robotic armassisted sonography: Review of\ntechnical developments and potential clinical applications,” Am. J.\nRoentgenol., vol. 208, pp. 733738, 2017.\n[7] J. Grundy, M. Abdelrazek and M. K. Curumsing, “Vision: Improved\nDevelopment of Mobile eHealth Applications,” 2018 IEEE/ACM 5th\nInternational Conference on Mobile Software Engineering and Systems\n(MOBILESoft), Gothenburg, Sweden, 2018, pp. 219-223.\n[8] H.-Y . Tang et al., “Miniaturizing ultrasonic system for portable health\ncare and ﬁtness,” IEEE Trans. Biomed. Circuits Syst., vol. 9, no. 6, pp.\n767776, Dec. 2015.\n[9] J. Kang et al., “A System-on-Chip Solution for Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2103.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q9,Which prediction models were used in this study?,VGG-16 with transfer learning and fine-tuning.,,,,"[4, 7, 2]","['ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.', ' and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C. Silva, F. D. M. Guedes and\nJ. J. P. C. Rodrigues, “A mobile healthcare solution for ambient assisted\nliving environments,” 2014 IEEE 16th International Conference on e-\nHealth Networking, Applications and Services (Healthcom), Natal, 2014,\npp. 170-175. doi: 10.1109/HealthCom.2014.7001836\n[4] J. M. Quero et al., “Health Care Applications Based on Mobile Phone\nCentric Smart Sensor Network,” 2007 29th Annual International Con-\nference of the IEEE Engineering in Medicine and Biology Society, Lyon,\n2007, pp. 6298-6301. doi: 10.1109/IEMBS.2007.4353795\n[5] Kvedar et al., “Connected health: A review of technologies and strategies\nto improve patient care with telemedicine and telehealth,”Health Affairs,\nvol. 33, pp. 194199, 2014.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n505\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\n[6] Swerdlow, D.R. et. al., “Robotic armassisted sonography: Review of\ntechnical developments and potential clinical applications,” Am. J.\nRoentgenol., vol. 208, pp. 733738, 2017.\n[7] J. Grundy, M. Abdelrazek and M. K. Curumsing, “Vision: Improved\nDevelopment of Mobile eHealth Applications,” 2018 IEEE/ACM 5th\nInternational Conference on Mobile Software Engineering and Systems\n(MOBILESoft), Gothenburg, Sweden, 2018, pp. 219-223.\n[8] H.-Y . Tang et al., “Miniaturizing ultrasonic system for portable health\ncare and ﬁtness,” IEEE Trans. Biomed. Circuits Syst., vol. 9, no. 6, pp.\n767776, Dec. 2015.\n[9] J. Kang et al., “A System-on-Chip Solution for Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking', ' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2541.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,Unknown from this paper.,,,,"[5, 9, 8]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ', ' Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking, Applica-\ntion & Services (HealthCom), Boston, MA, 2015, pp. 211-216. doi:\n10.1109/HealthCom.2015.7454500\n[12] R. K. Megalingam, G. Pocklassery, V . Jayakrishnan and G. Mourya,\n“PULSS: Portable ultrasound scanning system,” 2013 IEEE Global Hu-\nmanitarian Technology Conference: South Asia Satellite (GHTC-SAS),\nTrivandrum, 2013, pp. 119-123. doi: 10.1109/GHTC-SAS.2013.6629900\n[13] M. Shoaib, U. Ahmad and A. Al-Amri, “Multimedia framework to\nsupport eHealth applications,” Multimedia Tools and Applications, Dec.\n2014, vol. 73, no. 3, pp. 2081-2101.\n[14] P. Vaish, R. Bharath, P. Rajalakshmi and U. B. Desai, “Smartphone\nbased automatic abnormality detection of kidney in ultrasound images,”\n2016 IEEE 18th International Conference on e-Health Networking,\nApplications and Services (Healthcom), Munich, 2016, pp. 1-6. doi:\n10.1109/HealthCom.2016.7749492\n[15] R. Bharath, P. Vaish and P. Rajalakshmi, “Implementation of diagnosti-\ncally driven compression algorithms via WebRTC for IoT enabled tele-\nsonography,” 2016 IEEE EMBS Conference on Biomedical Engineer-\ning and Sciences (IECBES), Kuala Lumpur, 2016, pp. 204-209. doi:\n10.1109/IECBES.2016.7843443\n[16] R. Bharath and P. Rajalakshmi, “WebRTC based invariant scattering\nconvolution network for automated validation of ultrasonic videos for\nIoT enabled tele-sonography,” 2018 IEEE 4th World Forum on Internet\nof Things (WF-IoT), Singapore, 2018, pp. 790-795. doi: 10.1109/WF-\nIoT.2018.8355197\n[17] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/V']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2118.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q11,How was data augmentation or generation used in this study?,Unknown from this paper.,,,,"[2, 9, 7]","[' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ', ' and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C. Silva, F. D. M. Guedes and\nJ. J. P. C. Rodrigues, “A mobile healthcare solution for ambient assisted\nliving environments,” 2014 IEEE 16th International Conference on e-\nHealth Networking, Applications and Services (Healthcom), Natal, 2014,\npp. 170-175. doi: 10.1109/HealthCom.2014.7001836\n[4] J. M. Quero et al., “Health Care Applications Based on Mobile Phone\nCentric Smart Sensor Network,” 2007 29th Annual International Con-\nference of the IEEE Engineering in Medicine and Biology Society, Lyon,\n2007, pp. 6298-6301. doi: 10.1109/IEMBS.2007.4353795\n[5] Kvedar et al., “Connected health: A review of technologies and strategies\nto improve patient care with telemedicine and telehealth,”Health Affairs,\nvol. 33, pp. 194199, 2014.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n505\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\n[6] Swerdlow, D.R. et. al., “Robotic armassisted sonography: Review of\ntechnical developments and potential clinical applications,” Am. J.\nRoentgenol., vol. 208, pp. 733738, 2017.\n[7] J. Grundy, M. Abdelrazek and M. K. Curumsing, “Vision: Improved\nDevelopment of Mobile eHealth Applications,” 2018 IEEE/ACM 5th\nInternational Conference on Mobile Software Engineering and Systems\n(MOBILESoft), Gothenburg, Sweden, 2018, pp. 219-223.\n[8] H.-Y . Tang et al., “Miniaturizing ultrasonic system for portable health\ncare and ﬁtness,” IEEE Trans. Biomed. Circuits Syst., vol. 9, no. 6, pp.\n767776, Dec. 2015.\n[9] J. Kang et al., “A System-on-Chip Solution for Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2107.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,Unknown from this paper.,,,,"[5, 4, 6]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', 'ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2541.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q13,Which type of explainability techniques are used?,CNN based framework,,,,"[6, 2, 5]","[' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C', ' eHealth ap-\nplications using scalable video coding (SVC) extension for\nMPEC-4 A VC/H.264 which enables a doctor in a different\nlocation to administer the scanning in real-time. This solution\nis feasible in areas where high bandwidth network connectivity\nis available and may not be a suitable solution for rural areas.\nIn [14] In [14], authors proposed an abnormality detection\nbased on Viola Jones and Support Vector Machine (SVM)\nclassiﬁer to detect the abnormality in ultrasound image. In\n[15], authors proposed a framework for compressing the\nultrasound images using web real-time communication (We-\nbRTC) framework technology for low data real-time eHealth\napplications. However, it still requires the high performance\nnetwork connectivity which is not available in rural areas.\nAlso, in [16] the authors have developed similar frameworks\nfor tele-diagnosis wherein the ultrasound scanning information\nis streamed to the expert side for getting inferences. In all the\nabove studies the major drawbacks include the following: (1)\nthe infrastructure needs to be replaced, (2) requirement of high\nperformance network connectivity and (3) manual observance\nof the ultrasound data which is prone to errors and entirely\ndepends on the skill of the sonographers. Hence, in this paper,\nwe developed a web based architecture wherein a clinician can\nupload the ultrasound image and can get accurate diagnosis\ninformation. This architecture does not require any change in\nthe existing infrastructure and is an easily scalable solution\nespecially for developing nations.\nThe rest of this paper is organized as follows. Section\nII describes the proposed architecture and the functional\nunits present. Section III describes the developed CNN based\nclassiﬁcation framework for accurate classiﬁcation of fatty\nliver using ultrasound images. In Section IV , we discuss\nthe dataset developed for analyzing the performance of the\nproposed architecture and the key insights observed from the\nperformance analysis. Finally, Section V concludes the paper\nby summarizing the work performed and discussed the future\nscope of this work.\nII. P ROPOSED NOVEL E HEALTH ARCHITECTURE FOR\nACCURATE WEB BASED FATTY LIVER CLASSIFICATION\nFig. 1 shows the ultrasound images of liver under normal\nconditions and those with affected by fatty liver. Excess fat\naccumulated in the liver can lead to fatty liver conditions\nand can cause severe liver damage. The normal liver usually\ncontains 5 to 10% of the fat and if the fat concentration\nexceeds beyond this limit, it is observed as a fatty liver\n(a)\n(b)\nFig. 1. (a) Ultrasound images of liver under normal conditions, (b) Ultrasound\nimages of liver under fatty liver conditions.\nWeb \nBrowser\nPre−processingUltrasound\nLiver Image\nNormal\nAbnormal\nOutput Prediction\nCNN Based\nClassifier\nClient Interface (Remote Location) Cloud Based Web Application\nFig. 2. Proposed novel low-cost and scalable eHealth architecture for accurate\nweb based fatty liver classiﬁcation\ncondition. The liver in general is capable of repairing if the\nold cells are damaged by rebuilding the new liver cells until\nrepeated liver damage occurs. Currently, the fatty liver disease\nis becoming a more prevailing condition, affecting about 25 to\n30 % of the population in both the developed and developing\ncountries [17]. If the condition is left untreated, fatty liver\nleads to more harmful steatohepatitis gradually leading into\nliver cancer. Early detection of fatty liver can thus prevent from\nthe permanent failure of the liver. To identify the fatty liver\ncondition, ultrasound imaging is the widely used diagnostic\nmethod.\nFig. 2 shows the proposed architecture for the novel low-\ncost and scalable eHealth architecture for fatty liver classiﬁca-\ntion. The entire architecture can be', ' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2522.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,"Accuracy, confusion matrix, F-score, Precision, Recall.",,,,"[5, 4, 9]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', 'ician.\nFig. 3. Developed web interface along with the ultrasound and prediction\nwhen tested using a normal liver image.\nFig. 4. Developed web interface along with the ultrasound and prediction\nwhen tested using a liver image with a fatty liver condition.\nIII. D EVELOPED CNN BASED ACCURATE FATTY LIVER\nCLASSIFICATION FRAMEWORK\nIn [19], we have developed a novel CNN based accurate\nfatty liver classiﬁcation model using ultrasound images. The\nsame model is adopted in this paper for the classiﬁcation of\nfatty liver using ultrasound images. The architecture comprises\nof a pre-trained VGG-16 model along with the transfer learn-\ning and ﬁne-tuning. VGGNet using CNN is initially designed\nwith different layer depths aiming for image recognition tasks.\nPreliminary analysis of VGGNet offered a promising accuracy\nof 92.7% when validated using the ImagNet dataset com-\nprising of 14 million images from 1000 classes [20]. In this\npaper, we make use the 16 layers VGG-16 with convolution\nblocks(including convolution layers and max-pooling layers)\nand a fully connected classiﬁer. The ﬁne-tuning is carried out\nusing the pre-trained VGG-16 model in Keras. Traditionally,\nthe convolution 2D layers in VGG-16 consists of 512 nodes for\nconvolution layers, however, in our experiment we ﬁne-tuned\nthe network from using 512 nodes to 256 nodes. Also, we ﬁne-\ntuned fully connected layers having 4096 nodes to 256 nodes,\nand the output layer comprises two neurons whose output\ncorresponds to the two classes (normal and abnormal) in this\nstudy. During the training of the model, the weight parameters\nof the output layer are initialized using random Gaussian\ndistribution, and the training is performed over 100 epochs.\nSince the development of the CNN based accurate fatty liver\nclassiﬁcation framework is not our primary contribution in this\npaper, we would like to advise the readers to kindly refer [19]\nfor more details on the developed model. However, we would\nlike to highlight that the database used for validation in [19]\nis different from what we used here.\nIV. R ESULTS\nThe publicly available datasets for ultrasound images of the\nliver are very few. Hence, due to the unavailability of the\npublic datasets, we acquired and developed our own dataset\nusing a GE LOGIQ F6 ultrasound scanner in collaboration\nwith MNR Medical College, Sangareddy, Hyderabad, India.\nThe dataset consisted of 58 representative liver images com-\nprising of both fatty liver (22 images) and normal conditions\n(36 images). We cropped the images which does not convey\nany diagnostic information such as hospital name, time of the\ndiagnosis, etc. Also, since the pre-trained models are trained\nwith an input image size of 224×224 pixels, the images of\nour dataset were also resized to 224×224 pixels for compat-\nibility. The performance of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2129.0,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,Unknown from this paper,,,,"[5, 8, 9]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', ' Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking, Applica-\ntion & Services (HealthCom), Boston, MA, 2015, pp. 211-216. doi:\n10.1109/HealthCom.2015.7454500\n[12] R. K. Megalingam, G. Pocklassery, V . Jayakrishnan and G. Mourya,\n“PULSS: Portable ultrasound scanning system,” 2013 IEEE Global Hu-\nmanitarian Technology Conference: South Asia Satellite (GHTC-SAS),\nTrivandrum, 2013, pp. 119-123. doi: 10.1109/GHTC-SAS.2013.6629900\n[13] M. Shoaib, U. Ahmad and A. Al-Amri, “Multimedia framework to\nsupport eHealth applications,” Multimedia Tools and Applications, Dec.\n2014, vol. 73, no. 3, pp. 2081-2101.\n[14] P. Vaish, R. Bharath, P. Rajalakshmi and U. B. Desai, “Smartphone\nbased automatic abnormality detection of kidney in ultrasound images,”\n2016 IEEE 18th International Conference on e-Health Networking,\nApplications and Services (Healthcom), Munich, 2016, pp. 1-6. doi:\n10.1109/HealthCom.2016.7749492\n[15] R. Bharath, P. Vaish and P. Rajalakshmi, “Implementation of diagnosti-\ncally driven compression algorithms via WebRTC for IoT enabled tele-\nsonography,” 2016 IEEE EMBS Conference on Biomedical Engineer-\ning and Sciences (IECBES), Kuala Lumpur, 2016, pp. 204-209. doi:\n10.1109/IECBES.2016.7843443\n[16] R. Bharath and P. Rajalakshmi, “WebRTC based invariant scattering\nconvolution network for automated validation of ultrasonic videos for\nIoT enabled tele-sonography,” 2018 IEEE 4th World Forum on Internet\nof Things (WF-IoT), Singapore, 2018, pp. 790-795. doi: 10.1109/WF-\nIoT.2018.8355197\n[17] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/V', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,,,,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?",Unknown from this paper,,,,"[5, 6, 8]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', ' Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for improving the state of\nthe art research in the development of scalable and low-cost\neHealth applications.\nV. CONCLUSION\nIn this paper, we proposed and developed a low-cost and\neasily scalable eHealth architecture comprising of a web\napplication for automatic classiﬁcation of fatty liver using\nultrasound images. The developed web application is easy to\nuse and requires no change in the current infrastructure. The\nultrasound images obtained using the traditional ultrasound\nscanners can be uploaded to the developed web application\nusing moderate network connectivity and the web application\nthen classiﬁes the image into either normal or abnormal using\na CNN based framework. The performance of the proposed\nframework is tested with with 58 ultrasound images, we have\ndeveloped a custom database comprising of 58 ultrasound\nimages with liver information of which 36 correspond to liver\nimages are normal conditions and the rest correspond to a fatty\nliver condition. It is observed that the proposed framework\nachieves an average accuracy of 91.37%. Also, the latency\nanalysis shows that the proposed CNN model predicts within\n20 ms when running on a PC with Intel i7 processor and 16 GB\nRAM. Also when considered along with the network latency\nthe total latency observed is approximately 150 ms when used\nwith moderate network connectivity. Hence, we strongly feel\nthat the proposed eHealth framework will help future research\nin developing low-cost, easily scalable and ubiquitous eHealth\nframeworks. Our future scope of this work is to develop a more\nrobust classiﬁcation framework which can eliminate the false\nnegatives. Also, we would like to consider real-time trials in\ncoordination with hospitals to analyze the performance in real\nconstraints.\nVI. ACKNOWLEDGMENT\nWe are thankful to the team of radiologists at Asian Institute\nof Gastroenterology and MNR Medical College & Hospital,\nHyderabad, Telangana, India, for spending their valuable time\nduring this research. This research was partly funded by\nvisvesvaraya Ph.D. Scheme, Media Lab Asia, MEITY , Govt.\nof India and partly funded by Indian Institute of Technology\nHyderabad.\nREFERENCES\n[1] B. Farahani, F. Firouzi, V . Chang, M. Badaroglu, N. Constant, and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C', ' Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking, Applica-\ntion & Services (HealthCom), Boston, MA, 2015, pp. 211-216. doi:\n10.1109/HealthCom.2015.7454500\n[12] R. K. Megalingam, G. Pocklassery, V . Jayakrishnan and G. Mourya,\n“PULSS: Portable ultrasound scanning system,” 2013 IEEE Global Hu-\nmanitarian Technology Conference: South Asia Satellite (GHTC-SAS),\nTrivandrum, 2013, pp. 119-123. doi: 10.1109/GHTC-SAS.2013.6629900\n[13] M. Shoaib, U. Ahmad and A. Al-Amri, “Multimedia framework to\nsupport eHealth applications,” Multimedia Tools and Applications, Dec.\n2014, vol. 73, no. 3, pp. 2081-2101.\n[14] P. Vaish, R. Bharath, P. Rajalakshmi and U. B. Desai, “Smartphone\nbased automatic abnormality detection of kidney in ultrasound images,”\n2016 IEEE 18th International Conference on e-Health Networking,\nApplications and Services (Healthcom), Munich, 2016, pp. 1-6. doi:\n10.1109/HealthCom.2016.7749492\n[15] R. Bharath, P. Vaish and P. Rajalakshmi, “Implementation of diagnosti-\ncally driven compression algorithms via WebRTC for IoT enabled tele-\nsonography,” 2016 IEEE EMBS Conference on Biomedical Engineer-\ning and Sciences (IECBES), Kuala Lumpur, 2016, pp. 204-209. doi:\n10.1109/IECBES.2016.7843443\n[16] R. Bharath and P. Rajalakshmi, “WebRTC based invariant scattering\nconvolution network for automated validation of ultrasonic videos for\nIoT enabled tele-sonography,” 2018 IEEE 4th World Forum on Internet\nof Things (WF-IoT), Singapore, 2018, pp. 790-795. doi: 10.1109/WF-\nIoT.2018.8355197\n[17] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/V']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,,,,free_text
A_Novel_Web_Application_Framework_for_Ubiquitous_Classification_of_Fatty_Liver_Using_Ultrasound_Images,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[5, 7, 9]","[' of the proposed model is analyzed\nconsidering classiﬁcation accuracy, confusion matrix, Fscore,\nPrecision, and Recall as the key performance metrics. The\ndescription of the considered performance metrics are given\nbelow:\nFscore = 2 recall∗precision\n(recall+precision) , (1)\nR\necall = N(TP )\nN(T\nP) +N(FN ) , (2)\nP\nrecision = N(TP )\nN(T\nP) +N(FP ) , (3)\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n504\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE I\nCONFUSION MATRIX OF THE PROPOSED ALGORITHM\nTrue class Predicted class\nNormal Abnormal\nNormal (36) 33 3\nAbnormal (22) 2 20\nTABLE II\nPERFORMANCE ANALYSIS OF THE PROPOSED ALGORITHM FOR\nDETECTION AND CLASSIFICATION OF FATTY LIVER .\nClass Precision Recall F1score Support\nNormal 94.2 91.6 92.8 36\nAbnormal 86.9 90.9 88.8 22\nAvg/total 90.5 91.2 90.8 58\nAccur\nacy = N(TP ) +N(TN )\nN(T\nP) +N(TN ) +N(FP ) +N(FN ) , (4)\nwhere N(T\nP) indicates the total true positives, N(FP )\nindicates total false positives, N(TN ) indicates total true\nnegatives and N(FN ) indicates the false negatives. All these\nmeasures are computed for each class, and an overall measure\nof the algorithm is computed by taking the average of all\nthese measures across the two classes. Also, we have analyzed\nthe latency for classiﬁcation to understand the suitability of\nthe framework in areas where moderate internet connection\nis available. From the analysis it is observed that the devel-\noped fatty liver classiﬁcation framework achieved an average\nclassiﬁcation accuracy of 91.37%. Out of the 22 images\nrepresenting fatty liver, 20 are classiﬁed correctly while the\nother 2 images are classiﬁed as normal. Similarly, 3 of the 36\nnormal images are classiﬁed to be abnormal. Table IV shows\nthe confusion matrix obtained when the developed framework\nis validated using the developed dataset. One can observe that\nthe developed algorithm achieves a classiﬁcation accuracy of\n91.37%. Table II gives the obtained Fscore, Precision, and\nRecall. The highest precision of 94.2% is achieved for Normal\ncategory followed by 86.9% for Abnormal. On an average, the\nproposed algorithm offers a precision of 90.5%. Regarding\nrecall, the highest is achieved for Normal class with 91.6%;\nthe lowest recall is obtained with Abnormal 90.9% and the\naverage recall obtained is 91.2%. Finally, the average Fscore\nis observed to be 90.8%, where maximum is Normal 92.8%\nand minimum for Abnormal 88.8% respectively.\nAlso, it is observed that the developed classiﬁcation model\noffers a latency of 20 ms for an image to be classiﬁed\nwhen running on an Intel i7 processor with 16 GB RAM.\nHowever, it is observed that in most of the cases, the latency\ndoes not exceed 150 ms with moderate network connectivity\n(approximately 2 Mbps bandwidth). We strongly feel that this\npaper can aid future researchers for', ' and\nK. Mankodiya, “Towards fog-driven IoT eHealth: Promises and chal-\nlenges of IoT in medicine and healthcare,” Future Generat. Com-\nput. Syst., vol. 78, pp. 659676, Jan. 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X17307677\n[2] Strasser, R., Kam, S.M., and Regalado, S.M. (2016). Rural Health Care\nAccess and Policy in Developing Countries. Annual Review of Public\nHealth, 37, 395-412\n[3] D. F. M. Rodrigues, E. T. Horta, B. M. C. Silva, F. D. M. Guedes and\nJ. J. P. C. Rodrigues, “A mobile healthcare solution for ambient assisted\nliving environments,” 2014 IEEE 16th International Conference on e-\nHealth Networking, Applications and Services (Healthcom), Natal, 2014,\npp. 170-175. doi: 10.1109/HealthCom.2014.7001836\n[4] J. M. Quero et al., “Health Care Applications Based on Mobile Phone\nCentric Smart Sensor Network,” 2007 29th Annual International Con-\nference of the IEEE Engineering in Medicine and Biology Society, Lyon,\n2007, pp. 6298-6301. doi: 10.1109/IEMBS.2007.4353795\n[5] Kvedar et al., “Connected health: A review of technologies and strategies\nto improve patient care with telemedicine and telehealth,”Health Affairs,\nvol. 33, pp. 194199, 2014.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n505\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. \n\n[6] Swerdlow, D.R. et. al., “Robotic armassisted sonography: Review of\ntechnical developments and potential clinical applications,” Am. J.\nRoentgenol., vol. 208, pp. 733738, 2017.\n[7] J. Grundy, M. Abdelrazek and M. K. Curumsing, “Vision: Improved\nDevelopment of Mobile eHealth Applications,” 2018 IEEE/ACM 5th\nInternational Conference on Mobile Software Engineering and Systems\n(MOBILESoft), Gothenburg, Sweden, 2018, pp. 219-223.\n[8] H.-Y . Tang et al., “Miniaturizing ultrasonic system for portable health\ncare and ﬁtness,” IEEE Trans. Biomed. Circuits Syst., vol. 9, no. 6, pp.\n767776, Dec. 2015.\n[9] J. Kang et al., “A System-on-Chip Solution for Point-of-Care Ultrasound\nImaging Systems: Architecture and ASIC Implementation,” in IEEE\nTransactions on Biomedical Circuits and Systems, vol. 10, no. 2, pp.\n412-423, April 2016. doi: 10.1109/TBCAS.2015.2431272\n[10] Stawicki, Stanislaw Peter, and David Paul Bahner. “Modern sonology\nand the bedside practitioner: evolution of ultrasound from curious\nnovelty to essential clinical tool,” European Journal of Trauma and\nEmergency Surgery 41.5, 457-460, 2015.\n[11] R. Bharath et al., “Portable ultrasound scanner for remote diagnosis,”\n2015 17th International Conference on E-health Networking', '] S. M. Abd El-Kader and E. M. El-Den Ashmawy, “Non-alcoholic fatty\nliver disease: The diagnosis and management.” World J. Hepatol , Apr.\n2015, vol. 7, pp. 846858. doi: 10.4254/wjh.v7.i6.846.\n[18] P. V ogel, T. Klooster, V . Andrikopoulos and M. Lungu, “A Low-\nEffort Analytics Platform for Visualizing Evolving Flask-Based Python\nWeb Services,” 2017 IEEE Working Conference on Software Visual-\nization (VISSOFT), Shanghai, 2017, pp. 109-113. doi: 10.1109/VIS-\nSOFT.2017.13\n[19] D. S. Reddy, R. Bharath and P. Rajalakshmi, “A Novel Computer-Aided\nDiagnosis Framework Using Deep Learning for Classiﬁcation of Fatty\nLiver Disease in Ultrasound Imaging,” 2018 IEEE 20th International\nConference on e-Health Networking, Applications and Services (Health-\ncom), Ostrava, 2018, pp. 1-5. doi: 10.1109/HealthCom.2018.8531118\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proc. Int. Conf. Learn. Representa-\ntions, 2015.\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n506\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 24,2025 at 21:02:36 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2111.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,Unknown from this paper.,,,,"[10, 14, 5]","['follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut', ' the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVIDUAL COMPONENTS\nFig. 6 shows that the AUC areas of both 2DCNN and 3DCNN\nmodels are below 0.5, and their poor recall performance makes\nthem unsuitable for clinical assessment. Our model has an\nAUC area of up to 0.76, completely enveloping the baseline,\nindicating that our model not only outperforms other models\nbut also performs well at any judgment threshold.\nC. Ablation Study\nIn this section, we conducted ablation experiments on data\naugmentation, attention module, and the number of motion\nselection boxes, and the results are shown in Table II. Here,\nSE–ST represents SE block and STE block in our model,\nand SpaChan represents multihead attention enhancement in\nour model. Both the SpaChan module and SE module. Null\nrepresents that the attention module is not used. K represents\nthe motion selection box threshold.\n1) Data Enhancement Comparison: To demonstrate the\neffectiveness of the frame difference data augmentation and\ncutting method, we plotted a performance comparison graph\nTABLE III\nCOMPUTATIONAL EFFICIENCY AND COMPLEXITY OF\nEACH COMPONENT OF THE MODEL\nFig. 7. Performance comparison of different models before and after data\naugmentation. The model * represents the performance improvement of the\nmodel after data enhancement, and the part with diagonal lines on the column\nrepresents a decrease in numerical values.\nof 2DCNN, 3DCNN, and our model before and after data\naugmentation, as shown in Fig. 7.\nAccording to the survey results in Table II, the performance\nof our model has been comprehensively improved after imple-\nmenting data augmentation techniques. Moreover, in order to\nexhibit the efficacy of the frame difference data augmentation\nand cutting techniques, we illustrate the performance com-\nparison charts of 2DCNN, 3DCNN, and our model pre and\npostdata augmentation, as demonstrated in Fig. 7. It appears\nthat the 2DCNN model is incapable of effectively deriving\nbenefits from data augmentation, owing to the model’s limited\ncapacity for feature mining. Nevertheless, other models display\na substantial improvement as a result of data augmentation.\nThe implementation of data augmentation was found to sig-\nnificantly decrease the loss of 3DCNN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same', ' for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2578.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",HD patients and controls from neurology department.,,,,"[10, 22, 5]","['follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut', ' al., “Comparison of walking protocols and gait\nassessment systems for machine learning-based classification of Parkin-\nson’s disease,” Sensors, vol. 19, no. 24, p. 5363, Dec. 2019.\n[37] R. Li et al., “Fine-grained intoxicated gait classification using a bilinear\nCNN,” IEEE Sensors J., vol. 23, no. 23, pp. 29733–29748, Dec. 2023.\n[38] A. Mühlbäck et al., “Establishing normative data for the evaluation of\ncognitive performance in Huntington’s disease considering the impact\nof gender, age, language, and education,” J. Neurol., vol. 270, no. 10,\npp. 4903–4913, Oct. 2023.\n[39] R. Alkhatib, M. O. Diab, C. Corbier, and M. E. Badaoui, “Machine\nlearning algorithm for gait analysis and classification on early detection\nof Parkinson,” IEEE Sensors Lett., vol. 4, no. 6, pp. 1–4, Jun. 2020.\n[40] G. Vignoud et al., “Video-based automated assessment of movement\nparameters consistent with MDS-UPDRS III in Parkinson’s disease,”\nJ. Parkinson’s Disease, vol. 12, no. 7, pp. 2211–2222, 2022.\n[41] B. Jin, Y . Qu, L. Zhang, and Z. Gao, “Diagnosing Parkinson disease\nthrough facial expression recognition: Video analysis,” J. Med. Internet\nRes., vol. 22, no. 7, Jul. 2020, Art. no. e18697.\n[42] I. C. Duta, L. Liu, F. Zhu, and L. Shao, “Improved residual networks for\nimage and video recognition,” in Proc. 25th Int. Conf. Pattern Recognit.\n(ICPR), Jan. 2021, pp. 9415–9422.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[44] D. Tran, H. Wang, M. Feiszli, and L. Torresani, “Video classification\nwith channel-separated convolutional networks,” in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2019, pp. 5551–5560.\n[45] N. Singla, “Motion detection based on frame difference method,” Int.\nJ. Inf. Comput. Technol., vol. 4, no. 15, pp. 1559–1565, 2014.\n[46] A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in\nProc. 18th Int. Conf. Pattern Recognit. (ICPR), 2006, pp. 850–855.\n[47] A. Tharwat, “Classification assessment methods,” Appl. Comput. Infor-\nmat., vol. 17, no. 1, pp. 168–192, Jan. 2021.\n[48] Y . L. Chang, C. S. Chan, and P. Remagnino, “Action recognition on\ncontinuous video,” Neural Comput. Appl., vol. 33, no. 4, pp. 1233–1243,\nFeb. 2021.\n[49] C. Li, Q. Zhong, D. X', ' for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2573.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Unknown from this paper.,,,,"[10, 7, 16]","['follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut', ' larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res', ' to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2562.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q4,How was the dataset described in this study before predictive modeling was performed?,Multiframe image data before enhancement processing.,,,,"[15, 7, 10]","['NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead', ' larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res', 'follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2566.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?",Training set and test set at a ratio of 7:3.,,,,"[7, 5, 11]","[' larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res', ' for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel', ' s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cutting\nmethods.\na) Frame difference method: the frame difference\nmethod was used [45] for data augmentation, by adding images\nwith significant differences between consecutive frames to the\ndataset. Therefore, high-quality images could be more effec-\ntively used by the model. Specifically, the frame difference\nmethod is a method of obtaining the contour of a mov-\ning object by performing differential operations on adjacent\nframes in a video image sequence. When there is abnor-\nmal object movement in the video, there will be significant\ndifferences between frames. Subtract two frames [I (x, y, t)\nand I (x, y, t − 1)] to obtain the absolute value of the pixel\ndifference [D(x, y, t)] between the two images and determine\nwhether it is greater than the threshold to determine whether\nthere is object motion in the image sequence, as shown in the\nfollowing formula:\nD(x, y, t) = |I (x, y, t) − I (x, y, t − 1)|. (8)\nFirst, we performed median filtering, binarization, and dila-\ntion corrosion on the grayscale image to extract the contour of\nthe object. Second, a motion selection frame was performed\nby comparing the difference D(x, y, t) between the front\nand back frames, the area with a motion difference value\nis greater than the set motion threshold was selected. As\nshown in Fig. 5, using the frame difference method allows\nnot only to distinguish the characters from the background\nbut also to extract the parts of the human body that have\nundergone obvious changes. It should be noted that the motion\nselection boxes will have a large number of overlapping areas.\nIn this study, the representative motion selection boxes were\nselected according to the nonmaximum-suppression (NMS)\nalgorithm [46]. The principle of the NMS algorithm is to retain\nthe detection box with the highest confidence score for the\nsame object among multiple detection boxes while suppressing\nother boxes with lower confidence scores. Finally, according to\nthe relationship between the number of motion-selected boxes\nin the image and the set threshold k, it is determined whether\nthe frame image needs to be copied. According to the ablation\nexperimental results, we determine the value of k to be 1,\nwhich can maximize the gain of model performance.\nb) Image cutting method: After processing by using the\nframe difference method, we could obtain a large amount\nof effective image data, and then, these data were subjected\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 5. Pixel intensity changes in a screenshot of an HD patient’s video with\na 1-s interval between the front and back. The']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2577.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q6,What preprocessing techniques on the included variables/features were applied in this study?,Frame difference method and data enhancement.,,,,"[5, 7, 6]","[' for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel', ' larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res', ' the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel level. Third, to better\nextract the spatial, temporal, and channel features from the\nfeature maps, we employed attention modules to optimize\nthe 3DResNet module. Specifically, we designed two bottle-\nneck modules: squeeze excitation (SE) attention modules and\nspatial–temporal excitation (STE) attention modules. By rea-\nsonably cross-embedding different bottleneck modules into\nthe 3DRes-Net model, our model can learn the interdepen-\ndencies among different dimensions, promote useful features,\nand suppress redundant features. Finally, all features were\nprocessed using a fully connected layer, and a softmax layer\nwas employed to obtain the probability output for each disease\ntype.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 2. Pipeline of the proposed 3-D RAJNet, which mainly consists of an improved residual network block, the spatial–channel attention model, and a\nfeature classifier.\nA. Upstream Feature Extraction Module\n1) Feature Pre-Extraction: For the few-shot learning task\nof HD disease clinical assessment, we must perform the\ndata enhancement and feature pre-extraction to avoid model\noverfitting and improve the generalization and robustness of\nthe model. In addition, since video information often include\nirrelevant background noise, we used the frame difference\nmethod [45] to further process the data. The frame difference\nmethod can prevent the model from learning information that\nis irrelevant to the target and further enhance the effective\ndata information. A detailed data enhancement algorithm is\nintroduced in Section IV.\nIn this module, the RGB images extracted from the orig-\ninal video are first converted into a grayscale map, and the\nframe-difference method is then used for data enhancement.\nGiven input video IM ∈ RT ∗C∗H∗W , our model first cuts the\nsize of the input image by half and then stacks it in the depth\ndimension, getting a processed video IM ∈ RT ∗4C∗H/2∗W/2, to\nachieve a larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2556.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q7,How is missing data handled in this study?,Unknown from this paper.,,,,"[10, 7, 22]","['follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut', ' larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res', ' al., “Comparison of walking protocols and gait\nassessment systems for machine learning-based classification of Parkin-\nson’s disease,” Sensors, vol. 19, no. 24, p. 5363, Dec. 2019.\n[37] R. Li et al., “Fine-grained intoxicated gait classification using a bilinear\nCNN,” IEEE Sensors J., vol. 23, no. 23, pp. 29733–29748, Dec. 2023.\n[38] A. Mühlbäck et al., “Establishing normative data for the evaluation of\ncognitive performance in Huntington’s disease considering the impact\nof gender, age, language, and education,” J. Neurol., vol. 270, no. 10,\npp. 4903–4913, Oct. 2023.\n[39] R. Alkhatib, M. O. Diab, C. Corbier, and M. E. Badaoui, “Machine\nlearning algorithm for gait analysis and classification on early detection\nof Parkinson,” IEEE Sensors Lett., vol. 4, no. 6, pp. 1–4, Jun. 2020.\n[40] G. Vignoud et al., “Video-based automated assessment of movement\nparameters consistent with MDS-UPDRS III in Parkinson’s disease,”\nJ. Parkinson’s Disease, vol. 12, no. 7, pp. 2211–2222, 2022.\n[41] B. Jin, Y . Qu, L. Zhang, and Z. Gao, “Diagnosing Parkinson disease\nthrough facial expression recognition: Video analysis,” J. Med. Internet\nRes., vol. 22, no. 7, Jul. 2020, Art. no. e18697.\n[42] I. C. Duta, L. Liu, F. Zhu, and L. Shao, “Improved residual networks for\nimage and video recognition,” in Proc. 25th Int. Conf. Pattern Recognit.\n(ICPR), Jan. 2021, pp. 9415–9422.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[44] D. Tran, H. Wang, M. Feiszli, and L. Torresani, “Video classification\nwith channel-separated convolutional networks,” in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2019, pp. 5551–5560.\n[45] N. Singla, “Motion detection based on frame difference method,” Int.\nJ. Inf. Comput. Technol., vol. 4, no. 15, pp. 1559–1565, 2014.\n[46] A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in\nProc. 18th Int. Conf. Pattern Recognit. (ICPR), 2006, pp. 850–855.\n[47] A. Tharwat, “Classification assessment methods,” Appl. Comput. Infor-\nmat., vol. 17, no. 1, pp. 168–192, Jan. 2021.\n[48] Y . L. Chang, C. S. Chan, and P. Remagnino, “Action recognition on\ncontinuous video,” Neural Comput. Appl., vol. 33, no. 4, pp. 1233–1243,\nFeb. 2021.\n[49] C. Li, Q. Zhong, D. X']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2552.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[21, 19, 23]","[', P. van Someren, R. A. C. Roos, and\nJ. G. van Dijk, “EEG may serve as a biomarker in Huntington’s disease\nusing machine learning automatic classification,” Sci. Rep., vol. 8, no. 1,\nOct. 2018, Art. no. 16090.\n[29] A. Mengarelli, A. Tigrini, S. Fioretti, and F. Verdini, “Identification of\nneurodegenerative diseases from gait rhythm through time domain and\ntime-dependent spectral descriptors,” IEEE J. Biomed. Health Informat.,\nvol. 26, no. 12, pp. 5974–5982, Dec. 2022.\n[30] J. Klucken et al., “Unbiased and mobile gait analysis detects motor\nimpairment in Parkinson’s disease,” PLoS ONE, vol. 8, no. 2, Feb. 2013,\nArt. no. e56956.\n[31] B. A. MacWilliams, K. L. Carroll, A. K. Stotts, L. M. Kerr, and\nM. H. Schwartz, “Discrimination between hereditary spastic paraplegia\nand cerebral palsy based on gait analysis data: A machine learning\napproach,” Gait Posture, vol. 98, pp. 34–38, Oct. 2022.\n[32] L. van de Venis, B. P. C. van de Warrenburg, V . Weerdesteyn,\nB. J. H. van Lith, A. C. H. Geurts, and J. Nonnekes, “Improving gait\nadaptability in patients with hereditary spastic paraplegia (Move-HSP):\nStudy protocol for a randomized controlled trial,” Trials, vol. 22, no. 1,\npp. 1–10, Dec. 2021.\n[33] Y . Hutabarat, D. Owaki, and M. Hayashibe, “Recent advances in\nquantitative gait analysis using wearable sensors: A review,” IEEE\nSensors J., vol. 21, no. 23, pp. 26470–26487, Dec. 2021.\n[34] S. Moon et al., “Classification of Parkinson’s disease and essential\ntremor based on balance and gait characteristics from wearable motion\nsensors via machine learning techniques: A data-driven approach,”\nJ. NeuroEng. Rehabil., vol. 17, no. 1, pp. 1–8, Dec. 2020.\n[35] C. G. Goetz et al., “Movement disorder society-sponsored revision\nof the unified Parkinson’s disease rating scale (MDS-UPDRS): Scale\npresentation and clinimetric testing results,” Movement Disorders,\nOff. J. Movement Disorder Soc., vol. 23, no. 15, pp. 2129–2170,\n2008.\n[36] R. Z. U. Rehman et al., “Comparison of walking protocols and gait\nassessment systems for machine learning-based classification of Parkin-\nson’s disease,” Sensors, vol. 19, no. 24, p. 5363, Dec. 2019.\n[37] R. Li et al., “Fine-grained intoxicated gait classification using a bilinear\nCNN,” IEEE Sensors J., vol. 23, no. 23, pp. 29733–29748, Dec. 2023.\n[38] A. Mühlbäck et al., “Establishing normative data for the evaluation of\ncognitive performance in Huntington’s disease considering the impact\nof gender, age, language, and education,” J. Neurol., vol. 270, no. 10', ' Eskofier, J. Klucken, and E. Nöth, “Multimodal assessment of\nParkinson’s disease: A deep learning approach,” IEEE J. Biomed. Health\nInformat., vol. 23, no. 4, pp. 1618–1630, Jul. 2019.\n[13] F. D. Acosta-Escalante, E. Beltrán-Naturi, M. C. Boll,\nJ. A. Hernández-Nolasco, and P. Pancardo García, “Meta-classifiers\nin Huntington’s disease patients classification, using iPhone’s\nmovement sensors placed at the ankles,” IEEE Access, vol. 6,\npp. 30942–30957, 2018.\n[14] A. Mannini, D. Trojaniello, A. Cereatti, and A. Sabatini, “A machine\nlearning framework for gait classification using inertial sensors: Applica-\ntion to elderly, post-stroke and Huntington’s disease patients,” Sensors,\nvol. 16, no. 1, p. 134, Jan. 2016.\n[15] Y .-W. Ma, J.-L. Chen, Y .-J. Chen, and Y .-H. Lai, “Explainable deep\nlearning architecture for early diagnosis of Parkinson’s disease,” Soft\nComput., vol. 27, no. 5, pp. 2729–2738, Mar. 2023.\n[16] A. Shcherbak, E. Kovalenko, and A. Somov, “Detection and classifica-\ntion of early stages of Parkinson’s disease through wearable sensors and\nmachine learning,” IEEE Trans. Instrum. Meas., vol. 72, pp. 1–9, 2023.\n[17] A. Talitckii et al., “Comparative study of wearable sensors, video, and\nhandwriting to detect Parkinson’s disease,” IEEE Trans. Instrum. Meas.,\nvol. 71, pp. 1–10, 2022.\n[18] A. Lauraitis, R. Maskeliunas, R. Damaševicius, D. Polap, and\nM. Wozniak, “A smartphone application for automated decision support\nin cognitive task based evaluation of central nervous system motor disor-\nders,” IEEE J. Biomed. Health Informat., vol. 23, no. 5, pp. 1865–1876,\nSep. 2019.\n[19] R. Riad et al., “Predicting clinical scores in Huntington’s disease:\nA lightweight speech test,” J. Neurol., vol. 269, no. 9, pp. 5008–5021,\n2022.\n[20] Z. Chang et al., “Accurate detection of cerebellar smooth pursuit eye\nmovement abnormalities via mobile phone video and machine learning,”\nSci. Rep., vol. 10, no. 1, Oct. 2020, Art. no. 18641.\n[21] W. Huang, W. Xu, R. Wan, P. Zhang, Y . Zha, and M. Pang, “Auto\ndiagnosis of Parkinson’s disease via a deep learning model based on\nmixed emotional facial expressions,” IEEE J. Biomed. Health Informat.,\nvol. 28, no. 5, pp. 2547–2557, May 2024.\n[22] Z. Zha, H. Tang, Y . Sun, and J. Tang, “Boosting few-shot fine-grained\nrecognition with background suppression and foreground alignment,”\nIEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 8, pp. 3947–3961,\nAug.', ' “Efficient non-maximum suppression,” in\nProc. 18th Int. Conf. Pattern Recognit. (ICPR), 2006, pp. 850–855.\n[47] A. Tharwat, “Classification assessment methods,” Appl. Comput. Infor-\nmat., vol. 17, no. 1, pp. 168–192, Jan. 2021.\n[48] Y . L. Chang, C. S. Chan, and P. Remagnino, “Action recognition on\ncontinuous video,” Neural Comput. Appl., vol. 33, no. 4, pp. 1233–1243,\nFeb. 2021.\n[49] C. Li, Q. Zhong, D. Xie, and S. Pu, “Collaborative spatiotemporal\nfeature learning for video action recognition,” in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 7864–7873.\n[50] S. Maddury, “Automated Huntington’s disease prognosis via biomedical\nsignals and shallow machine learning,” 2023, arXiv:2302.03605.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2037.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q9,Which prediction models were used in this study?,"RAJNet, 3-D ResNet-based backbone network",,,,"[5, 15, 13]","[' for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel', 'NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead', '-end assessment; and c) low cost\nand convenience.\n1) Comparison With Other Models:First, the main meth-\nods for evaluating HD previously included scale analysis,\nfacial muscle group analysis model, and gait analysis for\ncomparison. Their shortcomings are very obvious. The scale\nanalysis uses the UHDRS diagnostic confidence level (DCL),\nwhich is based on the evaluation of motor signs is relatively\nlimited. The facial muscle group analysis uses methods, such\nas coordinate analysis and SVM, to analyze the amplitude\ncharacteristics of facial expressions and the shaking of facial\nsmall muscle groups. It is a non-end-to-end model with weak\ngeneralization ability. The gait analysis measures the severity\nof a patient’s illness based on high-level gait characteristics,\nrequiring patients to wear specialized pressure sensor equip-\nment. This is not only inconvenient for HD patients in remote\nareas but also has a relatively high cost. However, our model is\nan end-to-end 3-D multihead attention residual network, which\neliminates the inconvenience of wearing specialized pressure\nsensor devices to obtain corresponding data in gait analysis\nmodels. We also integrate many effective modules into our\nproposed model, such as data enhancement based on the frame\ndifference method and multihead attention module. Therefore,\nour model can achieve higher accuracy more conveniently and\nefficiently.\nSecond, as shown in Table I, the performance of our\nmodel is far better than classic models, such as 2DCNN\nand 3DCNN in terms of accuracy, recall, accuracy, and loss\nvalue performance. Given that the 2DCNN and 3DCNN model\nnetworks are relatively simple and difficult to extract effective\ninformation, they are lack of specificity analysis in diagnosing\nHD. After adding a specific attention module that captures\npatient features to our model, our model has significantly\nimproved the data utilization.\nThird, compared with the performance of the HD diagnostic\nmodels based on near-infrared spectroscopy data in Table I,\nour model has the highest accuracy, followed by LDA [50],\nQDA [50], Log.Regr. [50], and SVM [50]. Meanwhile, our\nmodel demonstrates significantly higher accuracy compared\nto the second-ranked LDA model. In addition, the acquisition\nof near-infrared spectroscopy data requires complex operations\nwhich may lead to poor repeatability. Moreover, these models\nrequire many training iterations to obtain satisfactory results,\nwhich places high demands on computer performance. Our\nmodel, by contrast, is end-to-end and can be operated by\nsimply obtaining images or videos of HD patients. Most\nimportantly, our model can achieve excellent performance with\na small amount of epoch training.\n2) ROC Curve Comparison:In order to better demonstrate\nthe high recall and accuracy performance of our model,\nwe plotted the receiver operating characteristic (ROC) curves\nof 2DCNN, 3DCNN, and our model in Fig. 6. The ROC\ncurve is of great significance in measuring the performance\nof binary classification models under different thresholds. The\nx-axis represents the false positive rate, the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVID']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2568.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,Motion selection box threshold and attention modules.,,,,"[16, 15, 8]","[' to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA', 'NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead', '  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the ResNet module, the network changes the\nlearning objective. Rather learning a complete output H(x),\nit only learns the difference between output H(x) and input\nx (the residual).\nTo extract deep information from few-shot data, we used\nan extended basic residual module as the basic block of the\nbackbone (bottleneck). As shown in (5), the basic residual\nmodule first receives the input information into two layers of\n3-D convolution Conv3\nB with a kernel size of three to process\nthe feature map Ft+1 and then adds the output value with the\noriginal input elementwise\nFt+4 = ReLU\n(\nConv3\nB\n(\nConv3\nB (Ft+3) ⊕ Ft+3\n))\n. (5)\nCompared with the basic residual module, the improved bot-\ntleneck prioritizes refined feature extraction. This is achieved\nby adding two further 3-D convolutions, each featuring a\none-unit convolutional kernel. This approach facilitates the\nextraction of more detailed information and expands the\nfeature space during the small sample learning processes.\nMoreover, we utilized the SE-block and STE-block modules\nto enhance the attention and increase the robustness of the\nbottleneck module. The intricate structure of the attention\nmodule is presented in Section III-C.\nC. Attention-Based Optimization Modules\nFor pose recognition and classification of HD patients,\nwe use attention mechanisms to focus on local features.\nAlthough traditional ResNet networks can obtain promising\nresults by stacking multilayer network structures, they rarely\nconsider how to determine the subtle differences in body\nmovements between patients and person without Huntington’s\ndisease. Their models are not suitable for image classifica-\ntion of this scene. Therefore, focusing on important feature\ninformation and preventing deep learning from overfitting are\nFig. 4. (a) SE module learns channel importance via global feature\naggregation. (b) STE module incorporates spatial–temporal information with\na channel average operation.\ncritical for HD severity assessment tasks. To achieve a better\nperformance, the spatiotemporal and channel characteristics of\nthe data should be extracted and analyzed comprehensively. In\nthis study, we designed two different ResNet bottlenecks by\ncombining attention to spatial–temporal and channel features\nand stacked them in the layers in rotation, which could\neffectively guarantee the robustness and stability of our model.\n1) SE Block: As shown in Fig. 4(a), as a kind of channel\nexcitation, the SE block can explicitly model the interde-\npendencies between feature channels. It can access global\ninformation and determine the importance of each feature\nchannel in two steps. This allows the network to promote\nuseful features and suppress irrelevant ones, enhancing its\nsensitivity to informative features.\nThe squeeze operation performs feature compression along\nthe spatial dimension, turning each 2-D feature channel into a\nreal number that somehow has a global perceptual field. The\noutput dimension matches the number of input feature chan-\nnels. It characterizes the global distribution of the response\nover the feature channels and makes the']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2559.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q11,How was data augmentation or generation used in this study?,Frame difference and image-cutting methods.,,,,"[11, 8, 7]","[' s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cutting\nmethods.\na) Frame difference method: the frame difference\nmethod was used [45] for data augmentation, by adding images\nwith significant differences between consecutive frames to the\ndataset. Therefore, high-quality images could be more effec-\ntively used by the model. Specifically, the frame difference\nmethod is a method of obtaining the contour of a mov-\ning object by performing differential operations on adjacent\nframes in a video image sequence. When there is abnor-\nmal object movement in the video, there will be significant\ndifferences between frames. Subtract two frames [I (x, y, t)\nand I (x, y, t − 1)] to obtain the absolute value of the pixel\ndifference [D(x, y, t)] between the two images and determine\nwhether it is greater than the threshold to determine whether\nthere is object motion in the image sequence, as shown in the\nfollowing formula:\nD(x, y, t) = |I (x, y, t) − I (x, y, t − 1)|. (8)\nFirst, we performed median filtering, binarization, and dila-\ntion corrosion on the grayscale image to extract the contour of\nthe object. Second, a motion selection frame was performed\nby comparing the difference D(x, y, t) between the front\nand back frames, the area with a motion difference value\nis greater than the set motion threshold was selected. As\nshown in Fig. 5, using the frame difference method allows\nnot only to distinguish the characters from the background\nbut also to extract the parts of the human body that have\nundergone obvious changes. It should be noted that the motion\nselection boxes will have a large number of overlapping areas.\nIn this study, the representative motion selection boxes were\nselected according to the nonmaximum-suppression (NMS)\nalgorithm [46]. The principle of the NMS algorithm is to retain\nthe detection box with the highest confidence score for the\nsame object among multiple detection boxes while suppressing\nother boxes with lower confidence scores. Finally, according to\nthe relationship between the number of motion-selected boxes\nin the image and the set threshold k, it is determined whether\nthe frame image needs to be copied. According to the ablation\nexperimental results, we determine the value of k to be 1,\nwhich can maximize the gain of model performance.\nb) Image cutting method: After processing by using the\nframe difference method, we could obtain a large amount\nof effective image data, and then, these data were subjected\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 5. Pixel intensity changes in a screenshot of an HD patient’s video with\na 1-s interval between the front and back. The', '  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the ResNet module, the network changes the\nlearning objective. Rather learning a complete output H(x),\nit only learns the difference between output H(x) and input\nx (the residual).\nTo extract deep information from few-shot data, we used\nan extended basic residual module as the basic block of the\nbackbone (bottleneck). As shown in (5), the basic residual\nmodule first receives the input information into two layers of\n3-D convolution Conv3\nB with a kernel size of three to process\nthe feature map Ft+1 and then adds the output value with the\noriginal input elementwise\nFt+4 = ReLU\n(\nConv3\nB\n(\nConv3\nB (Ft+3) ⊕ Ft+3\n))\n. (5)\nCompared with the basic residual module, the improved bot-\ntleneck prioritizes refined feature extraction. This is achieved\nby adding two further 3-D convolutions, each featuring a\none-unit convolutional kernel. This approach facilitates the\nextraction of more detailed information and expands the\nfeature space during the small sample learning processes.\nMoreover, we utilized the SE-block and STE-block modules\nto enhance the attention and increase the robustness of the\nbottleneck module. The intricate structure of the attention\nmodule is presented in Section III-C.\nC. Attention-Based Optimization Modules\nFor pose recognition and classification of HD patients,\nwe use attention mechanisms to focus on local features.\nAlthough traditional ResNet networks can obtain promising\nresults by stacking multilayer network structures, they rarely\nconsider how to determine the subtle differences in body\nmovements between patients and person without Huntington’s\ndisease. Their models are not suitable for image classifica-\ntion of this scene. Therefore, focusing on important feature\ninformation and preventing deep learning from overfitting are\nFig. 4. (a) SE module learns channel importance via global feature\naggregation. (b) STE module incorporates spatial–temporal information with\na channel average operation.\ncritical for HD severity assessment tasks. To achieve a better\nperformance, the spatiotemporal and channel characteristics of\nthe data should be extracted and analyzed comprehensively. In\nthis study, we designed two different ResNet bottlenecks by\ncombining attention to spatial–temporal and channel features\nand stacked them in the layers in rotation, which could\neffectively guarantee the robustness and stability of our model.\n1) SE Block: As shown in Fig. 4(a), as a kind of channel\nexcitation, the SE block can explicitly model the interde-\npendencies between feature channels. It can access global\ninformation and determine the importance of each feature\nchannel in two steps. This allows the network to promote\nuseful features and suppress irrelevant ones, enhancing its\nsensitivity to informative features.\nThe squeeze operation performs feature compression along\nthe spatial dimension, turning each 2-D feature channel into a\nreal number that somehow has a global perceptual field. The\noutput dimension matches the number of input feature chan-\nnels. It characterizes the global distribution of the response\nover the feature channels and makes the', ' larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64\nF (IM )\n)\n(1)\nwhere Conv 64\nF represents a convolutional layer with three\ninputs and 64 output channels, N() represents a standardized\noperation, and Ft represents the 3-D feature map in each tth\nstate of model processing. After a layer of 3-D convolution,\na layer of activation (ReLU) and normalization (batch normal-\nization) are also applied as follows:\nFt = (BN(Relu(Ft ))). (2)\n2) Multihead Attention Enhancement: Subsequently, we\nleverage on multihead attention to extract nonredundant infor-\nmation in feature maps Ft more efficiently. Specifically,\nwe built two types of attention-learning modules: channel and\nspatial attention.\na) Channel attention mechanism: As shown in Fig. 3,\nchannel attention focuses on the interactions between different\nchannels. First, we extract the background information from\nFt+1 through the average pooling layer. We then leveraged\non a channel-compressed convolutional layer and an ReLU\nfunction for scaling. Finally, a channel-expanded convolutional\nlayer was used to restore the original number of channels.\nThe output of the channel-expanded convolutional layer is\nrepresented as the average pooled channel-attention-weighted\nWC\nA .\nThe channel attention based on max pooling replaces only\nthe first layer with max pooling to obtain weight WC\nM . As a\nresult, the processing of our channel attention module can be\nsummarized as follows:\nFt+2 =\n(\nWC\nA + WC\nM\n)\n∗ Ft+1. (3)\nb) Spatial attention mechanism: Unlike channel atten-\ntion, spatial attention focuses on the most effective feature\nmap information. The feature maps Ft+3 first pass through\nthe average-pooling and max-pooling layers and are then\nconcatenated to obtain the fused features. Finally, a convo-\nlutional operation was leveraged to generate the final spatial\nattention feature weights. This process can be summarized as\nfollows:\nFt+3 = Conv3\nS((Max(Ft+2) ⊕ Mean(Ft+2))) ∗ Ft+2 (4)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 3. Illustration of spatial–channel attention module. The channel attention\nmodule focuses on important semantic classes, while the spatial attention\nmodule models context dependencies.\nwhere Conv3\nS represents the 3-D convolutional layer with two\ninputs and one output channel and ⊕ represents the operation\nof concatenating.\nB. Backbone Model\nFor the backbone model, we used the 3-D ResNet module,\nwhich consisted of four stacked layers with 3, 4, 23, and 3 bot-\ntleneck layers. In the Res']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2548.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared to baseline and other models.",,,,"[14, 5, 16]","[' the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVIDUAL COMPONENTS\nFig. 6 shows that the AUC areas of both 2DCNN and 3DCNN\nmodels are below 0.5, and their poor recall performance makes\nthem unsuitable for clinical assessment. Our model has an\nAUC area of up to 0.76, completely enveloping the baseline,\nindicating that our model not only outperforms other models\nbut also performs well at any judgment threshold.\nC. Ablation Study\nIn this section, we conducted ablation experiments on data\naugmentation, attention module, and the number of motion\nselection boxes, and the results are shown in Table II. Here,\nSE–ST represents SE block and STE block in our model,\nand SpaChan represents multihead attention enhancement in\nour model. Both the SpaChan module and SE module. Null\nrepresents that the attention module is not used. K represents\nthe motion selection box threshold.\n1) Data Enhancement Comparison: To demonstrate the\neffectiveness of the frame difference data augmentation and\ncutting method, we plotted a performance comparison graph\nTABLE III\nCOMPUTATIONAL EFFICIENCY AND COMPLEXITY OF\nEACH COMPONENT OF THE MODEL\nFig. 7. Performance comparison of different models before and after data\naugmentation. The model * represents the performance improvement of the\nmodel after data enhancement, and the part with diagonal lines on the column\nrepresents a decrease in numerical values.\nof 2DCNN, 3DCNN, and our model before and after data\naugmentation, as shown in Fig. 7.\nAccording to the survey results in Table II, the performance\nof our model has been comprehensively improved after imple-\nmenting data augmentation techniques. Moreover, in order to\nexhibit the efficacy of the frame difference data augmentation\nand cutting techniques, we illustrate the performance com-\nparison charts of 2DCNN, 3DCNN, and our model pre and\npostdata augmentation, as demonstrated in Fig. 7. It appears\nthat the 2DCNN model is incapable of effectively deriving\nbenefits from data augmentation, owing to the model’s limited\ncapacity for feature mining. Nevertheless, other models display\na substantial improvement as a result of data augmentation.\nThe implementation of data augmentation was found to sig-\nnificantly decrease the loss of 3DCNN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same', ' for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through gait\nanalysis [36], thereby showing great potential as a clinical\ntool for movement disorders.\nC. Machine Learning-Based Model\nArtificial intelligence models are used in the clinical\nassessment of ND diseases [37], [38], [39]. In addition to\nthe above-mentioned biomarker-based models and gait-based\nmodels that use artificial intelligence models, audio and\nvideo-based diagnostic models have recently emerged. Given\nthat these audio and video data are often easier and more direct\nto obtain, audio-based and video-based diagnostic models can\ngenerate higher diagnostic accuracy.\nFor example, in a recent study, researchers first extracted\n60 speech features from voice samples of in-affected and\npresymptomatic gene carriers. After training the model in a\nsample 51, they could predict the clinical HD characteristics in\nthe independent set [19]. These variables were associated with\nthe standard grades for training according to the rating scale.\nBy doing so, an accurate prediction model can be obtained.\nSimilarly, a video-based machine learning model has been\ndeveloped to analyze the relationship between hand motion\nand MDS-UPDRS III score [40]. This study analyzed 272 PD\nand non-PD hand movement videos with DeepLabCut was\nused to identify joint points of the joints in the 2-D images\nof the video. HandGraphCNN was then applied to predict\nthe 2-D and 3-D point positions of points. Finally, linear\nregression (LR) and DT were used to analyze finger tapping\nand hand movements, and pronation-supination movements of\nthe hands to get a highly significant correlation of the result\nwith the MDS-UPDRS scores. A clinical assessment model\nbased on facial landmark [41] has used face feature extractors\nto get key points of a facial landmark of 176 video recordings\nfrom 33 PD patients and 31 HCs. Machine learning algorithms,\nsuch as LR, SVM, DT, RF, and LSTM, were used to analyze\nthe variation of the relative coordinates variation to distinguish\nbetween patients and HCs.\nIII. O UR SOLUTION\nThis section describes our 3-D residual and attention\nenhancement joint network (RAJNet) in detail. Unlike large\ndataset available in [42] and [43], the scarcity of medical data\nsignificantly limits the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel', ' to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2573.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q13,Which type of explainability techniques are used?,SE and STE attention modules,,,,"[9, 6, 17]","[', which could\neffectively guarantee the robustness and stability of our model.\n1) SE Block: As shown in Fig. 4(a), as a kind of channel\nexcitation, the SE block can explicitly model the interde-\npendencies between feature channels. It can access global\ninformation and determine the importance of each feature\nchannel in two steps. This allows the network to promote\nuseful features and suppress irrelevant ones, enhancing its\nsensitivity to informative features.\nThe squeeze operation performs feature compression along\nthe spatial dimension, turning each 2-D feature channel into a\nreal number that somehow has a global perceptual field. The\noutput dimension matches the number of input feature chan-\nnels. It characterizes the global distribution of the response\nover the feature channels and makes the global perceptual field\navailable to the layers close to the input. This was accom-\nplished by performing global average pooling (GPooling) on\nthe original feature map C ∗W ∗H and obtaining a feature map\nof size 1∗1∗C with a global perceptual field. The processing of\nour channel attention module can be summarized as follows:\nFt+5 = Sig(FC(Relu(FC(GPooling(Ft+4)))) ∗ Ft+4). (6)\n2) STE Block: As shown in Fig. 4(b), the STE is another\nmodule that extracts spatiotemporal features from videos\nby generating a spatiotemporal attention map. Traditional\nspatial–temporal feature extraction mainly uses 3-D convo-\nlution; however, introducing 3-D convolution directly to the\ninput significantly increases the computational effort of the\nmodel. We first use a channel average on Ft+4 to get a\nglobal channel feature F′\nt+4 ∈ RN∗T ∗1∗H∗W , and then, F′\nt+4\nis reshaped into dimensions that can be manipulated by 3-D\nconvolution, i.e. (N, 1, T, H, W ), where N represents the\nbatch size, T denotes the data time dimension, the channel\ndimension C is compressed to 1, and H ∗ W denotes the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\ndata height and width, respectively. Subsequently, we use a\nconvolutional layer to extract the attention map from F′\nt+4.\nThis attention map is multiplied by Ft+4 to obtain the final\nspatial–temporal features Ft+5.\n3) Learning Strategy: In the final part of the model,\nwe leverage multihead attention to enhance the model’s classi-\nfication ability. Subsequently, we used a fully connected layer\nand a softmax function to complete the disease probability\noutput of the model. This process can be summarized as\nfollows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were ', ' the performance of deep learning models.\nTo extract HD features from limited data, our model integrates\nmore effective attention modules [44], allowing it to perform\nbetter on HD evaluation tasks.\nAs shown in Fig. 2, our solution includes three steps. First,\nwe design an upstream feature extraction module, where input\nvideo samples receive data enhancement and feature extraction\n(Fi ) using the frame difference method and convolutional\nnetwork module, respectively. Subsequently, multihead atten-\ntion, including channel and spatial attention, is leveraged to\nimprove the model’s generalization and robustness. Second,\nwe used a 3-D ResNet-based backbone network module to\ninitially collaboratively learn spatial–temporal characteristics\nand then capture the correspondence between the features\nof the two modalities at the pixel level. Third, to better\nextract the spatial, temporal, and channel features from the\nfeature maps, we employed attention modules to optimize\nthe 3DResNet module. Specifically, we designed two bottle-\nneck modules: squeeze excitation (SE) attention modules and\nspatial–temporal excitation (STE) attention modules. By rea-\nsonably cross-embedding different bottleneck modules into\nthe 3DRes-Net model, our model can learn the interdepen-\ndencies among different dimensions, promote useful features,\nand suppress redundant features. Finally, all features were\nprocessed using a fully connected layer, and a softmax layer\nwas employed to obtain the probability output for each disease\ntype.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 2. Pipeline of the proposed 3-D RAJNet, which mainly consists of an improved residual network block, the spatial–channel attention model, and a\nfeature classifier.\nA. Upstream Feature Extraction Module\n1) Feature Pre-Extraction: For the few-shot learning task\nof HD disease clinical assessment, we must perform the\ndata enhancement and feature pre-extraction to avoid model\noverfitting and improve the generalization and robustness of\nthe model. In addition, since video information often include\nirrelevant background noise, we used the frame difference\nmethod [45] to further process the data. The frame difference\nmethod can prevent the model from learning information that\nis irrelevant to the target and further enhance the effective\ndata information. A detailed data enhancement algorithm is\nintroduced in Section IV.\nIn this module, the RGB images extracted from the orig-\ninal video are first converted into a grayscale map, and the\nframe-difference method is then used for data enhancement.\nGiven input video IM ∈ RT ∗C∗H∗W , our model first cuts the\nsize of the input image by half and then stacks it in the depth\ndimension, getting a processed video IM ∈ RT ∗4C∗H/2∗W/2, to\nachieve a larger batch size. Here, IM ∈ RT ∗C∗H∗W represents\nthe multiframe image data before data enhancement process-\ning. T denotes the data time dimension, H ∗ W denote the\ndata height and width, C is the number of channels, and 4 C\nrepresents the stacked cut images in the channel dimension. In\nour experiment, T was set to 10, and both H and W were 512.\nWe then leverage the frame difference method to initially\nextract action features, which are presented in Section IV.\nAfter data enhancement processing, we first used a layer\nof the 3-D convolution module to perform preliminary feature\nextraction on the data, as shown in the following equation:\nFt = N\n(\nConv64', ' Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA, and other models, our model has achieved better perfor-\nmance in the motor assessment of HD diseases. In particular,\nour model can obtain more reliable clinical assessment results\nthan typical video classification models. Based on this fact,\nthe superiority of the model mainly depends on the following\nreasons.\n1) The high-definition motor assessment model we\ndesigned is a small-sample learning model, which effec-\ntively utilizes data processing models such as data\naugmentation and frame difference and enriches and\nexpands effective features.\n2) The spatial–temporal attention module designed in this\narticle enables the model to filter out the redundant\nfeatures in the data well, and effectively capture the\nregions with large patient variation intensity.\n3) The SE and STE modules we use effectively enhance\nthe HD motor signs recognition ability of the base\nResNet module, improving the assessment performance\nand stability of the model.\nThe model’s current inadequacies lie in the insufficiency of\ndata for each stage of HD patients, and the model’s inability\nto determine disease stage the patient is at. In addition, the\nuse of multiple data enhancement algorithms has resulted in a\nlack of generalization in the model. In the future, we plan to\nincorporate patient voices, self-assessment scale, patient video\ndata, and patient genetic data for multimodal diagnosis to\nimprove the model’s interpretability and diagnostic reliability.\nMoreover, future work will examine further aspects of the\nmodel, including testing at different stages of the disease and\nduring the course in single individuals, Furthermore, it will\nallow motor phenotype comparison with other ethnic groups\nin a better way than the instruments used so far. Features\nrecognized in this way will also allow comparison with other\naspects of the phenotype, including other clinical features,\nas well as imaging and laboratory biomarkers. The source code\ncan be found at: https://github.com/JackAILab/RAJNet.\nACKNOWLEDGMENT\nThe authors thank the anonymous reviewers for their\nvaluable suggestions.\nREFERENCES\n[1] G. P. Bates et al., “Huntington disease,” Nature Rev. Disease Primers,\nvol. 1, no. 1, pp. 1–21, Apr. 2015.\n[2] P. Dayalu and R. L. Albin, “Huntington disease: Pathogenesis and\ntreatment,” Neurologic clinics, vol. 33, no. 1, pp. 101–114, 2015.\n[3] S. J. Tabrizi et al., “A biological classification of Huntington’s dis-\nease: The integrated staging system,” Lancet Neurol., vol. 21, no. 7,\npp. 632–644, Jul. 2022.\n[4] C. A. Ross et al., “Movement disorder society task force viewpoint:\nHuntington’s disease diagnostic categories,” Movement Disorders Clin.\nPract., vol. 6, no. 7, pp. 541–546, Sep. 2019.\n[5] H.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2539.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,"Accuracy, recall, ROC curves, AUC.",,,,"[13, 3, 4]","['-end assessment; and c) low cost\nand convenience.\n1) Comparison With Other Models:First, the main meth-\nods for evaluating HD previously included scale analysis,\nfacial muscle group analysis model, and gait analysis for\ncomparison. Their shortcomings are very obvious. The scale\nanalysis uses the UHDRS diagnostic confidence level (DCL),\nwhich is based on the evaluation of motor signs is relatively\nlimited. The facial muscle group analysis uses methods, such\nas coordinate analysis and SVM, to analyze the amplitude\ncharacteristics of facial expressions and the shaking of facial\nsmall muscle groups. It is a non-end-to-end model with weak\ngeneralization ability. The gait analysis measures the severity\nof a patient’s illness based on high-level gait characteristics,\nrequiring patients to wear specialized pressure sensor equip-\nment. This is not only inconvenient for HD patients in remote\nareas but also has a relatively high cost. However, our model is\nan end-to-end 3-D multihead attention residual network, which\neliminates the inconvenience of wearing specialized pressure\nsensor devices to obtain corresponding data in gait analysis\nmodels. We also integrate many effective modules into our\nproposed model, such as data enhancement based on the frame\ndifference method and multihead attention module. Therefore,\nour model can achieve higher accuracy more conveniently and\nefficiently.\nSecond, as shown in Table I, the performance of our\nmodel is far better than classic models, such as 2DCNN\nand 3DCNN in terms of accuracy, recall, accuracy, and loss\nvalue performance. Given that the 2DCNN and 3DCNN model\nnetworks are relatively simple and difficult to extract effective\ninformation, they are lack of specificity analysis in diagnosing\nHD. After adding a specific attention module that captures\npatient features to our model, our model has significantly\nimproved the data utilization.\nThird, compared with the performance of the HD diagnostic\nmodels based on near-infrared spectroscopy data in Table I,\nour model has the highest accuracy, followed by LDA [50],\nQDA [50], Log.Regr. [50], and SVM [50]. Meanwhile, our\nmodel demonstrates significantly higher accuracy compared\nto the second-ranked LDA model. In addition, the acquisition\nof near-infrared spectroscopy data requires complex operations\nwhich may lead to poor repeatability. Moreover, these models\nrequire many training iterations to obtain satisfactory results,\nwhich places high demands on computer performance. Our\nmodel, by contrast, is end-to-end and can be operated by\nsimply obtaining images or videos of HD patients. Most\nimportantly, our model can achieve excellent performance with\na small amount of epoch training.\n2) ROC Curve Comparison:In order to better demonstrate\nthe high recall and accuracy performance of our model,\nwe plotted the receiver operating characteristic (ROC) curves\nof 2DCNN, 3DCNN, and our model in Fig. 6. The ROC\ncurve is of great significance in measuring the performance\nof binary classification models under different thresholds. The\nx-axis represents the false positive rate, the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVID', ' However,\nshortcomings are to be mentioned. Clinical scoring scales [4],\n[5] require well-trained neurologists to obtain reliable data, and\nthey are not easily adaptable for remote clinical assessment\n1) Although the reliability of biological marker-based [3]\ndiagnostic methods is good, these methods require\nexpensive instruments with inherent issues to be used\non a large scale. Use of wearable devices [7], [8] can\ngenerate data at a low cost. However, this method is a\ncontact method, which is not convenient for long-term\ntracking and analysis of patients, and the probability of\ninhomogeneous assessment is relatively high.\n2) Previously, several machine learning models [13], [17]\nhave been used in HD assessment, including the anal-\nysis models of wearable devices and voice. However,\nsome obvious defects hinder their application in clinical\npractice. For example, voice-based methods need to first\nextract voice features and then use statistics or machine\nlearning models for analysis. This two-stage method\nmay lead to unstable analysis results.\n3) HD is rare disease and the prevalence is low as 0.38 per\n100 000 person-years. Therefore, it is difficult to obtain\na large number of HD video data for traditional machine\nlearning models. Indeed, data scarcity usually results\nin unstable analysis of traditional machine learning\nmodel [22]. How to obtain accurate assessment results\nthrough few-shot data is very important to model devel-\nopment and clinical application.\nBased on the above considerations, we have developed\nan end-to-end few-shot video analysis model. The proposed\nFig. 1. Comparison of six pose examples of HD patients with normal poses,\nincluding: 1—upper bodies, 2—sitting, 3—front-facing standing, 4—side-fac-\ning standing, 5—back-facing standing, and 6—walking. To avoid revealing the\nidentity of the patients, we removed the eye region from their face images.\nmodel utilize facial details and body posture details [9], [23]\nto perform analysis, as shown in Fig. 1. Specifically, we first\ndesign a frame difference method to perform data enhancement\non some key video data. Second, we have embedded two\ntypes of effective attention modules into the 3-D convolutional\nnetwork to enhance the ability of the deep learning model\nto extract HD video features. Finally, we have employed the\nresidual structure to repeatedly stack the extracted features and\nthen use a fully connected layer to obtain the final diagnosis\nresult. Therefore, the main contributions of this article are\nsummarized as follows.\n1) We have developed an effective end-to-end high-\ndefinition diagnosis model based on posture videos.\nCompared with using gait or biomarker data, posture\nvideo-based data are more convenient and can provide\nreliable diagnostic results.\n2) Both spatial–temporal excitation attention and spatial–\nchannel attention modules have been incorporated into\nthe 3-D residual network to enhance its feature extrac-\ntion capabilities. In comparison with other machine\nlearning approaches, superior and more consistent per-\nformance is demonstrated by this architecture.\n3) We have designed a frame difference algorithm to\nimprove the video data through the data enhancement.\nThis new method can overcome issues related to small\nsample size and improve outcomes in this few-shot\nlearning task, thereby providing reliable assessment\noutcomes. Compared with other Huntington evaluation\nmodels, experiments confirm that our model can achieve\nbetter recall and accuracy.\nOverall, HD is a rare genetic disorder with a wide spectrum\nof signs and symptoms. There is an urgent need to develop\nan automatic assessment of HD in clinical practice. Several\nassessments have been developed including the analysis mod-\nels of clinical scoring scales, biological markers and wearable\ndevices, image, and voice. Although the above models can\nobtain promising results, some obvious defects hinder their', ' demonstrated by this architecture.\n3) We have designed a frame difference algorithm to\nimprove the video data through the data enhancement.\nThis new method can overcome issues related to small\nsample size and improve outcomes in this few-shot\nlearning task, thereby providing reliable assessment\noutcomes. Compared with other Huntington evaluation\nmodels, experiments confirm that our model can achieve\nbetter recall and accuracy.\nOverall, HD is a rare genetic disorder with a wide spectrum\nof signs and symptoms. There is an urgent need to develop\nan automatic assessment of HD in clinical practice. Several\nassessments have been developed including the analysis mod-\nels of clinical scoring scales, biological markers and wearable\ndevices, image, and voice. Although the above models can\nobtain promising results, some obvious defects hinder their\napplication in clinical practice. In the present study, we have\ndeveloped an effective proficient pose video-based end-to-end\ndiagnostic model for HD. This new method can overcome\nissues related to few-shot and improve outcomes in this\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nfew-shot learning task, thereby providing dependable and\noutstanding diagnostic outcomes. The remainder of this article\nis organized as follows. Section II introduces recent work\nrelated to HD diagnosis. Section III elaborates on the proposed\nframework, followed by extensive experimental results and\nanalysis in Section IV. Last, the conclusion is drawn in\nSection V.\nII. R ELATED WORKS\nA. Biological Markers-Based Model\nBiomarkers are very common detection methods in the clin-\nical assessment of HD [24]. Currently, several new biomarkers\nhave been developed [3] to detect symptoms of ND diseases.\nFor example, blood-based assessments have been reported as\npossible biomarkers for the evaluation of HD [25]. Given that\nlipid composition and disturbances in lipoprotein metabolism\nmay be associated with the pathogenesis of HD, the plasma\nlipoprotein profile has been proposed as predictive biomark-\ners for HD progression [3]. In addition, MRI [26] and\nEEG [27], [28] have been explored biomarkers for HD. Based\non EEG features associated with disease progression in HD,\nan automatic classifier has been developed to distinguish\nhealthy controls (HCs) from HD gene carriers. Similarly, MRI-\nbased [26] imaging biomarkers have been used to determine\nthe degree of morphological changes.\nB. Wearable Device-Based Clinical Assessment Model\nUnlike biomarker-based methods, wearable device-based\nmethods are less costly and more straightforward measure-\nments for disease evaluation. Indeed, wearable or portable\nsensors have been shown to be effective in HD signs assess-\nment signs [7], [29]. Based on the data collected by sensors,\nsuch as stride frequency, stride length, and walking speed,\nseveral algorithms have been used for classifier training to\ndetermine the progression of ND diseases such as Parkinson’s\ndisease (PD) [30], HD [13], and other hereditary disorders\n(HA) [31], [32]. For example, machine learning classification\nmodels have been applied to analyze gait datasets from wear-\nable devices in PD [16], [33], including neural network (NN),\nsupport vector machine (SVM), k-nearest neighbors (kNNs),\ndecision tree (DT), random forest (RF), and logistic regres-\nsion (LR) [34]. These models were further optimized through\nthe comparison with the Movement Disorder Society Unified\nPD Rating Scale (MDS-UPDRS) III [35]. As a result, these\nmodels can assess the severity of ND diseases through']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2565.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,"Accuracy, recall, precision, loss, AUC, ROC curves.",,,,"[10, 13, 15]","['follows:\nFt+6 = Softmax(FC(Multi(Ft+5))) (7)\nwhere Softmax represents a softmax function and multi rep-\nresents multihead attention. As shown in (7), we adopt cross\nentropy as the loss function of our model to better fit the\nsample distribution.\nIV. C ASE STUDY\nA. Data and Evaluation\n1) Data Description: HD patients were recruited from the\nDepartment of Neurology, The First Affiliated Hospital, Sun\nYat-sen University, between December 2020 and December\n2022. Among HD patients, there are 27 males and 21 females.\nThe mean age of HD was 42.33 ± 10.85 years old. The median\nCAG repeat numbers were 45, ranging from 39 to 61. The\nmedian values of the total UHDRS and of the maximal chorea\nscores were 42.91 ± 22.57 and 11.30 ± 6.49, respectively.\nThe median scores of CAP and total functional score (TFC)\nwere 490 ± 115.00 and 10.05 ± 2.62, respectively. All\nfilming and usage in this project have been approved by\nthe ethics committees of The First Affiliated Hospital, Sun\nYat-sen University. Patients and their family provided their\ninformed consent and full attention has been paid to the\npatient’s privacy. The clinical data of patients, including the\ndemographic information and clinical assessment scale scores\n(UHDRS), have been saved in their medical records.\nHCs participants were recruited from the Department of\nNeurology, The First Affiliated Hospital, Sun Yat-sen Uni-\nversity, from December 2020 to December 2022, including\n25 males and 23 females. The mean age of participants\nwas 41.98 ± 9.47 years old. HC were recruited from three\npopulation groups: 1) family members of patients (such as\nasymptomatic partners or gene-negative family members); 2)\nvolunteers recruited through the “one-stop medical service”\nWeChat public account platform; and 3) volunteers recruited\nthrough the online platform of the Department of Neurology,\nThe First Affiliated Hospital, Sun Yat-sen University.\nWe obtained 1511 video data sets from patients and controls\nby using mobile phones 30 frames/s. The videos were mainly\ntaken by family members after appropriate instructions and by\nstaff from the First Affiliated Hospital, Sun Yat-sen University.\nEach participant provided 6–50 videos, including in standing\nposture, from front, side, and back; and sitting posture, walk-\ning, and from the face.\n2) Data Processing: First of all, due to interference in the\nshooting process, it was necessary to manually filter out higher\nquality video samples with clear shooting and less background\ninterference. After the screening, the total number of video\ndata per patient was reduced to approximately six segments,\nthe average length of these videos was 60–80 s.\nNext, the video data were then converted into image matri-\nces for processing. We frame sample the video at a rate of one\nimage every second. Afterward, the image was converted into\na grayscale image to remove unnecessary color information\nand reduce the learning burden of the 3D-CNN network. After\ndata enhancement, the image matrix was saved with 20 images\nin one 3-D matrix. During network training, the dataset was\ndivided into a training set and a test set at a ratio of 7:3.\n3) Data Enhancement: In the context of the small number\nof participants in this rare disease, a need arose to improve\nthe model during training performance. This data enhance-\nment was performed using frame difference and image-cut', '-end assessment; and c) low cost\nand convenience.\n1) Comparison With Other Models:First, the main meth-\nods for evaluating HD previously included scale analysis,\nfacial muscle group analysis model, and gait analysis for\ncomparison. Their shortcomings are very obvious. The scale\nanalysis uses the UHDRS diagnostic confidence level (DCL),\nwhich is based on the evaluation of motor signs is relatively\nlimited. The facial muscle group analysis uses methods, such\nas coordinate analysis and SVM, to analyze the amplitude\ncharacteristics of facial expressions and the shaking of facial\nsmall muscle groups. It is a non-end-to-end model with weak\ngeneralization ability. The gait analysis measures the severity\nof a patient’s illness based on high-level gait characteristics,\nrequiring patients to wear specialized pressure sensor equip-\nment. This is not only inconvenient for HD patients in remote\nareas but also has a relatively high cost. However, our model is\nan end-to-end 3-D multihead attention residual network, which\neliminates the inconvenience of wearing specialized pressure\nsensor devices to obtain corresponding data in gait analysis\nmodels. We also integrate many effective modules into our\nproposed model, such as data enhancement based on the frame\ndifference method and multihead attention module. Therefore,\nour model can achieve higher accuracy more conveniently and\nefficiently.\nSecond, as shown in Table I, the performance of our\nmodel is far better than classic models, such as 2DCNN\nand 3DCNN in terms of accuracy, recall, accuracy, and loss\nvalue performance. Given that the 2DCNN and 3DCNN model\nnetworks are relatively simple and difficult to extract effective\ninformation, they are lack of specificity analysis in diagnosing\nHD. After adding a specific attention module that captures\npatient features to our model, our model has significantly\nimproved the data utilization.\nThird, compared with the performance of the HD diagnostic\nmodels based on near-infrared spectroscopy data in Table I,\nour model has the highest accuracy, followed by LDA [50],\nQDA [50], Log.Regr. [50], and SVM [50]. Meanwhile, our\nmodel demonstrates significantly higher accuracy compared\nto the second-ranked LDA model. In addition, the acquisition\nof near-infrared spectroscopy data requires complex operations\nwhich may lead to poor repeatability. Moreover, these models\nrequire many training iterations to obtain satisfactory results,\nwhich places high demands on computer performance. Our\nmodel, by contrast, is end-to-end and can be operated by\nsimply obtaining images or videos of HD patients. Most\nimportantly, our model can achieve excellent performance with\na small amount of epoch training.\n2) ROC Curve Comparison:In order to better demonstrate\nthe high recall and accuracy performance of our model,\nwe plotted the receiver operating characteristic (ROC) curves\nof 2DCNN, 3DCNN, and our model in Fig. 6. The ROC\ncurve is of great significance in measuring the performance\nof binary classification models under different thresholds. The\nx-axis represents the false positive rate, the y-axis represents\nthe true rate, and the area under curve (AUC) area is the\narea enclosed by the ROC curve and the coordinate axis,\nwhich is an important indicator of model performance [47].\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nFig. 6. ROC curves of 2DCNN, 3DCNN, and our model. The blue dashed\nline is a random classifier and is often used as a model performance baseline.\nTABLE II\nABLATION EXPERIMENTS OF INDIVID', 'NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2576.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?",Confidence intervals and model performance comparisons.,,,,"[16, 15, 22]","[' to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA', 'NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead', ' al., “Comparison of walking protocols and gait\nassessment systems for machine learning-based classification of Parkin-\nson’s disease,” Sensors, vol. 19, no. 24, p. 5363, Dec. 2019.\n[37] R. Li et al., “Fine-grained intoxicated gait classification using a bilinear\nCNN,” IEEE Sensors J., vol. 23, no. 23, pp. 29733–29748, Dec. 2023.\n[38] A. Mühlbäck et al., “Establishing normative data for the evaluation of\ncognitive performance in Huntington’s disease considering the impact\nof gender, age, language, and education,” J. Neurol., vol. 270, no. 10,\npp. 4903–4913, Oct. 2023.\n[39] R. Alkhatib, M. O. Diab, C. Corbier, and M. E. Badaoui, “Machine\nlearning algorithm for gait analysis and classification on early detection\nof Parkinson,” IEEE Sensors Lett., vol. 4, no. 6, pp. 1–4, Jun. 2020.\n[40] G. Vignoud et al., “Video-based automated assessment of movement\nparameters consistent with MDS-UPDRS III in Parkinson’s disease,”\nJ. Parkinson’s Disease, vol. 12, no. 7, pp. 2211–2222, 2022.\n[41] B. Jin, Y . Qu, L. Zhang, and Z. Gao, “Diagnosing Parkinson disease\nthrough facial expression recognition: Video analysis,” J. Med. Internet\nRes., vol. 22, no. 7, Jul. 2020, Art. no. e18697.\n[42] I. C. Duta, L. Liu, F. Zhu, and L. Shao, “Improved residual networks for\nimage and video recognition,” in Proc. 25th Int. Conf. Pattern Recognit.\n(ICPR), Jan. 2021, pp. 9415–9422.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[44] D. Tran, H. Wang, M. Feiszli, and L. Torresani, “Video classification\nwith channel-separated convolutional networks,” in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2019, pp. 5551–5560.\n[45] N. Singla, “Motion detection based on frame difference method,” Int.\nJ. Inf. Comput. Technol., vol. 4, no. 15, pp. 1559–1565, 2014.\n[46] A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in\nProc. 18th Int. Conf. Pattern Recognit. (ICPR), 2006, pp. 850–855.\n[47] A. Tharwat, “Classification assessment methods,” Appl. Comput. Infor-\nmat., vol. 17, no. 1, pp. 168–192, Jan. 2021.\n[48] Y . L. Chang, C. S. Chan, and P. Remagnino, “Action recognition on\ncontinuous video,” Neural Comput. Appl., vol. 33, no. 4, pp. 1233–1243,\nFeb. 2021.\n[49] C. Li, Q. Zhong, D. X']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2569.0,free_text
A_Novel_End_to_End_3_D_Residual_and_Attention_Enhancement_Joint_Few_Shot_Learning_Model_for_Huntington_Clinical_Assessment,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[16, 15, 19]","[' to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead attention enhancement in our model.\nAccording to the data in Table II, when the motion selection\nbox threshold k = 1, the overall performance of the model is\nbetter than when k = 4. Model performance under different\nTABLE IV\nCOMPARISON OF MODEL PERFORMANCE AFTER DELETION\nOF SIX POSES .* R EPRESENTS THE STANDING POSTURE\nFig. 9. Impact of different attention modules on the model performance.\nattention levels SpaChan and SE–ST > SpaChan > Null.\nWhen k is equal to 1 and both SpaChan and SE–ST modules\nare utilized, the model performs optimally. Specifically, the\nSE–ST module enhances acc by 0.18 dB, the SpaChan module\nenhances acc by 0.18 dB, and the combination of the two mod-\nules further enhances acc by 0.27 dB. Moreover, it is notable\nthat excluding these attention modules leads to a significant\ndecrease in recall, resulting in a substantial amount of model\nmisjudgments. The aforementioned experiments demonstrate\nthe efficacy of the attention module we formulated, guaran-\nteeing the model to produce stable and efficacious outcomes\nin HD motor assessment.\n4) Explainability Experiments for Six Postures: Inter-\npretable models are important in clinical applications.\nAs shown in Table IV, we also designed a series of inter-\npretable experiments to provide insight into our model’s\npreferences for six different postures and to capture features\nthat are highly relevant to determining the presence or absence\nof disease. We use the following six categories for our\nexperiments and comparisons, sequentially removing from all\npose videos: 1) upper body videos; 2) sitting pose videos;\n3) front standing pose videos; 4) side standing pose videos;\n5) back standing posture video; and 6) walking video.\nWe delete the corresponding pose data from the dataset one\nby one according to the above classification, and then retrain\nand test our model.\nFrom the experimental results, it can be found that among\nthe six posture videos, the videos of front facing, and walking\nhave the greatest impact on the model results, which indicates\nthat they are the most helpful for the diagnosis of HD. This\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\n2509211 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 74, 2025\nis because these two postures can fully reflect the action\ncharacteristics of HD. On the contrary, the video with the least\nhelpful effect is the upper body video, because this type of\nvideo only reflects local feature information.\nV. C ONCLUSION\nIn this article, we propose an end-to-end deep learning\nmodel based on videos of patients in multiple poses, which can\neffectively identify HD patients. According to the experimental\nresults compared with traditional models, such as SVM, LDA,\nQDA', 'NN. In addition, the recall\nand presentation indicators showed significant improvement.\nOverall, the results demonstrate the efficacy of our chosen\ndata augmentation method in enhancing model performance.\nIn addition, we also calculated the computational complex-\nity and computational efficiency of each component of the\nmodel. As can be seen from Table III, the SpaChan and\nSE–ST modules we introduced can effectively improve the\nperformance of the model. The SE–ST module will cause an\nincrease in the number of parameters [Params (M)], but has\nonly a slight impact on the computational efficiency [FLOPs\n(G)]. The SpaChan module is a lightweight module and does\nnot introduce too much additional computational cost. More-\nover, when the two attention modules are combined with the\nmodel at the same time, the overall performance of the model\ncan be further improved, which confirms the effectiveness of\nour proposed method. This lightweight design model can be\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 22,2025 at 09:09:51 UTC from IEEE Xplore.  Restrictions apply. \n\nHUANG et al.: NOVEL END-TO-END 3-D-RESIDUAL AND ATTENTION ENHANCEMENT JOINT FEW-SHOT LEARNING MODEL 2509211\nFig. 8. Confidence intervals of model acc and loss under the threshold\nof different motion selection boxes. The first line is the loss change of the\nmodel, and the second line is the acc change of the model. The dark curve\nin the figure is the average curve of loss/acc, while the light part is the 95%\nconfidence interval of loss/acc.\ntrained and tested on consumer-grade GPUs, which ensures\nwidespread deployment and application of remote medical\ndiagnosis.\nFurthermore, in comparison with our model, despite the\ngood accuracy achieved by 2DCNN and 3DCNN models with\nsimple structures, their recall and precision are considerably\nlow indicating unreliable accuracy. Specifically, the model\nattains high accuracy rates by identifying most samples as\nHD samples, resulting in numerous misjudgments leading\nto significantly low recall and precision rates, rendering it\ninappropriate for medical clinical evaluation.\n2) Motion Selected Boxes Threshold: To investigate the\neffect of motion selection boxes on model performance in\ndata augmentation, we established the box count thresholds as\nk = 1, 4, and 6, respectively. Fig. 8 exhibits the comparison\nof the confidence interval for accuracy and loss during the\ntraining of the model.\nThe confidence interval reflects the uncertainty or bias of\nthe model’s prediction results. As for the confidence interval\nof loss, when the model tends to converge, the average loss\nof the model is k = 1 < k = 4 < k = 6 and the width of\nthe confidence interval is k = 1 < k = 4 < k = 6. Among\nthem, the model with k = 1 has the smallest average loss\nand the narrowest confidence interval. As for the confidence\ninterval of acc, when the model tends to converge, the average\nacc of the model is k = 1 > k = 6 > k = 4, and the width\nof the confidence interval is k = 1 < k = 6 < k = 4.\nAmong them, the model with k = 1 has the highest average\naccuracy and the narrowest confidence interval. Two identical\nresults collectively indicate that the model achieves optimal\nperformance when the motion selection box threshold is set\nto 2.\n3) Attention Module Comparison:In this section, we com-\npared the effects of channel attention and spatiotemporal\nattention on model performance. As shown in Fig. 9, SE–ST\nrepresents SE block and STE block in our model, and SpaChan\nrepresents multihead', ' Eskofier, J. Klucken, and E. Nöth, “Multimodal assessment of\nParkinson’s disease: A deep learning approach,” IEEE J. Biomed. Health\nInformat., vol. 23, no. 4, pp. 1618–1630, Jul. 2019.\n[13] F. D. Acosta-Escalante, E. Beltrán-Naturi, M. C. Boll,\nJ. A. Hernández-Nolasco, and P. Pancardo García, “Meta-classifiers\nin Huntington’s disease patients classification, using iPhone’s\nmovement sensors placed at the ankles,” IEEE Access, vol. 6,\npp. 30942–30957, 2018.\n[14] A. Mannini, D. Trojaniello, A. Cereatti, and A. Sabatini, “A machine\nlearning framework for gait classification using inertial sensors: Applica-\ntion to elderly, post-stroke and Huntington’s disease patients,” Sensors,\nvol. 16, no. 1, p. 134, Jan. 2016.\n[15] Y .-W. Ma, J.-L. Chen, Y .-J. Chen, and Y .-H. Lai, “Explainable deep\nlearning architecture for early diagnosis of Parkinson’s disease,” Soft\nComput., vol. 27, no. 5, pp. 2729–2738, Mar. 2023.\n[16] A. Shcherbak, E. Kovalenko, and A. Somov, “Detection and classifica-\ntion of early stages of Parkinson’s disease through wearable sensors and\nmachine learning,” IEEE Trans. Instrum. Meas., vol. 72, pp. 1–9, 2023.\n[17] A. Talitckii et al., “Comparative study of wearable sensors, video, and\nhandwriting to detect Parkinson’s disease,” IEEE Trans. Instrum. Meas.,\nvol. 71, pp. 1–10, 2022.\n[18] A. Lauraitis, R. Maskeliunas, R. Damaševicius, D. Polap, and\nM. Wozniak, “A smartphone application for automated decision support\nin cognitive task based evaluation of central nervous system motor disor-\nders,” IEEE J. Biomed. Health Informat., vol. 23, no. 5, pp. 1865–1876,\nSep. 2019.\n[19] R. Riad et al., “Predicting clinical scores in Huntington’s disease:\nA lightweight speech test,” J. Neurol., vol. 269, no. 9, pp. 5008–5021,\n2022.\n[20] Z. Chang et al., “Accurate detection of cerebellar smooth pursuit eye\nmovement abnormalities via mobile phone video and machine learning,”\nSci. Rep., vol. 10, no. 1, Oct. 2020, Art. no. 18641.\n[21] W. Huang, W. Xu, R. Wan, P. Zhang, Y . Zha, and M. Pang, “Auto\ndiagnosis of Parkinson’s disease via a deep learning model based on\nmixed emotional facial expressions,” IEEE J. Biomed. Health Informat.,\nvol. 28, no. 5, pp. 2547–2557, May 2024.\n[22] Z. Zha, H. Tang, Y . Sun, and J. Tang, “Boosting few-shot fine-grained\nrecognition with background suppression and foreground alignment,”\nIEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 8, pp. 3947–3961,\nAug.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2560.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,Diagnostic.,,,,"[5, 7, 2]","['olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis', ' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates', ' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2532.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",Unknown from this paper.,,,,"[5, 14, 15]","['olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis', 'wang, J. Jang, J. Leem, J.-Y . Park,\nH.-K. Kim, and W. Lee, “Serum bilirubin as a predictor of incident\nmetabolic syndrome: a 4-year retrospective longitudinal study of 6205\ninitially healthy korean men,” Diabetes & metabolism, vol. 40, no. 4,\npp. 305–309, 2014.\n[13] J. Olza, C. M. Aguilera, M. Gil-Campos, R. Leis, G. Bueno, M. Valle,\nR. Ca˜nete, R. Tojo, L. A. Moreno, and´A. Gil, “A continuous metabolic\nsyndrome score is associated with speciﬁc biomarkers of inﬂamma-\ntion and cvd risk in prepubertal children,” Annals of Nutrition and\nMetabolism, vol. 66, no. 2-3, pp. 72–79, 2015.\n[14] A. Bener, M. T. Yousafzai, S. Darwish, A. O. Al-Hamaq, E. A. Nasralla,\nand M. Abdul-Ghani, “Obesity index that better predict metabolic\nsyndrome: body mass index, waist circumference, waist hip ratio, or\nwaist height ratio,”Journal of obesity, vol. 2013, 2013.\n[15] G. Sagun, A. Oguz, E. Karagoz, A. T. Filizer, G. Tamer, B. Mesciet al.,\n“Application of alternative anthropometric measurements to predict\nmetabolic syndrome,”Clinics, vol. 69, no. 5, pp. 347–353, 2014.\n[16] D.-Y . Hsiung, C.-W. Liu, P.-C. Cheng, and W.-F. Ma, “Using non-\ninvasive assessment methods to predict the risk of metabolic syndrome,”\nApplied Nursing Research, vol. 28, no. 2, pp. 72–77, 2015.\n[17] M. Romero-Salda ˜na, F. J. Fuentes-Jim ´enez, M. Vaquero-Abell ´an,\nC. ´Alvarez-Fern´andez, G. Molina-Recio, and J. L´opez-Miranda, “New\nnon-invasive method for early detection of metabolic syndrome in the\nworking population,” European Journal of Cardiovascular Nursing,\nvol. 15, no. 7, pp. 549–558, 2016.\n[18] M. Romero-Salda ˜na, P. Tauler, M. Vaquero-Abell ´an, A.-A. L ´opez-\nGonz´alez, F.-J. Fuentes-Jim ´enez, A. Aguil ´o, C. ´Alvarez-Fern´andez,\nG. Molina-Recio, and M. Bennasar-Veny, “Validation of a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D.', ' a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D. G. Altman and P. Royston, “The cost of dichotomising continuous\nvariables,”Bmj, vol. 332, no. 7549, p. 1080, 2006.\n[22] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.\n5–32, 2001.\n[23] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, “Classiﬁca-\ntion and regression trees. belmont, ca: Wadsworth,”International Group,\nvol. 432, pp. 151–166, 1984.\n939\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\n[24] J. H. Friedman, “Greedy function approximation: a gradient boosting\nmachine,”Annals of statistics, pp. 1189–1232, 2001.\n[25] J. D. Kelleher, B. Mac Namee, and A. D’arcy,Fundamentals of machine\nlearning for predictive data analytics: algorithms, worked examples, and\ncase studies. MIT Press, 2015.\n[26] C. X. Ling, J. Huang, H. Zhang et al., “Auc: a statistically consistent\nand more discriminating measure than accuracy,” inIjcai, vol. 3, 2003,\npp. 519–524.\n[27] F. J. Dankers, A. Traverso, L. Wee, and S. M. van Kuijk, “Prediction\nmodeling methodology,” in Fundamentals of Clinical Data Science.\nSpringer, 2019, pp. 101–120.\n[28] A. Niculescu-Mizil and R. Caruana, “Predicting good probabilities\nwith supervised learning,” in Proceedings of the 22nd international\nconference on Machine learning. ACM, 2005, pp. 625–632.\n[29] M. H. DeGroot and S. E. Fienberg, “The comparison and evaluation\nof forecasters,”Journal of the Royal Statistical Society: Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncompar']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2542.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Unknown from this paper.,,,,"[6, 4, 7]","[' classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Discriminative Performance: The primary metric that\nwe use to judge the discriminative power of a model is\nthe Area Under the Curve (AUC) of the receiver operating\ncharacteristic (ROC) curve. Models with ideal discrimination\npower have an AUC of 1.0 and the ones with no discrimi-\nnatory ability have AUC equal 0.5. Along with AUC we also\nreport balanced accuracy (or average class accuracy), precision\n(or positive predictive value) and recall (or true positive\nrate). Balanced accuracy is calculated as the average of the\nproportion of correct predictions for each class individually.\nBalanced accuracy is chosen ahead of regular accuracy as it\nprovides a better overview of the model performance than the\nclassiﬁcation accuracy when the classes are not balanced [25].\nPrecision is the proportion of positive identiﬁcations by the\nmodel that were actually correct. Recall is the proportion of\nactual positives that were identiﬁed correctly.\nIt should be noted, that AUC is the strongest indicator of the\nmodel performance among all the discriminative performance\nmetrics that we report here. AUC is statistically consistent and\nmore discriminating than accuracy [26]. Balanced accuracy,\nprecision and recall are all dependent on the threshold we\nchoose to apply on the probability estimations of the class\nprediction to get the class labels. Choosing an appropriate\ndecision threshold is highly dependent on the desired outcome\nof the predictive model (high speciﬁcity or high sensitivity).\nSince our goal is to give a probability score for MetS, AUC\nis more relevant for us. For computing balanced accuracy,\nprecision and recall, a decision threshold of 0.5 is assumed.\nA 5-fold cross-validation was performed to get a more robust\nestimate of the validation metrics. The hyper-parameters for\nthe different classiﬁers were optimized empirically.\n2) Model Calibration: Model calibration refers to the\nagreement between subgroups of predicted probabilities and\ntheir observed frequencies [27]. For predictive models in\nmedicine, usually the predicted probabilities of the different\nclass labels are important as they show how conﬁdent the\nmodel is about the predictions. Models like LR are usually\nwell calibrated as it directly optimizes the log-loss. But many\nother machine learning models like RF are thought to be\npoorly calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning', ')\nGender M = 918 F = 1396 M=4 4 1F=5 0 0\nAge 55.20 ±8.47 57.47 ±8.16\nHeight(in cm) 170.79 ±9.04 171.16 ±9.03\nWeight(in KG) 80.31 ±15.96 88.38 ±15.46\nSystolic blood pres-\nsure(in mmHg) 127.87 ±16.27 134.09 ±15.11\nDiastolic blood pres-\nsure(in mmHg) 79.23 ±10.04 82.46 ±9.96\nWaist circumference\n(in cm) 91.46 ±13.64 100.56 ±11.83\nBlood sugar (in\nmg/dL) 112.37 ±25.74 120.98 ±31.54\nTriglycerides (in\nmg/dL) 173.78 ±94.61 232.93 ±100.16\nHDL-C (in mg/dL) 59.86 ±18.18 49.80 ±15.85\nHbA1c (in Percent-\nage) 5.43 ±0.59 5.65 ±0.73\nBlood Sugar Medica-\ntion Y = 103, N = 2086 Y=9 9 ,N=7 7 7\nTriglyceride Medica-\ntion Y = 72, N = 2117 Y=6 8 ,N=8 0 8\nHDL Medication Y = 167, N = 2022 Y = 145, N = 731\nM = Male, F= Female,Y=Y e s ,N=n o\naccording to the Deurenberg equation [17]. Deurenberg, West-\nstrate and Seidell proposed (1) for estimating BFP where sex\nis 0 for females and 1 for males [19].\nBFP =1 .20 ∗BMI +0 .23 ∗age −10.8 ∗sex −5.4 (1)\nAfter computing the new features, we construct a few\nsubsets of the whole feature space. Although it may seem\ncounter intuitive to use a subset of the features instead of all, it\nis often observed that only a few of those features are actually\nrelated to the target concept. Irrelevant and redundant features\nmay confuse the machine learning algorithm by shrouding the\ndistributions of the small set of truly relevant features for\nthe task at hand [20]. In this situation, feature selection is\nimportant to speed up learning and to improve concept quality.\nIt should be noted that the feature subsets here are created\nmanually mostly based on prior work and domain knowledge.\nThese smaller subsets are necessary to compare our work\nwith the results of Romero-Salda˜na et al. and establish some\nbaselines [17]. We deﬁne the following feature sets:\n1) Set A: age, sex, height, weight, systolic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts', ' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2514.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q4,How was the dataset described in this study before predictive modeling was performed?,Unknown from this paper.,,,,"[7, 2, 8]","[' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates', ' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', ' self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates a balanced accuracy\nof 0.73 (recall: 0.55, precision: 0.80) on the whole dataset.\nThe results are in accordance with the later work that was\npublished to validate this method on a bigger dataset [18]. The\nlow recall again underlines the fact that the ﬁxed rule based\napproach suffers from a high number of false negatives, i.e it\n936\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE III\nCOMPREHENSIVE PERFORMANCE COMPARISON OF ALL THE CLASSIFIERS ACROSS DIFFERENT FEATURE SETS\nClassiﬁers\nFeature Sets\nSet A Set B Set C [17] Set D\nAUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR\nLogistic Re-\ngression\n0.88\n(+/-\n0.02)\n0.78\n(+/-\n0.05)\n0.78\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.87\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.77\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.81\n(+/-\n0.01)\n0.73\n(+/-\n0.03)\n0.71\n(+/-\n0.03)\n0.56\n(+/-\n0.10)\n0.85\n(+/-\n0.03)\n0.76\n(+/-\n0.03)\n0.75\n(+/-\n0.05)\n0.67\n(+/-\n0.10)\nRandom For-\nest [22]\n0.90\n(+/-\n0.02)\n0.82\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.80\n(+/-\n0.10)\n0.86\n(+/-\n0.01)\n0.79\n(+/-\n0.02)\n0.74\n(+/-\n0.06)\n0.76\n(+/-\n0.09)\n0.84\n(+/-\n0.02)\n0.76\n(+/-\n0.01)\n0.72\n(+/-\n0.06)\n0.70\n(+/-\n0.09)\n0.81\n(+/-\n0.04)\n0.73\n(+/-\n0.04)\n0.68\n(+/-\n0.05)\n0.68\n(+/-\n0.06)\nGBM [24]\n0.89\n(+/-\n0.02)\n0.80\n(+/-\n0.03)\n0.78\n(+/-\n0.05)\n0.74\n(+/-\n0.13)\n0.87\n(+/-\n0.02)\n0.79\n(+/-\n0.03)\n0.76\n(+/']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2533.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?","20% test set, rest for training.",,,,"[8, 2, 10]","[' self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates a balanced accuracy\nof 0.73 (recall: 0.55, precision: 0.80) on the whole dataset.\nThe results are in accordance with the later work that was\npublished to validate this method on a bigger dataset [18]. The\nlow recall again underlines the fact that the ﬁxed rule based\napproach suffers from a high number of false negatives, i.e it\n936\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE III\nCOMPREHENSIVE PERFORMANCE COMPARISON OF ALL THE CLASSIFIERS ACROSS DIFFERENT FEATURE SETS\nClassiﬁers\nFeature Sets\nSet A Set B Set C [17] Set D\nAUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR\nLogistic Re-\ngression\n0.88\n(+/-\n0.02)\n0.78\n(+/-\n0.05)\n0.78\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.87\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.77\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.81\n(+/-\n0.01)\n0.73\n(+/-\n0.03)\n0.71\n(+/-\n0.03)\n0.56\n(+/-\n0.10)\n0.85\n(+/-\n0.03)\n0.76\n(+/-\n0.03)\n0.75\n(+/-\n0.05)\n0.67\n(+/-\n0.10)\nRandom For-\nest [22]\n0.90\n(+/-\n0.02)\n0.82\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.80\n(+/-\n0.10)\n0.86\n(+/-\n0.01)\n0.79\n(+/-\n0.02)\n0.74\n(+/-\n0.06)\n0.76\n(+/-\n0.09)\n0.84\n(+/-\n0.02)\n0.76\n(+/-\n0.01)\n0.72\n(+/-\n0.06)\n0.70\n(+/-\n0.09)\n0.81\n(+/-\n0.04)\n0.73\n(+/-\n0.04)\n0.68\n(+/-\n0.05)\n0.68\n(+/-\n0.06)\nGBM [24]\n0.89\n(+/-\n0.02)\n0.80\n(+/-\n0.03)\n0.78\n(+/-\n0.05)\n0.74\n(+/-\n0.13)\n0.87\n(+/-\n0.02)\n0.79\n(+/-\n0.03)\n0.76\n(+/', ' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', '-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2546.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q6,What preprocessing techniques on the included variables/features were applied in this study?,Unknown from this paper.,,,,"[2, 10, 8]","[' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', '-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.', ' self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates a balanced accuracy\nof 0.73 (recall: 0.55, precision: 0.80) on the whole dataset.\nThe results are in accordance with the later work that was\npublished to validate this method on a bigger dataset [18]. The\nlow recall again underlines the fact that the ﬁxed rule based\napproach suffers from a high number of false negatives, i.e it\n936\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE III\nCOMPREHENSIVE PERFORMANCE COMPARISON OF ALL THE CLASSIFIERS ACROSS DIFFERENT FEATURE SETS\nClassiﬁers\nFeature Sets\nSet A Set B Set C [17] Set D\nAUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR\nLogistic Re-\ngression\n0.88\n(+/-\n0.02)\n0.78\n(+/-\n0.05)\n0.78\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.87\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.77\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.81\n(+/-\n0.01)\n0.73\n(+/-\n0.03)\n0.71\n(+/-\n0.03)\n0.56\n(+/-\n0.10)\n0.85\n(+/-\n0.03)\n0.76\n(+/-\n0.03)\n0.75\n(+/-\n0.05)\n0.67\n(+/-\n0.10)\nRandom For-\nest [22]\n0.90\n(+/-\n0.02)\n0.82\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.80\n(+/-\n0.10)\n0.86\n(+/-\n0.01)\n0.79\n(+/-\n0.02)\n0.74\n(+/-\n0.06)\n0.76\n(+/-\n0.09)\n0.84\n(+/-\n0.02)\n0.76\n(+/-\n0.01)\n0.72\n(+/-\n0.06)\n0.70\n(+/-\n0.09)\n0.81\n(+/-\n0.04)\n0.73\n(+/-\n0.04)\n0.68\n(+/-\n0.05)\n0.68\n(+/-\n0.06)\nGBM [24]\n0.89\n(+/-\n0.02)\n0.80\n(+/-\n0.03)\n0.78\n(+/-\n0.05)\n0.74\n(+/-\n0.13)\n0.87\n(+/-\n0.02)\n0.79\n(+/-\n0.03)\n0.76\n(+/']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2534.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q7,How is missing data handled in this study?,Unknown from this paper.,,,,"[5, 4, 12]","['olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis', ')\nGender M = 918 F = 1396 M=4 4 1F=5 0 0\nAge 55.20 ±8.47 57.47 ±8.16\nHeight(in cm) 170.79 ±9.04 171.16 ±9.03\nWeight(in KG) 80.31 ±15.96 88.38 ±15.46\nSystolic blood pres-\nsure(in mmHg) 127.87 ±16.27 134.09 ±15.11\nDiastolic blood pres-\nsure(in mmHg) 79.23 ±10.04 82.46 ±9.96\nWaist circumference\n(in cm) 91.46 ±13.64 100.56 ±11.83\nBlood sugar (in\nmg/dL) 112.37 ±25.74 120.98 ±31.54\nTriglycerides (in\nmg/dL) 173.78 ±94.61 232.93 ±100.16\nHDL-C (in mg/dL) 59.86 ±18.18 49.80 ±15.85\nHbA1c (in Percent-\nage) 5.43 ±0.59 5.65 ±0.73\nBlood Sugar Medica-\ntion Y = 103, N = 2086 Y=9 9 ,N=7 7 7\nTriglyceride Medica-\ntion Y = 72, N = 2117 Y=6 8 ,N=8 0 8\nHDL Medication Y = 167, N = 2022 Y = 145, N = 731\nM = Male, F= Female,Y=Y e s ,N=n o\naccording to the Deurenberg equation [17]. Deurenberg, West-\nstrate and Seidell proposed (1) for estimating BFP where sex\nis 0 for females and 1 for males [19].\nBFP =1 .20 ∗BMI +0 .23 ∗age −10.8 ∗sex −5.4 (1)\nAfter computing the new features, we construct a few\nsubsets of the whole feature space. Although it may seem\ncounter intuitive to use a subset of the features instead of all, it\nis often observed that only a few of those features are actually\nrelated to the target concept. Irrelevant and redundant features\nmay confuse the machine learning algorithm by shrouding the\ndistributions of the small set of truly relevant features for\nthe task at hand [20]. In this situation, feature selection is\nimportant to speed up learning and to improve concept quality.\nIt should be noted that the feature subsets here are created\nmanually mostly based on prior work and domain knowledge.\nThese smaller subsets are necessary to compare our work\nwith the results of Romero-Salda˜na et al. and establish some\nbaselines [17]. We deﬁne the following feature sets:\n1) Set A: age, sex, height, weight, systolic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts', '[22] 0.139 0.138 0.138\nGBMs [24] 0.156 0.138 0.137\nEnsemble 0.153 0.141 0.140\nThough showing promising results for a relatively unex-\nplored problem, our study has certain limitations. We do\nacknowledge, our training cohort contains subjects with pre-\ndominately European origin, with an age range of 40 to\n70. This limits the generalizability of our models to other\npopulations. Secondly it is a retrospective study with all the\nusual limitations. Finally, we also acknowledge the limitations\nrising from using MetS as the target variable, instead of\nusing the secondary complications such as CVDs and type\n2 diabetes directly. Training on the secondary complications\ndirectly would have allowed us to circumvent the problems\nrising from the dichotomous deﬁnition currently used for MetS\n[36].\nThis study can be used as a prototype for developing a\nnon-invasive risk score for the diagnosis of MetS. In future,\nwe plan to validate our methods on different cohorts and also\non electronic health record (EHR) databases. EHR databases\nwith longitudinal patient data will also allow us to train our\nmodels with the secondary complications as target variable.\nFinally we would also like to add more features to our cohort,\nspecially those collected from wearable sensors such as heart\nrate, HRV and daily activity information to see if that improves\nthe diagnostic accuracy.\nACKNOWLEDGMENT\nWe would like to thank Prof. M. Rapp, Prof. P. M. Wippert,\nProf. H. V ¨oller and Dr. K. Bonaventura from University\nof Potsdam for helping us with the study design and data\ncollection.\nREFERENCES\n[1] R. H. Eckel, S. M. Grundy, and P. Z. Zimmet, “The metabolic syn-\ndrome,”The Lancet, vol. 365, no. 9468, pp. 1415–1428, 2005.\n[2] P. Zimmet, K. Alberti, and J. Shaw, “Global and societal implications\nof the diabetes epidemic,”Nature, vol. 414, no. 6865, p. 782, 2001.\n[3] P. Z. Zimmet, K. G. M. Alberti, and J. E. Shaw, “Mainstreaming\nthe metabolic syndrome: a deﬁnitive deﬁnition,” Medical Journal of\nAustralia, vol. 183, no. 4, p. 175, 2005.\n[4] World Health Organization and others, “Deﬁnition, diagnosis and clas-\nsiﬁcation of diabetes mellitus and its complications: report of a who\nconsultation. part 1, diagnosis and classiﬁcation of diabetes mellitus,”\nGeneva: World health organization, Tech. Rep., 1999.\n[5] Expert Panel on Detection, Evaluation and others, “Executive summary\nof the third report of the national cholesterol education program (ncep)\nexpert panel on detection, evaluation, and treatment of high blood\ncholesterol in adults (adult treatment panel iii).”JAMA, vol. 285, no. 19,\np. 2486, 2001.\n[6] K. G. M. M. Alberti, P. Zimmet, and J. Shaw, “Metabolic syndrome-a\nnew world-wide deﬁnition. a consensus statement from the international\ndiabetes federation,” Diabetic medicine, vol. 23, no. 5, pp. 469–480,\n2006.\n[7] S. M. Grundy, J. I.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2506.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[7, 14, 16]","[' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates', 'wang, J. Jang, J. Leem, J.-Y . Park,\nH.-K. Kim, and W. Lee, “Serum bilirubin as a predictor of incident\nmetabolic syndrome: a 4-year retrospective longitudinal study of 6205\ninitially healthy korean men,” Diabetes & metabolism, vol. 40, no. 4,\npp. 305–309, 2014.\n[13] J. Olza, C. M. Aguilera, M. Gil-Campos, R. Leis, G. Bueno, M. Valle,\nR. Ca˜nete, R. Tojo, L. A. Moreno, and´A. Gil, “A continuous metabolic\nsyndrome score is associated with speciﬁc biomarkers of inﬂamma-\ntion and cvd risk in prepubertal children,” Annals of Nutrition and\nMetabolism, vol. 66, no. 2-3, pp. 72–79, 2015.\n[14] A. Bener, M. T. Yousafzai, S. Darwish, A. O. Al-Hamaq, E. A. Nasralla,\nand M. Abdul-Ghani, “Obesity index that better predict metabolic\nsyndrome: body mass index, waist circumference, waist hip ratio, or\nwaist height ratio,”Journal of obesity, vol. 2013, 2013.\n[15] G. Sagun, A. Oguz, E. Karagoz, A. T. Filizer, G. Tamer, B. Mesciet al.,\n“Application of alternative anthropometric measurements to predict\nmetabolic syndrome,”Clinics, vol. 69, no. 5, pp. 347–353, 2014.\n[16] D.-Y . Hsiung, C.-W. Liu, P.-C. Cheng, and W.-F. Ma, “Using non-\ninvasive assessment methods to predict the risk of metabolic syndrome,”\nApplied Nursing Research, vol. 28, no. 2, pp. 72–77, 2015.\n[17] M. Romero-Salda ˜na, F. J. Fuentes-Jim ´enez, M. Vaquero-Abell ´an,\nC. ´Alvarez-Fern´andez, G. Molina-Recio, and J. L´opez-Miranda, “New\nnon-invasive method for early detection of metabolic syndrome in the\nworking population,” European Journal of Cardiovascular Nursing,\nvol. 15, no. 7, pp. 549–558, 2016.\n[18] M. Romero-Salda ˜na, P. Tauler, M. Vaquero-Abell ´an, A.-A. L ´opez-\nGonz´alez, F.-J. Fuentes-Jim ´enez, A. Aguil ´o, C. ´Alvarez-Fern´andez,\nG. Molina-Recio, and M. Bennasar-Veny, “Validation of a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D.', ': Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncomparisons to regularized likelihood methods,” Advances in large\nmargin classiﬁers, vol. 10, no. 3, pp. 61–74, 1999.\n[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay, “Scikit-learn: Machine learning in Python,”Journal of Machine\nLearning Research, vol. 12, pp. 2825–2830, 2011.\n[34] J. D. Hunter, “Matplotlib: A 2d graphics environment,” Computing in\nScience & Engineering, vol. 9, no. 3, pp. 90–95, 2007.\n[35] V . Valev and A. Asaithambi, “Multidimensional pattern recognition\nproblems and combining classiﬁers,” Pattern Recognition Letters,\nvol. 22, no. 12, pp. 1291–1297, 2001.\n[36] G.-D. Kang, L. Guo, Z.-R. Guo, X.-S. Hu, M. Wu, and H.-T. Yang,\n“Continuous metabolic syndrome risk score for predicting cardiovascular\ndisease in the chinese population,” Asia Paciﬁc journal of clinical\nnutrition, vol. 21, no. 1, pp. 88–96, 2012.\n940\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2275.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q9,Which prediction models were used in this study?,"Logistic Regression, Random Forest, Gradient Boosting Machines, Ensemble Classifier",,,,"[7, 2, 5]","[' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates', ' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', 'olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2530.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,Compared algorithms and feature subsets; used cross-validation and calibration.,,,,"[2, 5, 7]","[' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', 'olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis', ' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2533.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q11,How was data augmentation or generation used in this study?,Unknown from this paper.,,,,"[12, 5, 16]","['[22] 0.139 0.138 0.138\nGBMs [24] 0.156 0.138 0.137\nEnsemble 0.153 0.141 0.140\nThough showing promising results for a relatively unex-\nplored problem, our study has certain limitations. We do\nacknowledge, our training cohort contains subjects with pre-\ndominately European origin, with an age range of 40 to\n70. This limits the generalizability of our models to other\npopulations. Secondly it is a retrospective study with all the\nusual limitations. Finally, we also acknowledge the limitations\nrising from using MetS as the target variable, instead of\nusing the secondary complications such as CVDs and type\n2 diabetes directly. Training on the secondary complications\ndirectly would have allowed us to circumvent the problems\nrising from the dichotomous deﬁnition currently used for MetS\n[36].\nThis study can be used as a prototype for developing a\nnon-invasive risk score for the diagnosis of MetS. In future,\nwe plan to validate our methods on different cohorts and also\non electronic health record (EHR) databases. EHR databases\nwith longitudinal patient data will also allow us to train our\nmodels with the secondary complications as target variable.\nFinally we would also like to add more features to our cohort,\nspecially those collected from wearable sensors such as heart\nrate, HRV and daily activity information to see if that improves\nthe diagnostic accuracy.\nACKNOWLEDGMENT\nWe would like to thank Prof. M. Rapp, Prof. P. M. Wippert,\nProf. H. V ¨oller and Dr. K. Bonaventura from University\nof Potsdam for helping us with the study design and data\ncollection.\nREFERENCES\n[1] R. H. Eckel, S. M. Grundy, and P. Z. Zimmet, “The metabolic syn-\ndrome,”The Lancet, vol. 365, no. 9468, pp. 1415–1428, 2005.\n[2] P. Zimmet, K. Alberti, and J. Shaw, “Global and societal implications\nof the diabetes epidemic,”Nature, vol. 414, no. 6865, p. 782, 2001.\n[3] P. Z. Zimmet, K. G. M. Alberti, and J. E. Shaw, “Mainstreaming\nthe metabolic syndrome: a deﬁnitive deﬁnition,” Medical Journal of\nAustralia, vol. 183, no. 4, p. 175, 2005.\n[4] World Health Organization and others, “Deﬁnition, diagnosis and clas-\nsiﬁcation of diabetes mellitus and its complications: report of a who\nconsultation. part 1, diagnosis and classiﬁcation of diabetes mellitus,”\nGeneva: World health organization, Tech. Rep., 1999.\n[5] Expert Panel on Detection, Evaluation and others, “Executive summary\nof the third report of the national cholesterol education program (ncep)\nexpert panel on detection, evaluation, and treatment of high blood\ncholesterol in adults (adult treatment panel iii).”JAMA, vol. 285, no. 19,\np. 2486, 2001.\n[6] K. G. M. M. Alberti, P. Zimmet, and J. Shaw, “Metabolic syndrome-a\nnew world-wide deﬁnition. a consensus statement from the international\ndiabetes federation,” Diabetic medicine, vol. 23, no. 5, pp. 469–480,\n2006.\n[7] S. M. Grundy, J. I.', 'olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis', ': Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncomparisons to regularized likelihood methods,” Advances in large\nmargin classiﬁers, vol. 10, no. 3, pp. 61–74, 1999.\n[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay, “Scikit-learn: Machine learning in Python,”Journal of Machine\nLearning Research, vol. 12, pp. 2825–2830, 2011.\n[34] J. D. Hunter, “Matplotlib: A 2d graphics environment,” Computing in\nScience & Engineering, vol. 9, no. 3, pp. 90–95, 2007.\n[35] V . Valev and A. Asaithambi, “Multidimensional pattern recognition\nproblems and combining classiﬁers,” Pattern Recognition Letters,\nvol. 22, no. 12, pp. 1291–1297, 2001.\n[36] G.-D. Kang, L. Guo, Z.-R. Guo, X.-S. Hu, M. Wu, and H.-T. Yang,\n“Continuous metabolic syndrome risk score for predicting cardiovascular\ndisease in the chinese population,” Asia Paciﬁc journal of clinical\nnutrition, vol. 21, no. 1, pp. 88–96, 2012.\n940\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2272.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared to a rule-based baseline.",,,,"[2, 8, 7]","[' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', ' self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates a balanced accuracy\nof 0.73 (recall: 0.55, precision: 0.80) on the whole dataset.\nThe results are in accordance with the later work that was\npublished to validate this method on a bigger dataset [18]. The\nlow recall again underlines the fact that the ﬁxed rule based\napproach suffers from a high number of false negatives, i.e it\n936\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE III\nCOMPREHENSIVE PERFORMANCE COMPARISON OF ALL THE CLASSIFIERS ACROSS DIFFERENT FEATURE SETS\nClassiﬁers\nFeature Sets\nSet A Set B Set C [17] Set D\nAUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR AUC BA PPV TPR\nLogistic Re-\ngression\n0.88\n(+/-\n0.02)\n0.78\n(+/-\n0.05)\n0.78\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.87\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.77\n(+/-\n0.07)\n0.69\n(+/-\n0.16)\n0.81\n(+/-\n0.01)\n0.73\n(+/-\n0.03)\n0.71\n(+/-\n0.03)\n0.56\n(+/-\n0.10)\n0.85\n(+/-\n0.03)\n0.76\n(+/-\n0.03)\n0.75\n(+/-\n0.05)\n0.67\n(+/-\n0.10)\nRandom For-\nest [22]\n0.90\n(+/-\n0.02)\n0.82\n(+/-\n0.02)\n0.77\n(+/-\n0.05)\n0.80\n(+/-\n0.10)\n0.86\n(+/-\n0.01)\n0.79\n(+/-\n0.02)\n0.74\n(+/-\n0.06)\n0.76\n(+/-\n0.09)\n0.84\n(+/-\n0.02)\n0.76\n(+/-\n0.01)\n0.72\n(+/-\n0.06)\n0.70\n(+/-\n0.09)\n0.81\n(+/-\n0.04)\n0.73\n(+/-\n0.04)\n0.68\n(+/-\n0.05)\n0.68\n(+/-\n0.06)\nGBM [24]\n0.89\n(+/-\n0.02)\n0.80\n(+/-\n0.03)\n0.78\n(+/-\n0.05)\n0.74\n(+/-\n0.13)\n0.87\n(+/-\n0.02)\n0.79\n(+/-\n0.03)\n0.76\n(+/', ' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2539.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q13,Which type of explainability techniques are used?,Unknown from this paper.,,,,"[6, 10, 5]","[' classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Discriminative Performance: The primary metric that\nwe use to judge the discriminative power of a model is\nthe Area Under the Curve (AUC) of the receiver operating\ncharacteristic (ROC) curve. Models with ideal discrimination\npower have an AUC of 1.0 and the ones with no discrimi-\nnatory ability have AUC equal 0.5. Along with AUC we also\nreport balanced accuracy (or average class accuracy), precision\n(or positive predictive value) and recall (or true positive\nrate). Balanced accuracy is calculated as the average of the\nproportion of correct predictions for each class individually.\nBalanced accuracy is chosen ahead of regular accuracy as it\nprovides a better overview of the model performance than the\nclassiﬁcation accuracy when the classes are not balanced [25].\nPrecision is the proportion of positive identiﬁcations by the\nmodel that were actually correct. Recall is the proportion of\nactual positives that were identiﬁed correctly.\nIt should be noted, that AUC is the strongest indicator of the\nmodel performance among all the discriminative performance\nmetrics that we report here. AUC is statistically consistent and\nmore discriminating than accuracy [26]. Balanced accuracy,\nprecision and recall are all dependent on the threshold we\nchoose to apply on the probability estimations of the class\nprediction to get the class labels. Choosing an appropriate\ndecision threshold is highly dependent on the desired outcome\nof the predictive model (high speciﬁcity or high sensitivity).\nSince our goal is to give a probability score for MetS, AUC\nis more relevant for us. For computing balanced accuracy,\nprecision and recall, a decision threshold of 0.5 is assumed.\nA 5-fold cross-validation was performed to get a more robust\nestimate of the validation metrics. The hyper-parameters for\nthe different classiﬁers were optimized empirically.\n2) Model Calibration: Model calibration refers to the\nagreement between subgroups of predicted probabilities and\ntheir observed frequencies [27]. For predictive models in\nmedicine, usually the predicted probabilities of the different\nclass labels are important as they show how conﬁdent the\nmodel is about the predictions. Models like LR are usually\nwell calibrated as it directly optimizes the log-loss. But many\nother machine learning models like RF are thought to be\npoorly calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning', '-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.', 'olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2510.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,"AUC, balanced accuracy, recall, Brier score",,,,"[7, 2, 10]","[' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates', ' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', '-\ntween the different feature sets. When comparing the AUCs,\nwe observe that the ensemble classiﬁer performs best on\naverage across all the feature sets. For the larger feature sets,\nthe more advanced machine learning methods perform better\nthan the logistic regression. For set D which is the smallest\nfeature set with only 3 features, almost all the algorithms\n(with the exception of RF) shows similar performance. For\nbalanced accuracy, the ensemble classiﬁer outperforms all the\nother algorithms on every feature set. In terms of recall, the\nmore advanced machine learning methods demonstrate a better\nperformance than logistic regression.\nLooking at the standard deviation of all the validation\nmetrics reported here, we can also observe that the ensem-\nble method usually demonstrates a more stable performance\nacross the different cross validation folds. This supports the\nnotion that, because ensemble algorithms aggregate multiple\nclassiﬁers it is often more generalizable and robust compared\nto individual classiﬁer algorithms [35]. In a sense, the different\nbase algorithms compliment each other in an ensemble, which\noften results in a better classiﬁcation performance.\nThe ROC curves for the different classiﬁers can be visual-\nized in ﬁg. 2. The curves were generated using the same test\nset as described above using the features from set A.\nC. Model Calibration\nTo investigate the model calibration, we start with the\ncalibration plots for the classiﬁers described in the section\nIII-D. Fig. 3 shows the calibration plots based on the predicted\nprobabilities that were directly obtained from the models.\nThe calibration performance is evaluated with Brier score,\nreported in the legend of ﬁg. 3 and in table IV. The lower\nthe Brier score, the better calibrated the model is. Calibration\nperformance can also be evaluated by investigating the calibra-\ntion/reliability plot (the upper half of the ﬁgure). If the model\nis well calibrated the points will fall near the diagonal line.\nThe lower part of the plot shows histograms of the predicted\nprobabilities. From ﬁg. 3 we can see, as expected the logistic\nregression returns a well calibrated model. Notably, contrary\nto popular belief, RF produces the best calibrated model here.\nThe GBM and the ensemble classiﬁer display poor calibration\nperformance and exhibit an almost sigmoidal shape in the\ncalibration plots. Investigating the histogram in this ﬁgure it\ncan be observed that most of the predicted probabilities from\nthe GBM and the ensemble lies in central region with very\nfew probabilities reaching 0 or 1.\nFig. 4 shows the calibration plots and histograms of the clas-\nsiﬁers after applying the isotonic regression [31]. The adjusted\nBrier scores can be found in the legend of 4 and in table IV. It\ncan be immediately observed that the calibration performance\nof GBM and the ensemble model improves signiﬁcantly. The\nBrier scores of the calibrated GBM and ensemble models\nare now comparable with the native LR and RF outputs\nand in some cases even better (for GBMs). Inspecting the\nhistograms, it can be observed that the probabilities are much\nbetter distributed across the whole spectrum when compared\nto ﬁg. 3. Platt’s scaling was also tested on the models and\nthe corresponding Brier scores can be found in table IV.\nNo signiﬁcant differences were observed when comparing the\nBrier scores of the two different calibration techniques.\n937\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nFig. 1. Comparison of ROC curves for the different feature sets with\nthe ensemble classiﬁer.\nFig.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2529.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,"AUC, balanced accuracy, recall, calibration, Brier score.",,,,"[2, 12, 7]","[' obtain their\nMetS risk and undergo further diagnostic steps if necessary.\nIn recent years, machine learning has frequently been uti-\nlized to predict and aid medical diagnosis. Machine learning\nalgorithms especially non linear classiﬁers such as random\nforests, gradient boosting trees have often exhibited better\nperformance in predicting clinical outcomes than traditional\napproaches like logistic regression [9], [10]. But even with a\nstrong algorithm, the model can only be as good as the relevant\ninformation in the training dataset. Selecting the set of relevant\nfeatures that fully describes all concepts in a given dataset\ncan be equally important as selecting the right algorithm. In\nthis paper we not only compare the different algorithms with\neach other but also different feature subsets for predicting the\npresence of MetS.\nAdditionally, we do recognize the need of providing a con-\ntinuous risk score along with the diagnosis for MetS. There-\nfore, we also investigate the probability estimates obtained\nfrom the different classiﬁers and test if they are well calibrated\nin order to achieve the best individual risk predictions.\nII. P REVIOUS WORK\nFor the purpose of early detection of MetS a lot of work\nhas been done to discover novel biomarkers of metabolic\nrisk such as leukocyte and its sub-types, serum bilirubin,\nplasminogen activator inhibitor-1, adiponectin, resistin, C-\nreactive protein [11]–[13]. Some work also exists in identify-\ning anthropological features such as body mass index (BMI),\nwaist circumference, waist-hip ratio, waist-height ratio etc.\n[14], [15]. The discrepancy in the results reported in these\npapers points to the need of predictive models that are more\nrobust and generalize better to unseen data.\nHsiung, Liu, Cheng and Ma proposed a novel method\nto predict the presence of MetS using heart rate variability\n(HRV) and some other non-invasive measures such as BMI,\nbody fat, waist circumference, neck circumference [16]. The\nwork shows that data collected from cardiac bio-signals such\nas HRV can be predictive of the presence of MetS when\ncombined with some anthropological features. Although no\nﬁnal diagnostic accuracy metrics were reported in the paper,\nthe systolic blood pressure (SBP), BMI and the very low\nfrequency (VLF) sections of the HRV were found to be good\npredictors of the condition. Nevertheless, the lack of HRV data\nin current clinical practice makes it challenging to incorporate\nit into system whose purpose is to ease the detection of MetS.\nRomero-Salda˜na et al. proposed a new method based on\nonly anthropometric variables for early detection of MetS for\na Spanish working population (trained on 636 subjects and\nvalidated on 550 subjects) [17]. The features that were used\nfor the models were waist-height ratio, body mass index, blood\npressure and body fat percentage. Based on the outcome of\nthe model they proposed a decision tree based on waist-height\nratio (≥ 0.55) and hypertension (blood pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was', '[22] 0.139 0.138 0.138\nGBMs [24] 0.156 0.138 0.137\nEnsemble 0.153 0.141 0.140\nThough showing promising results for a relatively unex-\nplored problem, our study has certain limitations. We do\nacknowledge, our training cohort contains subjects with pre-\ndominately European origin, with an age range of 40 to\n70. This limits the generalizability of our models to other\npopulations. Secondly it is a retrospective study with all the\nusual limitations. Finally, we also acknowledge the limitations\nrising from using MetS as the target variable, instead of\nusing the secondary complications such as CVDs and type\n2 diabetes directly. Training on the secondary complications\ndirectly would have allowed us to circumvent the problems\nrising from the dichotomous deﬁnition currently used for MetS\n[36].\nThis study can be used as a prototype for developing a\nnon-invasive risk score for the diagnosis of MetS. In future,\nwe plan to validate our methods on different cohorts and also\non electronic health record (EHR) databases. EHR databases\nwith longitudinal patient data will also allow us to train our\nmodels with the secondary complications as target variable.\nFinally we would also like to add more features to our cohort,\nspecially those collected from wearable sensors such as heart\nrate, HRV and daily activity information to see if that improves\nthe diagnostic accuracy.\nACKNOWLEDGMENT\nWe would like to thank Prof. M. Rapp, Prof. P. M. Wippert,\nProf. H. V ¨oller and Dr. K. Bonaventura from University\nof Potsdam for helping us with the study design and data\ncollection.\nREFERENCES\n[1] R. H. Eckel, S. M. Grundy, and P. Z. Zimmet, “The metabolic syn-\ndrome,”The Lancet, vol. 365, no. 9468, pp. 1415–1428, 2005.\n[2] P. Zimmet, K. Alberti, and J. Shaw, “Global and societal implications\nof the diabetes epidemic,”Nature, vol. 414, no. 6865, p. 782, 2001.\n[3] P. Z. Zimmet, K. G. M. Alberti, and J. E. Shaw, “Mainstreaming\nthe metabolic syndrome: a deﬁnitive deﬁnition,” Medical Journal of\nAustralia, vol. 183, no. 4, p. 175, 2005.\n[4] World Health Organization and others, “Deﬁnition, diagnosis and clas-\nsiﬁcation of diabetes mellitus and its complications: report of a who\nconsultation. part 1, diagnosis and classiﬁcation of diabetes mellitus,”\nGeneva: World health organization, Tech. Rep., 1999.\n[5] Expert Panel on Detection, Evaluation and others, “Executive summary\nof the third report of the national cholesterol education program (ncep)\nexpert panel on detection, evaluation, and treatment of high blood\ncholesterol in adults (adult treatment panel iii).”JAMA, vol. 285, no. 19,\np. 2486, 2001.\n[6] K. G. M. M. Alberti, P. Zimmet, and J. Shaw, “Metabolic syndrome-a\nnew world-wide deﬁnition. a consensus statement from the international\ndiabetes federation,” Diabetic medicine, vol. 23, no. 5, pp. 469–480,\n2006.\n[7] S. M. Grundy, J. I.', ' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2534.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?","Cross-validation, AUC, Brier score, calibration plots.",,,,"[15, 3, 7]","[' a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D. G. Altman and P. Royston, “The cost of dichotomising continuous\nvariables,”Bmj, vol. 332, no. 7549, p. 1080, 2006.\n[22] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.\n5–32, 2001.\n[23] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, “Classiﬁca-\ntion and regression trees. belmont, ca: Wadsworth,”International Group,\nvol. 432, pp. 151–166, 1984.\n939\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\n[24] J. H. Friedman, “Greedy function approximation: a gradient boosting\nmachine,”Annals of statistics, pp. 1189–1232, 2001.\n[25] J. D. Kelleher, B. Mac Namee, and A. D’arcy,Fundamentals of machine\nlearning for predictive data analytics: algorithms, worked examples, and\ncase studies. MIT Press, 2015.\n[26] C. X. Ling, J. Huang, H. Zhang et al., “Auc: a statistically consistent\nand more discriminating measure than accuracy,” inIjcai, vol. 3, 2003,\npp. 519–524.\n[27] F. J. Dankers, A. Traverso, L. Wee, and S. M. van Kuijk, “Prediction\nmodeling methodology,” in Fundamentals of Clinical Data Science.\nSpringer, 2019, pp. 101–120.\n[28] A. Niculescu-Mizil and R. Caruana, “Predicting good probabilities\nwith supervised learning,” in Proceedings of the 22nd international\nconference on Machine learning. ACM, 2005, pp. 625–632.\n[29] M. H. DeGroot and S. E. Fienberg, “The comparison and evaluation\nof forecasters,”Journal of the Royal Statistical Society: Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12–22, 1983.\n[30] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,”\nMonthly weather review, vol. 78, no. 1, pp. 1–3, 1950.\n[31] B. Zadrozny and C. Elkan, “Obtaining calibrated probability estimates\nfrom decision trees and naive bayesian classiﬁers,” in Icml, vol. 1.\nCiteseer, 2001, pp. 609–616.\n[32] J. Platt et al., “Probabilistic outputs for support vector machines and\ncompar', ' pressure ≥ 128/85\nmmHg) which achieved a sensitivity of 91.6% and a speciﬁcity\nof 95.7% on the validation cohort. In a later work the method\nwas validated with a larger cohort of Spanish workers (60799\nsubjects) where a sensitivity of 54.7% and a speciﬁcity of\n94.9% was observed [18]. A low sensitivity indicates that\nproposed model incorrectly classiﬁed a lot of MetS patients\nas healthy. In the paper they also discover that cutoff for the\noptimal waist-height ratio varies across gender and different\nage groups. This indicates the need of a more sophisticated\nmodel encompassing more features.\nIII. M ETHODS\nA. Data Collection\nData collection was carried out in a mobile study center,\nwhich was placed in cities and rural areas across the state\nof Brandenburg, Germany (between 2016 and 2019). Partic-\nipants (aged 40 to 70 years old) were examined in mobile\nexamination units to diagnose MetS. Waist circumference was\nmeasured using a tape measure (Seca, Germany). The arterial\nblood pressure was measured manually by the auscultatory\nmethod using a sphygmomanometer with a stethoscope. Cap-\nillary blood was taken from the ﬁnger bulb for the analyses\nof blood glucose (in mg/dl), triglycerides and HDL choles-\nterol (in mg/dl) using the Alere Cholestech LDX analyzer\n(Alere, Germany). Additionally, body mass was measured\nusing an electronic digital scale (Seca 899, Seca, Germany).\nParticipants were asked for any drug treatment because of\ndiagnosed diabetes, arterial hypertension, hypertriglyceridemia\nor hypercholesterolemia. The study was approved by the\nethics committee of the University of Potsdam (ethics approval\nnumber: 40/2016).\nB. Cohort Description\nUsing the setting described above data from 2477 individ-\nuals were collected. After removing rows with missing values\nfor variables that would have been required to deﬁne the target\nvariable (in this case, presence of MetS) we end up with 2314\nsubjects. Following the diagnostic criteria described in section\nI, 941 subjects (40.66%) were identiﬁed as having MetS. A\nmore detailed description of the cohort can be found in Table\nII.\nC. Feature Engineering and Selection\nFollowing the work of Romero-Salda˜na et al. three addi-\ntional features were derived from the existing anthropometric\nvariables: waist to height ratio (WtHR), body mass index\n(BMI) in KG/m2 and body fat percentage (BFP) calculated\n934\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nTABLE II\nCOHORT DESCRIPTION\nFeature All (N= 2314) MetS (N = 941)\nGender M = 918 F = 1396 M=4 4 1F=5 0 0\nAge 55.20 ±8.47 57.47 ±8.16\nHeight(in cm) 170.79 ±9.04 171.16 ±9.03\nWeight(in KG) 80.31 ±15.96 88.38 ±15.46\nSystolic blood pres-\nsure(in mmHg) 127.87 ±16.27 134.09 ±15.11\nDiastolic blood pres-\nsure(in mmHg) 79.23 ±10.04 82.46 ±9.96\nWaist circumference\n(in cm) 91.46 ±13.64 100.', ' calibrated [28]. There are different ways to check for\nmodel calibration, calibration plot usually being the preferred\napproach [27], [29]. To evaluate the calibration performance\nwe also report the Brier score for each classiﬁer [30]. Brier\nscore measures the mean squared difference between the\npredicted probability assigned to the possible outcomes for\nan observation and the actual outcome. Lower the Brier score,\nthe better the predictions from the model are calibrated. The\ncalibration plots and the Brier scores were generated using a\ntest set of 463 observations (20% of the whole data) and the\nremaining data were used to train the models (using features\nfrom set A).\nAfter investigating the distortion characteristic (or calibra-\ntion) of each learning algorithm, calibration methods (e.g.\nisotonic regression, Platt’s scaling) are used to correct this\ndistortions [31], [32]. Isotonic regression in general is more\npowerful calibrator than Platt’s scaling as it can correct any\nmonotonic distortion and also unlike Platt’s, isotonic regres-\nsion is a non-parametric method. On the downside isotonic\nregression is more prone to overﬁtting when the dataset is\nsmall [28]. We report Brier scores of all the classiﬁers, both\nbefore and after the calibrations are performed.\nThe results from the experiments are discussed it details in\nthe next section.\nIV . RESULTS AND DISCUSSION\nWe start this section by comparing the discriminative perfor-\nmance of the different classiﬁer and feature set combinations.\nTable III shows the mean and the standard deviation (in\nbrackets) of all the performance metrics that were described\nin section III-E1, obtained from 5-fold cross validation for the\ncombinations. For every feature set, the algorithm with the\nbest average validation metric is highlighted with bold. When\nthe mean value is same for multiple algorithms (considering\nup to 2 decimal places) preference is given to classiﬁers with a\nlow standard deviation. The entire machine learning pipeline\nwas implemented in python using scikit-learn and the plots\nwere generated using matplotlib [33], [34].\nA. Comparing the Feature sets\nIt can be observed from Table III that set A clearly outper-\nforms the other feature sets both in terms of AUC, balanced\naccuracy and recall. This clearly supports our claim of the\nneed for models encompassing more features. Set B without\nincorporating any medicine intake information still manages to\noutperform set C and set D. Set C which is reported as the best\nperforming feature set in [17], performs poorly compared to\nthe other feature sets, most noticeably sometimes even worse\nthan set D which only comprises of the 3 features that we\ndirectly incorporate from the diagnostic criteria. Observing\nthe performance of set D it can be concluded that most of\nthe information already resides in these 3 variables. As they\ndirectly contribute to the diagnostic deﬁnition this observation\nis anticipated and self-explanatory. But comparing set D to\nset A and set B it can also be concluded that by adding more\nfeatures to the model signiﬁcant performance gains can be\nobserved. It should be noted, the recall drops sharply as the\nnumber of features are reduced.\nThe ROC curves for the different feature sets can also be\nvisualized in ﬁg. 1. The ROC curves in this case was generated\non a test dataset with 463 observations (20% of the whole data)\nand the rest of the data were used to train the models with the\nensemble classiﬁer.\nA comparison between our models and the rule-based\napproach from Romero-Salda˜na et al. was also performed [17].\nThe ﬁxed rule approach demonstrates']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2541.0,free_text
A_Machine_Learning_Approach_for_Non_Invasive_Diagnosis_of_Metabolic_Syndrome,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[14, 5, 0]","['wang, J. Jang, J. Leem, J.-Y . Park,\nH.-K. Kim, and W. Lee, “Serum bilirubin as a predictor of incident\nmetabolic syndrome: a 4-year retrospective longitudinal study of 6205\ninitially healthy korean men,” Diabetes & metabolism, vol. 40, no. 4,\npp. 305–309, 2014.\n[13] J. Olza, C. M. Aguilera, M. Gil-Campos, R. Leis, G. Bueno, M. Valle,\nR. Ca˜nete, R. Tojo, L. A. Moreno, and´A. Gil, “A continuous metabolic\nsyndrome score is associated with speciﬁc biomarkers of inﬂamma-\ntion and cvd risk in prepubertal children,” Annals of Nutrition and\nMetabolism, vol. 66, no. 2-3, pp. 72–79, 2015.\n[14] A. Bener, M. T. Yousafzai, S. Darwish, A. O. Al-Hamaq, E. A. Nasralla,\nand M. Abdul-Ghani, “Obesity index that better predict metabolic\nsyndrome: body mass index, waist circumference, waist hip ratio, or\nwaist height ratio,”Journal of obesity, vol. 2013, 2013.\n[15] G. Sagun, A. Oguz, E. Karagoz, A. T. Filizer, G. Tamer, B. Mesciet al.,\n“Application of alternative anthropometric measurements to predict\nmetabolic syndrome,”Clinics, vol. 69, no. 5, pp. 347–353, 2014.\n[16] D.-Y . Hsiung, C.-W. Liu, P.-C. Cheng, and W.-F. Ma, “Using non-\ninvasive assessment methods to predict the risk of metabolic syndrome,”\nApplied Nursing Research, vol. 28, no. 2, pp. 72–77, 2015.\n[17] M. Romero-Salda ˜na, F. J. Fuentes-Jim ´enez, M. Vaquero-Abell ´an,\nC. ´Alvarez-Fern´andez, G. Molina-Recio, and J. L´opez-Miranda, “New\nnon-invasive method for early detection of metabolic syndrome in the\nworking population,” European Journal of Cardiovascular Nursing,\nvol. 15, no. 7, pp. 549–558, 2016.\n[18] M. Romero-Salda ˜na, P. Tauler, M. Vaquero-Abell ´an, A.-A. L ´opez-\nGonz´alez, F.-J. Fuentes-Jim ´enez, A. Aguil ´o, C. ´Alvarez-Fern´andez,\nG. Molina-Recio, and M. Bennasar-Veny, “Validation of a non-invasive\nmethod for the early detection of metabolic syndrome: a diagnostic\naccuracy test in a working population,” BMJ open, vol. 8, no. 10, p.\ne020476, 2018.\n[19] P. Deurenberg, J. A. Weststrate, and J. C. Seidell, “Body mass index as\na measure of body fatness: age-and sex-speciﬁc prediction formulas,”\nBritish journal of nutrition, vol. 65, no. 2, pp. 105–114, 1991.\n[20] D. Koller and M. Sahami, “Toward optimal feature selection,” Stanford\nInfoLab, Tech. Rep., 1996.\n[21] D.', 'olic blood pressure\n(SBP), diastolic blood pressure (DBP), waist circumfer-\nence (WC), WtHR, BMI, BFP and all the medication\ninformation from table II\n2) Set B: All in set A except the medication information\n3) Set C [17]: WtHR, BFP, BMI, DBP\n4) Set D: WC, SBP, DBP\nThe difference between set A and set B is only that set\nA contains the medication information which set B does not.\nSince the medication information plays a role in the MetS\ndiagnostic criteria as well we wanted to test the models with\nand without it. More importantly, since we want our models\nto be operable on a wide variety of cohorts and we do\nrecognize that reliable medication information can be difﬁcult\nto obtain in some scenarios (e.g. electronic health records), it is\nnecessary to have a model without medications. Set C is the\nbest performing feature set as reported by Romero-Salda ˜na\net al. [17]. Set D includes the anthropometric features that\nwe directly take from the diagnostic criteria of MetS. It is\nimportant to note that unlike some previous work, we avoid\ndichotomising any continuous feature variables. The most\nnoteworthy drawback of dichotomising continuous features is\nthat individuals close to but on opposite sides of the cutoff\nare often characterized as being dissimilar rather than very\nsimilar [21]. Although, since we use the currently established\ndichotomous deﬁnition of MetS as the target variable, it\nis possible that our models implicitly learn the thresholds,\nspecially for the features that were directly taken from the\ndiagnostic criteria.\nD. Classiﬁcation Methods\nHere we brieﬂy describe the classiﬁcation algorithms we\nevaluated in this work.\n1) Logistic Regression (LR): LR works similar to linear\nregression, but with a binomial response variable. To avoid\noverﬁtting we use L2 regularization (also known as ridge\nregression).\n2) Random F orest (RF):RF is an ensemble of randomly\nconstructed independent decision trees [22]. It usually exhibits\nconsiderable performance improvements over single-tree clas-\nsiﬁer algorithms such as CART [23].\n3) Gradient Boosting Machines (GBMs):In GBMs the al-\ngorithm consecutively ﬁts new models (in this case regression\ntrees) to provide a more accurate estimate of the target variable\n[24]. Gradient boosting trains many models in an additive\nand sequential manner. Gradient boosting of regression trees\nusually produce highly robust and interpretable procedures for\nclassiﬁcation, even in situations when the training data are not\nso clean [24].\n4) Ensemble Classiﬁer: Finally we also built an ensemble\nclassiﬁer by combining the outputs from the three methods\nmentioned above (LR, RF and GBM). Instead of the more\npopular hard voting approach which selects the target label\nwhich was predicted by the majority of the classiﬁers, here we\nuse a soft voting ensemble technique which takes an average\nof output probabilities of the base classiﬁers and based on\nthe average value it decides on the target label. The primary\ndifference between the two approaches is that soft voting also\ntakes into account how certain each classiﬁer is about the\nprediction.\nE. Evaluation\nThe experiment was designed to compare each feature set\nand classiﬁcation algorithm combination against each other.\n935\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 25,2025 at 12:50:05 UTC from IEEE Xplore.  Restrictions apply. \n\nThe models are evaluated based on two main criteria, namely\nthe discriminative performance and the model calibration.\n1) Dis', 'A Machine Learning Approach for Non-Invasive\nDiagnosis of Metabolic Syndrome\nSuparno Datta∗†, Anne Schraplau‡, Harry Freitas da Cruz∗†, Jan Philipp Sachs∗†,\nFrank Mayer‡ and Erwin B¨ottinger∗†\n∗Digital Health Center\nHasso Plattner Institute gGmbH and University of Potsdam\nPotsdam, Germany\n† Hasso Plattner Institute for Digital Health at Mount Sinai\nIcahn School of Medicine at Mount Sinai\nNew York, United States\n‡University Outpatient Clinic, Sports Medicine and Sports Orthopaedics\nUniversity of Potsdam\nPotsdam, Germany\nEmail: suparno.datta@hpi.de\nAbstract—The metabolic syndrome is one of the major public\nhealth challenges worldwide. Prevalence of metabolic syndrome\nvastly increases the risk of type 2 diabetes and cardiovascular\ndiseases (CVDs). Metabolic Syndrome in general is under-\ndiagnosed and often goes undetected for years. In this paper\nwe present a machine learning based method for early detection\nof metabolic syndrome which uses only non-invasive features. We\ntrain and test our model based on data collected from a German\npopulation consisting of 2314 subjects (male = 918, female =\n1396). Out of 2314 subjects 941 were diagnosed with metabolic\nsyndrome (male = 441, female = 500). Features we consider\ninclude different anthropometric features (such as height, weight,\nwaist circumference), medications, age, gender etc.; machine\nlearning techniques we employed included gradient boosting\nmachines, random forest, logistic regression and an ensemble\nmodel. We compare our models against the ones that were\nproposed in previous literature and outperform them in our\ncohort. We achieve area under the curve values (AUCs) of up\nto 0.90 with the ensemble classiﬁer . The results achieved suggest\nthat machine learning can be a valuable tool to predict metabolic\nsyndrome with high discriminative power without relying on\nany invasive bio-markers, which signiﬁcantly facilitates early\ndetection.\nIndex T erms—Metabolic Syndrome, Machine learning, Predic-\ntive models, Biomedical informatics\nI. I NTRODUCTION\nOver the past three decades, there has been a stagger-\ning increase in the number of people with metabolic syn-\ndrome (MetS) worldwide [1]. MetS is characterized by hy-\nperglycemia, high blood pressure, central obesity and dyslipi-\ndemia. The increasing prevalence of MetS is associated with\nthe global epidemic of obesity and diabetes [2]. As a several-\nfold increase in the risk of diabetes and cardiovascular diseases\n(CVDs) is commonly associated with MetS, there is urgent\nneed for strategies to prevent this emerging global epidemic\nthat results in a considerable long-term burden for the public\nhealth system [1]–[3].\nA number of expert groups over the time have devel-\noped clinical criteria for MetS. The most widely accepted\nof these were produced by the World Health Organization\n(WHO), the National Cholesterol Education Program Third\nAdult Treatment Panel (NCEP ATP III), the International\nDiabetes Federation (IDF) and the American Heart Associa-\ntion/National Heart, Lung, and Blood Institute (AHA/NHLBI)\n[4]–[7]. To circumvent the inconsistencies arising from the\nmultiple criteria and to establish a uniﬁed working diagnostic\ntool for MetS, IDF and AHA/NHLBI together proposed a\nnew harmonized deﬁnition for this syndrome in 2009 [8].\nAccording to the new deﬁnition, the presence of any 3 of\n5 risk factors mentioned in Table I constitutes a diagnosis of\nMetS.']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2510.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q1,Is the aim of this study to predict future events (prognostic) or current disease status (diagnostic)?,Unknown from this paper.,,,,"[11, 10, 5]","[' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the', 'Ld +Lo)=1 ,\n2: identify dynamic or static activity byˆyt = f1(X,θ1)\nbased on featureσ(∑ ||Sα|l|2));\n3: if ˆyt ∈ ΛD, identify a dynamical activity on featuresD\nor segments∗\nt;\n4: else ˆyt ∈ ΛS, identify a static activity on featuresS or\nsegment s∗\nt;\n5: end if\n6: end while\nFor accuracy enhancement, the SVM model commonly utilizes\nthe radial basis function kernel and linear kernel commonly.\nAs a standard RNN method, the LSTM algorithm learns the\nlong-term dependencies based on the three gates, namely an\ninput gate, a forget gate to allow the LSTM unit to unlearn the\nprevious memory, and an output gate to decide the quantity of\nmemory transferring to the next hidden layers. However, it will\ncost ample time to optimize the parameters of the LSTM network\nand predict a result.\nC. Recognition Module\nFig. 5 shows the details of the communication network for\nmonitoring daily human activities in real time with a local LAN\nand a remote LAN. Algorithm 3 describes the procedure for\nidentifying human activities. When a new inputst is divided as a\ndynamical or static activity by the ﬁrst classiﬁerˆyt = f1(X,θ1)\n(line 1), it will be further identiﬁed as a dynamical or a static\nactivity by the second or third classiﬁer (lines 3 and 4).\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n420 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nAlgorithm 4: The online learning algorithm.\nInput: the old HC classiﬁersf(X,θ) and new predicted\ninput {s∗\nt,ˆyt};\nOutput: the updated HC classiﬁersˆf(X, ˆθ);\n1: compute covariance coefﬁcients ρr = corr(s∗\nt,s∗\nqˆyt );\n2: if ρr ≥ η\n3: update training dataset {[s∗\ni;s∗\nt],[yi;nc +1]};\n4: if ˆyt ∈ ΛD\n5: retrain the second classiﬁer ˆf2(X, ˆθ) on\n{F[s∗\ni;s∗\nt],[yi;nc +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n6: else ˆyt ∈ ΛS\n7: retrain the third classiﬁer ˆf3(X, ˆθ) on\n{F[s∗\ni;s∗\nt],[yi;nc +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe', ' ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label the activities au-\ntonomously. The HC [7] algorithms are used to establish the\nclassiﬁer for recognizing the 12 original activities (described in\nSection I). In the recognition module, the obtained HC classiﬁer\nis implemented for HAR in real time by carrying a smartphone\non the waist or putting it in the left pant pocket. In the online\nlearning module, a new activity that is not included in the 12\noriginal activities will be identiﬁed in an unsupervised learning\nmanner. Meanwhile, the old classiﬁer is updated. The signal\npreprocessing and feature extraction models are shared in both\ncreation and recognition modules.\nA. Signal Preprocessing and Feature Extraction\nThe collected signals typically exhibit various turbulence\n(e.g., magnetic ﬁelds and movement artifact), which affects the\nrecognition ability of the classiﬁer. The derived sensors signals\n(e.g., gravity and linear acceleration) that are transferred by the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 417\nFig. 3. Schematic diagram of signal preprocessing model. The 9-D raw signals [Sα (accelerometer), Sβ (gyroscope), andSγ (magnetometer)] are processed\nby the L1-norm (Manhattan distance) transfer, third-order zero phases low-pass elliptical ﬁlter (LPEF), attitude and heading reference system algorithm (AHRS\nﬁlter), and sum of gyroscope signals algorithms to provide the inputs, namely, the linear L1 norm acceleration|Sα|l, YZ-axis L1 norm orientation|S⊖ YZ |,s u m\nof gyroscope\n∑\nSβ, and L1-norm accelerometer signals|Sα|.\noriginal IMU sensors, i.e.,Sα (accelerometer), Sβ (gyroscope),\nand Sγ (magnetometer), can solve the problems.\nFirst, the linear L1-norm accelerometer|Sα| [by (4)] can ﬁx\nthe three axes of acceleration for overcoming the inﬂuence of\nthe changes in direction and position\n||Sα|| =\nLd∑\ni=1\n|sαi | (4)\nwhere sα is the subsegment of Sα with Ld length. Then, a\nthird-order zero phase low-pass elliptical ﬁlter (LPEF) [31] is\nimplemented to decompose|Sα| into the gravity and the linear\nacceleration (|Sα|l) vectors to remove the high-order noises.\nSecond, the attitude and heading reference system algo-\nrithm (AHRS ﬁlter) [32] calculates the orientation axesS⊖ and\ndetermine the smartphone’s reference system to increase the\nidentiﬁcation ability']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2469.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q2,"On what basis were eligible participants included in this study (symptoms, previous tests, registry, etc.)?",Unknown from this paper.,,,,"[2, 16, 15]","[' principal component analysis (PCA) [9], fast\n2168-2291 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 415\ncorrelation-based ﬁlter [10], and sequential forward selection\n(wrapper) [11] methods. By comparing the classiﬁcation rate of\nidentifying ﬁve daily activities carrying a smartphone on ﬁve\nbody positions, Khanet al.[12] proved that Kernel discriminant\nanalysis is better than linear discriminant analysis and signal\nmagnitude area. Shoaibet al. [13] demonstrated putting three\ntypes of sensors (i.e., accelerometer, gyroscope, and linear ac-\ncelerometer) on the wrist and pocket to identify less-repetitive\nactivities, such as smoking, eating, and drinking coffee. For\nsolving the problem of online time series segmentation, Ignatov\net al. [14] constructed the phase trajectory matrix and applied\nthe PCA technique to extract features of the ﬁrst period. Sousa\net al.[15] investigated the classiﬁcation capability with different\nfeature sets, such as time-domain (mathematical and statis-\ntical parameters), frequency-domain (wavelet transform), and\ndiscrete-domain (symbolic representations). Although the above\nworks achieved signiﬁcant progress for HAR, the limitations\nstill existed. Most of them ignored the affection of shift changes\nin position and orientation of the smartphone due to motion\nartifacts.\nDuring the past decades, many researchers have explored\nvarious ML and DL algorithms to identify more complex ac-\ntivities with higher recognition rates. Koseet al.[1] proved that\nk-nearest neighbor (k-NN) could obtain a higher classiﬁcation\naccuracy than Na¨ive Bayes (NB) by recognizing four kinds\nof activities, i.e., walking, running, sitting, and standing. Lee\nand Cho [16] designed a mixture-of-experts model for dealing\nwith uncertain and incomplete data. Reyes-Ortiz et al. [17]\nproposed a transition-aware HAR system based on the support\nvector machine (SVM) algorithm to classify a broad spectrum\n(up to 33 types) of activity with smartphone in real-time. Lee\nand Cho [18] presented a hierarchical hidden Markov model\n(HHMM) method to classify motions (standing, walking, go\nstairs, and running) and daily activities (shopping, taking a\nbus, and moving arms). A two-stage continuous hidden Markov\nmodel (HMM) algorithm was proposed to reduce the number of\nfeature subsets because it decomposed the complex activities\ninto several simpler ones [19]. Reiss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions', ' motion\nsensors,” Sensors, vol. 16, no. 4, 2016, Art. no. 426.\n[14] A. D. Ignatov and V . V . Strijov, “Human activity recognition using\nquasiperiodic time series collected from a single tri-axial accelerometer,”\nMultimedia Tools Appl., vol. 75, no. 12, pp. 7257–7270, 2016.\n[15] W. Sousa, E. Souto, J. Rodrigres, P. Sadarc, R. Jalali, and K. El-Khatib,\n“A comparative analysis of the impact of features on human activity\nrecognition with smartphone sensors,” in Proc. 23rd Brazilian Symp.\nMultimedia Web, 2017, pp. 397–404.\n[16] Y .-S. Lee and S.-B. Cho, “Activity recognition with android phone using\nmixture-of-experts co-trained with labeled and unlabeled data,”Neuro-\ncomputing, vol. 126, pp. 106–115, 2014.\n[17] J.-L. Reyes-Ortiz, L. Oneto, A. Samà, X. Parra, and D. Anguita,\n“Transition-aware human activity recognition using smartphones,”Neu-\nrocomputing, vol. 171, pp. 754–767, 2016.\n[18] Y .-S. Lee and S.-B. Cho, “Activity recognition using hierarchical hidden\nMarkov models on a smartphone with 3d accelerometer,” inProc. Int.\nConf. Hybrid Artif. Intell. Syst., 2011, pp. 460–467.\n[19] C. A. Ronao and S.-B. Cho, “Human activity recognition using smartphone\nsensors with two-stage continuous hidden Markov models,” inProc. IEEE\n10th Int. Conf. Natural Computation, 2014, pp. 681–686.\n[20] A. Reiss, G. Hendeby, and D. Stricker, “A competitive approach for human\nactivity recognition on smartphones,” inProc. Eur . Symp. Artif. Neural\nNetw., Comput. Intell. Mach. Learn., Bruges, Belgium, 2013, pp. 455–460.\n[21] M. H. M. Noor, Z. Salcic, I. Kevin, and K. Wang, “Adaptive sliding window\nsegmentation for physical activity recognition using a single tri-axial\naccelerometer,”Pervasive Mobile Comput., vol. 38, pp. 41–59, 2017.\n[22] Y . Lu, Y . Wei, L. Liu, J. Zhong, L. Sun, and Y . Liu, “Towards unsupervised\nphysical activity recognition using smartphone accelerometers,”Multime-\ndia Tools Appl., vol. 76, no. 8, pp. 10701–10719, 2017.\n[23] L. Cao, Y . Wang, B. Zhang, Q. Jin, and A. V . Vasilakos, “Gchar: An efﬁcient\ngroup-based contextaware human activity recognition on smartphone,”J.\nParallel Distrib. Comput., vol. 118, pp. 67–80, 2018.\n[24] C. A. Ronao and S.-B. Cho, “Human activity recognition with smartphone\nsensors using deep learning neural networks,”Expert Syst. Appl., vol. 59,\npp. 235–244, 2016.\n[25] N. Y . Hammerla, S. Halloran, and T. Ploetz, “Deep, convolutional, and\n', ' Lee, and T.-S. Kim, “A triaxial accelerometer-\nbased physical-activity recognition via augmented-signal features and a\nhierarchical recognizer,”IEEE Trans. Inf. Technol. Biomedicine, vol. 14,\nno. 5, pp. 1166–1172, Sep. 2010.\n[6] T. Sajana, C. S. Rani, and K. Narayana, “A survey on clustering techniques\nfor big data mining,”Indian J. Sci. Technol., vol. 9, no. 3, pp. 1–12, 2016.\n[7] C. N. Silla and A. A. Freitas, “A survey of hierarchical classiﬁcation across\ndifferent application domains,”Data Mining Knowl. Discovery, vol. 22,\nno. 1–2, pp. 31–72, 2011.\n[8] A. Wang, G. Chen, J. Yang, S. Zhao, and C.-Y . Chang, “A comparative\nstudy on human activity recognition using inertial sensors in a smart-\nphone,” IEEE Sensors J., vol. 16, no. 11, pp. 4566–4578, Jun. 2016.\n[9] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analy-\nsis,” J. Comput. Graphical Statist., vol. 15, no. 2, pp. 265–286, 2006.\n[10] S. Beura, B. Majhi, and R. Dash, “Automatic characterization of mam-\nmograms using fractal texture analysis and fast correlation based ﬁlter\nmethod,” inProc. 2nd Int. Conf. Perception Mach. Intell., 2015, pp. 85–91.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 423\n[11] G. Chandrashekar and F. Sahin, “A survey on feature selection methods,”\nComput. Elect. Eng., vol. 40, no. 1, pp. 16–28, 2014.\n[12] A. M. Khan, Y .-K. Lee, S. Lee, and T.-S. Kim, “Human activity recogni-\ntion via an accelerometer-enabled-smartphone using kernel discriminant\nanalysis,” inProc. IEEE 5th Int. Conf. Future Inf. Technol. (FutureTech),\n2010, pp. 1–6.\n[13] M. Shoaib, S. Bosch, O. D. Incel, H. Scholten, and P. J. Havinga, “Com-\nplex human activity recognition using smartphone and wrist-worn motion\nsensors,” Sensors, vol. 16, no. 4, 2016, Art. no. 426.\n[14] A. D. Ignatov and V . V . Strijov, “Human activity recognition using\nquasiperiodic time series collected from a single tri-axial accelerometer,”\nMultimedia Tools Appl., vol. 75, no. 12, pp. 7257–7270, 2016.\n[15] W. Sousa, E. Souto, J. Rodrigres, P. Sadarc, R. Jalali, and K. El-Khatib,\n“A comparative analysis of the impact of features on human activity\nrecognition with smartphone sensors,” in Proc. 23rd']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2534.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q3,"How were participants sampled in this study: by convenience, randomly, or consecutively?",Unknown from this paper,,,,"[11, 4, 7]","[' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the', ',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', 'Sα|\nare selected to compute the static features. The meanings of\nthe symbols are as follows: Standard deviationσ, average (e.g.,\n¯∑ Sβ), maximummax, minimummin, difference (recurrence\nrelation) ▽ , and the whole numbers (N).\nB. Creation Module\nThe creation module includes two steps: One aims to achieve\nautomatic labeling using the Hk-mC algorithm, and the other\none is to build the HC classiﬁer for HAR. The Hk-mC algo-\nrithm 1 is implemented in an unsupervised manner with the\nﬁxed structure. It labels the extracted features setFmjj\ns∗p\n,jj =\n1,2,...,Ly m computed on the processed segments s∗\np,p =\n1,2,...,M (lines 2 and 3). For each layer, it calculates a parti-\ntion of a dissimilarity object consisting ofkk ≤ nc medoids.\nGiven a set of M data objects S = sp, the objective is to\npartition the dataset intonc classes, with each cluster having one\nrepresentative object known as a medoidsm\njj [33]. The objective\nfunction of the k-medoids clustering is\nCostkk =\nLym∑\njj=1\n∑\nsp∈M\nDkk(sp,sm\njj) (6)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n418 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 4. Labeled tri-axis acceleration with ﬁxed activity programming.\nTABLE II\nTHE LAYERS ANDFEATURES FORCLUSTERING\nAlgorithm 1: Hierarchical k-medoids Clustering (Hk-mC).\nInput:\nthe raw IMU signalsS, the number of layersLy;\nOutput: labeled signal segments{sp,yp};\n1: divide S into M segments {sp};\n2: acquire the processed signalss∗\np by the algorithms in\nsection IV-A;\n3: extract features Fmjj\ns∗p\nbased on Table II;\n4: label the features and corresponding segments as\n{sp,yp};\nwhere Dkk is the dissimilarity between data object sp and\nmedoid sm\njj associated with cluster jj. In this article,Dkk is\nthe city block distance Dkk(sp,sm\njj)= |sp −sm\njj|. Then, the\nalgorithm clusters the features intokk classes and labels the\nsegments as{sp,yp} (line 4).\nFor clustering accuracy enhancement, the subjects were asked\nto do the 12 activities with a ﬁxed order (shown in Fig. 4). In\naddition, the number of clustering layers and order were ﬁxed\nand are reported in Table II. The mislabeled activities using the\nHk-mC algorithm will be removed or corrected manually.\nHowever, similar segments increase the computing burden for\nbuilding and updating a classiﬁer. To solve this problem for fast\ncomputation, a correlation-based data compression algorithm\nis designed to remove partially similar segments based on the\nsimilarity measurement theory [34]. The data compression algo-\nrithm aims to compare similarity by computing the covariance\ncoefﬁcient ρ (7) between every two matricessa and sb in the\nreconstructed datasetspk,k =1 ,2,...,n c with the same label,\nwhere a,b ∈ R+ and a ̸= b. For saving the compressing time, we\nadopt the seeker optimization algorithm [35] to control the speed\nby setting a trust degreeη.I fρ ≤ η, one']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,,,,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q4,How was the dataset described in this study before predictive modeling was performed?,Unknown from this paper.,,,,"[11, 4, 7]","[' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the', ',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', 'Sα|\nare selected to compute the static features. The meanings of\nthe symbols are as follows: Standard deviationσ, average (e.g.,\n¯∑ Sβ), maximummax, minimummin, difference (recurrence\nrelation) ▽ , and the whole numbers (N).\nB. Creation Module\nThe creation module includes two steps: One aims to achieve\nautomatic labeling using the Hk-mC algorithm, and the other\none is to build the HC classiﬁer for HAR. The Hk-mC algo-\nrithm 1 is implemented in an unsupervised manner with the\nﬁxed structure. It labels the extracted features setFmjj\ns∗p\n,jj =\n1,2,...,Ly m computed on the processed segments s∗\np,p =\n1,2,...,M (lines 2 and 3). For each layer, it calculates a parti-\ntion of a dissimilarity object consisting ofkk ≤ nc medoids.\nGiven a set of M data objects S = sp, the objective is to\npartition the dataset intonc classes, with each cluster having one\nrepresentative object known as a medoidsm\njj [33]. The objective\nfunction of the k-medoids clustering is\nCostkk =\nLym∑\njj=1\n∑\nsp∈M\nDkk(sp,sm\njj) (6)\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n418 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 4. Labeled tri-axis acceleration with ﬁxed activity programming.\nTABLE II\nTHE LAYERS ANDFEATURES FORCLUSTERING\nAlgorithm 1: Hierarchical k-medoids Clustering (Hk-mC).\nInput:\nthe raw IMU signalsS, the number of layersLy;\nOutput: labeled signal segments{sp,yp};\n1: divide S into M segments {sp};\n2: acquire the processed signalss∗\np by the algorithms in\nsection IV-A;\n3: extract features Fmjj\ns∗p\nbased on Table II;\n4: label the features and corresponding segments as\n{sp,yp};\nwhere Dkk is the dissimilarity between data object sp and\nmedoid sm\njj associated with cluster jj. In this article,Dkk is\nthe city block distance Dkk(sp,sm\njj)= |sp −sm\njj|. Then, the\nalgorithm clusters the features intokk classes and labels the\nsegments as{sp,yp} (line 4).\nFor clustering accuracy enhancement, the subjects were asked\nto do the 12 activities with a ﬁxed order (shown in Fig. 4). In\naddition, the number of clustering layers and order were ﬁxed\nand are reported in Table II. The mislabeled activities using the\nHk-mC algorithm will be removed or corrected manually.\nHowever, similar segments increase the computing burden for\nbuilding and updating a classiﬁer. To solve this problem for fast\ncomputation, a correlation-based data compression algorithm\nis designed to remove partially similar segments based on the\nsimilarity measurement theory [34]. The data compression algo-\nrithm aims to compare similarity by computing the covariance\ncoefﬁcient ρ (7) between every two matricessa and sb in the\nreconstructed datasetspk,k =1 ,2,...,n c with the same label,\nwhere a,b ∈ R+ and a ̸= b. For saving the compressing time, we\nadopt the seeker optimization algorithm [35] to control the speed\nby setting a trust degreeη.I fρ ≤ η, one']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2479.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q5,"What was the proedure for splitting the dataset into training, validation, and test sets in this study?",Unknown from this paper.,,,,"[4, 8, 11]","[',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', '.\nHowever, similar segments increase the computing burden for\nbuilding and updating a classiﬁer. To solve this problem for fast\ncomputation, a correlation-based data compression algorithm\nis designed to remove partially similar segments based on the\nsimilarity measurement theory [34]. The data compression algo-\nrithm aims to compare similarity by computing the covariance\ncoefﬁcient ρ (7) between every two matricessa and sb in the\nreconstructed datasetspk,k =1 ,2,...,n c with the same label,\nwhere a,b ∈ R+ and a ̸= b. For saving the compressing time, we\nadopt the seeker optimization algorithm [35] to control the speed\nby setting a trust degreeη.I fρ ≤ η, one of the two segments (sa\nand sb) will be deleted. In this article,η =0 .98.\nρ = corr(sa,sb)\n=\n∑\nii(saii − ¯sa)(sbii − ¯sb)√∑ (sa − ¯sa)2\n√∑ (sb − ¯sbii)2\n= ⟨sa − ¯sa,sb − ¯sb⟩\n||sa − ¯sa||||sb − ¯sb||. (7)\nFinally, the compressed training dataset {si,yi},i =\n1,2,...,N is used to train the classiﬁerfj(X,θj),j =1 ,2,3\nbased on random ML or DL algorithms. The ﬁrst binary classiﬁer\nf1(X,θ1)is to divide static and dynamical activities by the single\nfeature σ(∑ ||Sα|l|2)) (line 1). The second six-class classiﬁer\nf2(X,θ2)separates the dynamical activities based on the feature\nset D reported in Table I (ML-based) or the labeled segments\n{s∗\ni,yi} (DL-based). Similarly, the third classiﬁer f3(X,θ3)\ndivides the seven static activities by the feature setS (ML-based)\nor the labeled segments{s∗\ni,yi}(DL-based) in line 3.\nFive ML algorithms (i.e.,k-NN, DT, NB, ANN, and SVM) and\na DL method (LSTM) are chosen. As a nonparametric approach,\nthe k-NN algorithm can search a group ofk samples that are\nnearest to unknown samples based on distance functions. In this\narticle, we adopt the city block distance. The computed class\nattributes of thek-NN in thesek samples determined the labels.\nAs a result, the parameterk was deﬁned utilizing a bootstrap\nprocedure. However, thei-NN method needs to handle missing\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 419\nFig. 5. Schematic diagram of the Ada-HAR communication system. In the “personal monitoring” unit, the subject’s activity can be identiﬁed by carryingt h e\nsmartphone where it has installed software with the proposed algorithms. In the “local monitoring” unit, people can monitor the users’ activity by analyzing the\nreceived data from the Wi-Fi network. In the “remote monitoring” unit, the predicted activities can be monitored by receiving the signals in a multipoint wireless\nbridge connection network.\nAlgorithm 2: Hierarchical Classiﬁcation (HC).\nInput: the labeled signal segments(s∗\ni,yi) or features set\n', ' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2466.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q6,What preprocessing techniques on the included variables/features were applied in this study?,Unknown from this paper.,,,,"[3, 4, 2]","['iss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions was strongly dependent on the extracted\nfeatures from the used sensors. Different features were utilized\nfor achieving several kinds of HAR problems [22]. An efﬁcient\ngroup-based context-aware classiﬁcation method has been pro-\nposed to improve the classiﬁcation performance by exploiting\nhierarchical group-based scheme and context awareness rather\nthan the intensive computation [23].\nRecently, the advancement of HAR with inertial measurement\nunit (IMU) sensor broadly adopts DL algorithms. Ronao and\nCho [24] designed a three-(and four-) layer convolutional neural\nnetworks (CNN) with the short-time fast Fourier transform\nmethod for classifying six activities with a high classiﬁcation\naccuracy by comparing with the traditional ML algorithms, i.e.,\nNB, ANN, and j48 decision tree (DT). Hammerlaet al. [25]\nproved that bidirectional long short-term memory (Bi-LSTM)\nobtained more accurate results than deep NN, CNN, and other\nrecurrent neural network (RNN) by testing on three public\ndatasets. Although the DL-based approaches were widely known\nto extract features automatically, they were affected by the\nselected sensors. Wang et al. [8] compared the performance\nobtained using different inertial sensors in a smartphone. Using\nboth accelerometer and gyroscope was demonstrated to yield\naccuracy enhancement. In our previous work, we compared\nthe recognition rate for identifying 12 activities by using dif-\nferent combined sensors, such as accelerometer, gyroscope,\nmagnetometer, and orientation. By combining accelerometer,\ngyroscope, and direction, the designed deep CNN model could\nobtain high accuracy [26], [27]. The results proved that adopting\ntransferred signals and more sensors can perform the results.\nMoreover, the DL-based methods were time-consuming for\nclassiﬁer evolution in a dynamic situation [28].\nIII. PROBLEM STATEMENT\nThe following notations describe the data stream of the Ada-\nHAR system through training, testing, and updating procedures,\nas shown in Fig. 1. The embedded IMU sensors in a smartphone\ncan provide nine-dimensional (9-D) raw signalsS, including\nacceleration, magnetism, and angular velocity [29].S can be\ndivided into M segments, namely sp,p =1 ,2,...,M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile,', ',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', ' principal component analysis (PCA) [9], fast\n2168-2291 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 415\ncorrelation-based ﬁlter [10], and sequential forward selection\n(wrapper) [11] methods. By comparing the classiﬁcation rate of\nidentifying ﬁve daily activities carrying a smartphone on ﬁve\nbody positions, Khanet al.[12] proved that Kernel discriminant\nanalysis is better than linear discriminant analysis and signal\nmagnitude area. Shoaibet al. [13] demonstrated putting three\ntypes of sensors (i.e., accelerometer, gyroscope, and linear ac-\ncelerometer) on the wrist and pocket to identify less-repetitive\nactivities, such as smoking, eating, and drinking coffee. For\nsolving the problem of online time series segmentation, Ignatov\net al. [14] constructed the phase trajectory matrix and applied\nthe PCA technique to extract features of the ﬁrst period. Sousa\net al.[15] investigated the classiﬁcation capability with different\nfeature sets, such as time-domain (mathematical and statis-\ntical parameters), frequency-domain (wavelet transform), and\ndiscrete-domain (symbolic representations). Although the above\nworks achieved signiﬁcant progress for HAR, the limitations\nstill existed. Most of them ignored the affection of shift changes\nin position and orientation of the smartphone due to motion\nartifacts.\nDuring the past decades, many researchers have explored\nvarious ML and DL algorithms to identify more complex ac-\ntivities with higher recognition rates. Koseet al.[1] proved that\nk-nearest neighbor (k-NN) could obtain a higher classiﬁcation\naccuracy than Na¨ive Bayes (NB) by recognizing four kinds\nof activities, i.e., walking, running, sitting, and standing. Lee\nand Cho [16] designed a mixture-of-experts model for dealing\nwith uncertain and incomplete data. Reyes-Ortiz et al. [17]\nproposed a transition-aware HAR system based on the support\nvector machine (SVM) algorithm to classify a broad spectrum\n(up to 33 types) of activity with smartphone in real-time. Lee\nand Cho [18] presented a hierarchical hidden Markov model\n(HHMM) method to classify motions (standing, walking, go\nstairs, and running) and daily activities (shopping, taking a\nbus, and moving arms). A two-stage continuous hidden Markov\nmodel (HMM) algorithm was proposed to reduce the number of\nfeature subsets because it decomposed the complex activities\ninto several simpler ones [19]. Reiss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2503.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q7,How is missing data handled in this study?,Unknown from this paper.,,,,"[4, 9, 2]","[',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', ' ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 419\nFig. 5. Schematic diagram of the Ada-HAR communication system. In the “personal monitoring” unit, the subject’s activity can be identiﬁed by carryingt h e\nsmartphone where it has installed software with the proposed algorithms. In the “local monitoring” unit, people can monitor the users’ activity by analyzing the\nreceived data from the Wi-Fi network. In the “remote monitoring” unit, the predicted activities can be monitored by receiving the signals in a multipoint wireless\nbridge connection network.\nAlgorithm 2: Hierarchical Classiﬁcation (HC).\nInput: the labeled signal segments(s∗\ni,yi) or features set\n{Fhj\ns∗\ni\n,yi};\nOutput: the three layers HC classiﬁerfj(X,θj),j =1 ,2,3;\n1: build a binary classiﬁer f1(X,θ1) to classify dynamic\nand static activities by the featureσ(∑ ||Sα|l|2));\n2: train a six-class classiﬁer f2(X,θ2) to identify\ndynamical activities;\n3: establish a seven-class classiﬁer f3(X,θ3) to divide\nthe static activities;\ndata. It is sensitive to class outliers and lots of irrelevant at-\ntributes [36]. Different from thek-NN method, the DT approach\nneeds to set two parameters, namely the number of trees and the\nnumber of features in each split. A large number of trees can\nprovide a robust model for predicting real-world results [37].\nHowever, this operation should optimize more parameters such\nthat it will be time-consuming. The NB approach assumes that\nall variables are mutually correlated and contribute towards\nclassiﬁcation. When the class variable is given, the NB classiﬁer\nconsiders that the presence (or absence) of a particular attribute\nof a class is unrelated to the presence (or absence) of any other\nfeatures. However, the strong assumption on the shape of data\ndistribution limits the performance of the built classiﬁer [38].\nThe ANN method is a widely used method for classiﬁcation [39].\nIt combines several processing layers using simple elements\noperating in parallel to build the connection between inputs and\noutputs. However, the problems of overﬁtting and underﬁtting\nbring too much uncertainty learning in a dynamic situation. The\nSVM algorithm is to distinctly classify the signal segmentss∗\ni\nby ﬁnding a hyperplane in anN-dimensional (N-D) space. There\nare two primary parameters of the SVM approach, namely the\noptimum parameters of cost and the kernel width parameter.\nAlgorithm 3: The recognition algorithm.\nInput: the HC classiﬁersf(X,θ), current inputst, sample\nfrequency Fs, detection lengthLd, and overlayLo;\nOutput:\npredicted activitiesˆyt;\n1: while (t·Fs)/(Ld +Lo)=1 ,\n2: identify dynamic or static activity byˆyt = f1(X,θ1)\nbased on featureσ(∑ ||Sα|l|2));\n3: if ˆyt ∈ ΛD, identify a dynamical activity on featuresD\nor segments∗\nt;\n4: else ˆyt ∈ ΛS, identify a static activity on featuresS or\nsegment s∗\nt;\n5: end if\n6: end while\nFor accuracy enhancement, the SVM model commonly utilizes\nthe radial basis function kernel and linear kernel commonly.\nAs a standard RNN method, the LSTM algorithm learns the\nlong-term dependencies based on the three gates, namely an\ninput gate, a forget gate to allow the L', ' principal component analysis (PCA) [9], fast\n2168-2291 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 415\ncorrelation-based ﬁlter [10], and sequential forward selection\n(wrapper) [11] methods. By comparing the classiﬁcation rate of\nidentifying ﬁve daily activities carrying a smartphone on ﬁve\nbody positions, Khanet al.[12] proved that Kernel discriminant\nanalysis is better than linear discriminant analysis and signal\nmagnitude area. Shoaibet al. [13] demonstrated putting three\ntypes of sensors (i.e., accelerometer, gyroscope, and linear ac-\ncelerometer) on the wrist and pocket to identify less-repetitive\nactivities, such as smoking, eating, and drinking coffee. For\nsolving the problem of online time series segmentation, Ignatov\net al. [14] constructed the phase trajectory matrix and applied\nthe PCA technique to extract features of the ﬁrst period. Sousa\net al.[15] investigated the classiﬁcation capability with different\nfeature sets, such as time-domain (mathematical and statis-\ntical parameters), frequency-domain (wavelet transform), and\ndiscrete-domain (symbolic representations). Although the above\nworks achieved signiﬁcant progress for HAR, the limitations\nstill existed. Most of them ignored the affection of shift changes\nin position and orientation of the smartphone due to motion\nartifacts.\nDuring the past decades, many researchers have explored\nvarious ML and DL algorithms to identify more complex ac-\ntivities with higher recognition rates. Koseet al.[1] proved that\nk-nearest neighbor (k-NN) could obtain a higher classiﬁcation\naccuracy than Na¨ive Bayes (NB) by recognizing four kinds\nof activities, i.e., walking, running, sitting, and standing. Lee\nand Cho [16] designed a mixture-of-experts model for dealing\nwith uncertain and incomplete data. Reyes-Ortiz et al. [17]\nproposed a transition-aware HAR system based on the support\nvector machine (SVM) algorithm to classify a broad spectrum\n(up to 33 types) of activity with smartphone in real-time. Lee\nand Cho [18] presented a hierarchical hidden Markov model\n(HHMM) method to classify motions (standing, walking, go\nstairs, and running) and daily activities (shopping, taking a\nbus, and moving arms). A two-stage continuous hidden Markov\nmodel (HMM) algorithm was proposed to reduce the number of\nfeature subsets because it decomposed the complex activities\ninto several simpler ones [19]. Reiss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2486.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q8,How are outliers handled in this study?,Unknown from this paper.,,,,"[17, 4, 15]","[' 2017.\n[23] L. Cao, Y . Wang, B. Zhang, Q. Jin, and A. V . Vasilakos, “Gchar: An efﬁcient\ngroup-based contextaware human activity recognition on smartphone,”J.\nParallel Distrib. Comput., vol. 118, pp. 67–80, 2018.\n[24] C. A. Ronao and S.-B. Cho, “Human activity recognition with smartphone\nsensors using deep learning neural networks,”Expert Syst. Appl., vol. 59,\npp. 235–244, 2016.\n[25] N. Y . Hammerla, S. Halloran, and T. Ploetz, “Deep, convolutional, and\nrecurrent models for human activity recognition using wearables,” 2016,\narXiv:1604.08880.\n[26] W. Qi, H. Su, C. Yang, G. Ferrigno, E. De Momi, and A. Aliverti,\n“A fast and robust deep convolutional neural networks for complex\nhuman activity recognition using smartphone,”Sensors, vol. 19, 2019,\nArt. no. 3731.\n[27] W. Qi and A. Aliverti, “A multimodal wearable system for continuous\nand real-time breathing pattern monitoring during daily activity,”IEEE J.\nBiomed. Health Inform., 2019.\n[28] Y . Liang, Z. Cai, J. Yu, Q. Han, and Y . Li, “Deep learning based inference\nof private information using embedded sensors in smart devices,”IEEE\nNetw., vol. 32, no. 4, pp. 8–14, Jul./Aug. 2018.\n[29] O. Särkkä, T. Nieminen, S. Suuriniemi, and L. Kettunen, “A multi-position\ncalibration method for consumer-grade accelerometers, gyroscopes, and\nmagnetometers to ﬁeld conditions,” IEEE Sensors J., vol. 17, no. 11,\npp. 3470–3481, Jun. 2017.\n[30] J. O. Laguna, A. G. Olaya, and D. Borrajo, “A dynamic sliding window\napproach for activity recognition,” inProc. Int. Conf. User Model., Adap-\ntation, Personalization, 2011, pp. 219–230.\n[31] P. Kanjiya, V . M. Khadkikar, and M. S. ElMoursi, “Adaptive low-pass ﬁlter\nbased dc offset removal technique for three-phase PLLs,”IEEE Trans. Ind.\nElectron., vol. 65, no. 11, pp. 9025–9029, Nov. 2018.\n[32] D. Roetenberg, H. J. Luinge, C. T. Baten, and P. H. Veltink, “Compensation\nof magnetic disturbances improves inertial and magnetic sensing of human\nbody segment orientation,”IEEE Trans. Neural Syst. Rehabil. Eng., vol. 13,\nno. 3, pp. 395–405, Sep. 2005.\n[33] A. Rai and S. Upadhyay, “Bearing performance degradation assess-\nment based on a combination of empirical mode decomposition and\nk-medoids clustering,” Mech. Syst. Signal Process., vol. 93, pp. 16–29,\n2017.\n[34] R. Boddy and G. Smith,Statistical Methods in Practice: For Scientists\nand Technologists. Chichester, U.K.: Wiley, 2009.\n[35] C. Dai, W. Chen, and Y . Zhu', ',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', ' Lee, and T.-S. Kim, “A triaxial accelerometer-\nbased physical-activity recognition via augmented-signal features and a\nhierarchical recognizer,”IEEE Trans. Inf. Technol. Biomedicine, vol. 14,\nno. 5, pp. 1166–1172, Sep. 2010.\n[6] T. Sajana, C. S. Rani, and K. Narayana, “A survey on clustering techniques\nfor big data mining,”Indian J. Sci. Technol., vol. 9, no. 3, pp. 1–12, 2016.\n[7] C. N. Silla and A. A. Freitas, “A survey of hierarchical classiﬁcation across\ndifferent application domains,”Data Mining Knowl. Discovery, vol. 22,\nno. 1–2, pp. 31–72, 2011.\n[8] A. Wang, G. Chen, J. Yang, S. Zhao, and C.-Y . Chang, “A comparative\nstudy on human activity recognition using inertial sensors in a smart-\nphone,” IEEE Sensors J., vol. 16, no. 11, pp. 4566–4578, Jun. 2016.\n[9] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analy-\nsis,” J. Comput. Graphical Statist., vol. 15, no. 2, pp. 265–286, 2006.\n[10] S. Beura, B. Majhi, and R. Dash, “Automatic characterization of mam-\nmograms using fractal texture analysis and fast correlation based ﬁlter\nmethod,” inProc. 2nd Int. Conf. Perception Mach. Intell., 2015, pp. 85–91.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 423\n[11] G. Chandrashekar and F. Sahin, “A survey on feature selection methods,”\nComput. Elect. Eng., vol. 40, no. 1, pp. 16–28, 2014.\n[12] A. M. Khan, Y .-K. Lee, S. Lee, and T.-S. Kim, “Human activity recogni-\ntion via an accelerometer-enabled-smartphone using kernel discriminant\nanalysis,” inProc. IEEE 5th Int. Conf. Future Inf. Technol. (FutureTech),\n2010, pp. 1–6.\n[13] M. Shoaib, S. Bosch, O. D. Incel, H. Scholten, and P. J. Havinga, “Com-\nplex human activity recognition using smartphone and wrist-worn motion\nsensors,” Sensors, vol. 16, no. 4, 2016, Art. no. 426.\n[14] A. D. Ignatov and V . V . Strijov, “Human activity recognition using\nquasiperiodic time series collected from a single tri-axial accelerometer,”\nMultimedia Tools Appl., vol. 75, no. 12, pp. 7257–7270, 2016.\n[15] W. Sousa, E. Souto, J. Rodrigres, P. Sadarc, R. Jalali, and K. El-Khatib,\n“A comparative analysis of the impact of features on human activity\nrecognition with smartphone sensors,” in Proc. 23rd']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2499.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q9,Which prediction models were used in this study?,"k-NN, DT, NB, ANN, SVM, and LSTM",,,,"[4, 12, 11]","[',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', ' (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the average testing time and training\ntime computed on the compressed dataset are signiﬁcantly less\nthan those for the uncompressed dataset. Hence, compressing the\ntraining datasets saves more time for building a new classiﬁer.\nEven if the accuracy obtained on the uncompressed dataset is\ngreater than that on the compressed dataset, it is time-consuming\nto build the classiﬁer and predict an activity. Moreover, the trust\ndegree η can be adjusted to increase the accuracy using (7).\nB. Online Learning Examination\nThis experiment aims to prove the updating ability of Ada-\nHAR system for recognizing a new activity. Two new activities\n(squats and twisting hips) were collected from one subject\ncarrying the smartphone on the waist or put in the left pocket,\nrespectively. To conﬁrm that the proposed signal processing al-\ngorithms could overcome the effect of changing the direction of\nmobile phone, we combined the collected data of two directions\nwith the same testing protocol (shown in each top graph of\nFig. 6). The DL-based HC algorithm was not adopted because\nthe training procedure is costly. The experiments adopted an\nonline batch learning mechanism. The SVM method used the\nGaussian kernel. The built ML-based classiﬁers for identifying\nthe 12 original activities were used to predict the new activity\non the four datasets. The following four experiments were per-\nformed to evaluate the adaptive performance of both compressed\nand uncompressed datasets.\nFig. 6(a) displays a comparison results of the online accuracy\nand computational time for predicting squats by carrying the\nmobile phone on the waist. The top graph is the collected 3-axis\nacceleration with two directions. The middle graph is the online\nclassiﬁcation accuracy computed by (1) and (2) at time (epoch)t.\nBoth k-NN and SVM methods obtain a higher accuracy than the\nother methods. The bottom graph is the online predicting time.\nThe amplitudes ofk-NN, DT, and SVM are 0.05 s, while the other\napproaches require more than 1 s to predict activity. Hence, the\nk-NN-based HC classiﬁer is the best method for recognizing\nthe new activity. Fig. 6(b) portrays the results of predicting\nsquats by putting the mobile phone in the left pant pocket. The\nHC classiﬁers based onk-NN, SVM, and DT obtain a higher\naccuracy for identifying squats, andk-NN spends less time to\nretrain the classiﬁer. Fig. 6(c) and (d) shows the validation\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 421\nTABLE III\nTHE COMPARATIVERESULTS AMONG HC CLASSIFIER BASED ON K-NN, DT', ' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2481.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q10,What considerations were given to model selection and hyperparameter tuning in this study?,Unknown from this paper.,,,,"[3, 18, 0]","['iss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions was strongly dependent on the extracted\nfeatures from the used sensors. Different features were utilized\nfor achieving several kinds of HAR problems [22]. An efﬁcient\ngroup-based context-aware classiﬁcation method has been pro-\nposed to improve the classiﬁcation performance by exploiting\nhierarchical group-based scheme and context awareness rather\nthan the intensive computation [23].\nRecently, the advancement of HAR with inertial measurement\nunit (IMU) sensor broadly adopts DL algorithms. Ronao and\nCho [24] designed a three-(and four-) layer convolutional neural\nnetworks (CNN) with the short-time fast Fourier transform\nmethod for classifying six activities with a high classiﬁcation\naccuracy by comparing with the traditional ML algorithms, i.e.,\nNB, ANN, and j48 decision tree (DT). Hammerlaet al. [25]\nproved that bidirectional long short-term memory (Bi-LSTM)\nobtained more accurate results than deep NN, CNN, and other\nrecurrent neural network (RNN) by testing on three public\ndatasets. Although the DL-based approaches were widely known\nto extract features automatically, they were affected by the\nselected sensors. Wang et al. [8] compared the performance\nobtained using different inertial sensors in a smartphone. Using\nboth accelerometer and gyroscope was demonstrated to yield\naccuracy enhancement. In our previous work, we compared\nthe recognition rate for identifying 12 activities by using dif-\nferent combined sensors, such as accelerometer, gyroscope,\nmagnetometer, and orientation. By combining accelerometer,\ngyroscope, and direction, the designed deep CNN model could\nobtain high accuracy [26], [27]. The results proved that adopting\ntransferred signals and more sensors can perform the results.\nMoreover, the DL-based methods were time-consuming for\nclassiﬁer evolution in a dynamic situation [28].\nIII. PROBLEM STATEMENT\nThe following notations describe the data stream of the Ada-\nHAR system through training, testing, and updating procedures,\nas shown in Fig. 1. The embedded IMU sensors in a smartphone\ncan provide nine-dimensional (9-D) raw signalsS, including\nacceleration, magnetism, and angular velocity [29].S can be\ndivided into M segments, namely sp,p =1 ,2,...,M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile,', ' inertial and magnetic sensing of human\nbody segment orientation,”IEEE Trans. Neural Syst. Rehabil. Eng., vol. 13,\nno. 3, pp. 395–405, Sep. 2005.\n[33] A. Rai and S. Upadhyay, “Bearing performance degradation assess-\nment based on a combination of empirical mode decomposition and\nk-medoids clustering,” Mech. Syst. Signal Process., vol. 93, pp. 16–29,\n2017.\n[34] R. Boddy and G. Smith,Statistical Methods in Practice: For Scientists\nand Technologists. Chichester, U.K.: Wiley, 2009.\n[35] C. Dai, W. Chen, and Y . Zhu, “Seeker optimization algorithm for digital\nIIR ﬁlter design,”IEEE Trans. Ind. Electron., vol. 57, no. 5, pp. 1710–1718,\nMay 2009.\n[36] N. M. Hewahi and M. K. Saad, “Class outliers mining: Distance-based\napproach.” Int. J. Intell. Syst. Techn., vol. 2, no. 1, pp. 55–68, 2007.\n[37] A. Liaw et al., “Classiﬁcation and regression by randomforest,”RN e w s,\nvol. 2, no. 3, pp. 18–22, 2002.\n[38] S. D. Jadhav and H. Channe, “Comparative study of k-NN, Naive Bayes\nand decision tree classiﬁcation techniques,”Int. J. Sci. Res., vol. 5, no. 1,\npp. 1842–1845, 2016.\n[39] S. J. Russell and P. Norvig,Artiﬁcial Intelligence: A Modern Approach.\nMalaysia: Pearson Education Limited, 2016.\n[40] Y . Wang, X. Jia, Q. Jin, and J. Ma, “Mobile crowdsourcing: framework,\nchallenges, and solutions,”Concurrency Comput.: Practice Experience,\nvol. 29, no. 3, 2017, Art. no. e3789.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. ', '414 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nA Smartphone-Based Adaptive Recognition and\nReal-Time Monitoring System for Human Activities\nWen Qi , Student Member , IEEE,H a n gS u, Member , IEEE, and Andrea Aliverti\nAbstract—Human activity recognition (HAR) using smart-\nphones provides signiﬁcant healthcare guidance for telemedicine\nand long-term treatment. Machine learning and deep learning\n(DL) techniques are widely utilized for the scientiﬁc study of the\nstatistical models of human behaviors. However, the performance\nof existing HAR platforms is limited by complex physical activity.\nIn this article, we proposed an adaptive recognition and real-time\nmonitoring system for human activities (Ada-HAR), which is ex-\npected to identify more human motions in dynamic situations. The\nAda-HAR framework introduces an unsupervised online learning\nalgorithm that is independent of the number of class constraints.\nFurthermore, the adopted hierarchical clustering and classiﬁca-\ntion algorithms label and classify 12 activities (ﬁve dynamics, six\nstatics, and a series of transitions) autonomously. Finally, practical\nexperiments have been performed to validate the effectiveness and\nrobustness of the proposed algorithms. Compared with the methods\nmentioned in the literature, the results show that the DL-based\nclassiﬁer obtains a higher recognition rate (95.15%, waist, and\n92.20%, pocket). The decision-tree-based classiﬁer is the fastest\nmethod for modal evolution. Finally, the Ada-HAR system can\nmonitor human activity in real time, regardless of the direction\nof the smartphone.\nIndex Terms —Data compression, deep learning (DL),\nhierarchical classiﬁcation (HC), human activity recognition\n(HAR).\nI. INTRODUCTION\nR\nECOGNIZING human activities using inertial sensors in\na smartphone has attracted increasing research interests\nduring the past decades in various domains, ranging from home\nhealthcare to sports monitoring, rehabilitation, personalized\nmedicine, and mental disorders [1]. Recently, with the develop-\nment of the Internet of Things, machine learning (ML), and deep\nlearning (DL) techniques, human activity recognition (HAR)\ncan be achieved by transferring data in the body area networks\nand wireless Ethernet, allowing the assessment of the human\nphysical and physiological status [2]. However, many studies\ndevelop appropriate tasks for a given HAR system by resorting\nto extensive heuristic knowledge [3]. They are applicable in a\nlaboratory or well-controlled situation via body-ﬁxed mobile\nManuscript received May 24, 2019; revised September 2, 2019 and January\n1, 2020; accepted February 23, 2020. Date of publication April 24, 2020; date of\ncurrent version September 15, 2020. This article was recommended by Associate\nEditor Bin Guo.(Corresponding author: Wen Qi.)\nThe authors are with the Dipartimento di Elettronica, Informazione\ne Bioingegneria, Politecnico di Milano, 20133 Milano, Italy (e-mail:\nwen.qi@polimi.it; hang.su@polimi.it; andrea.aliverti@polimi.it).\nColor versions of one or more of the ﬁgures in this article are available online\nat http://ieeexplore.ieee.org.\nDigital Object Identiﬁer 10.1109/THMS.2020.2984181\ndevices [4]. The variability of the disturbances of the mobile\ndevices affects the recognition rate of the HAR system, such as\nmovement artifacts, baseline noise, the happening of new activ-\nity, and the differences among users. For example, identifying\nthe same walking status of the elderly and']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2242.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q11,How was data augmentation or generation used in this study?,Data compression to remove similar segments.,,,,"[4, 18, 11]","[',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', ' inertial and magnetic sensing of human\nbody segment orientation,”IEEE Trans. Neural Syst. Rehabil. Eng., vol. 13,\nno. 3, pp. 395–405, Sep. 2005.\n[33] A. Rai and S. Upadhyay, “Bearing performance degradation assess-\nment based on a combination of empirical mode decomposition and\nk-medoids clustering,” Mech. Syst. Signal Process., vol. 93, pp. 16–29,\n2017.\n[34] R. Boddy and G. Smith,Statistical Methods in Practice: For Scientists\nand Technologists. Chichester, U.K.: Wiley, 2009.\n[35] C. Dai, W. Chen, and Y . Zhu, “Seeker optimization algorithm for digital\nIIR ﬁlter design,”IEEE Trans. Ind. Electron., vol. 57, no. 5, pp. 1710–1718,\nMay 2009.\n[36] N. M. Hewahi and M. K. Saad, “Class outliers mining: Distance-based\napproach.” Int. J. Intell. Syst. Techn., vol. 2, no. 1, pp. 55–68, 2007.\n[37] A. Liaw et al., “Classiﬁcation and regression by randomforest,”RN e w s,\nvol. 2, no. 3, pp. 18–22, 2002.\n[38] S. D. Jadhav and H. Channe, “Comparative study of k-NN, Naive Bayes\nand decision tree classiﬁcation techniques,”Int. J. Sci. Res., vol. 5, no. 1,\npp. 1842–1845, 2016.\n[39] S. J. Russell and P. Norvig,Artiﬁcial Intelligence: A Modern Approach.\nMalaysia: Pearson Education Limited, 2016.\n[40] Y . Wang, X. Jia, Q. Jin, and J. Ma, “Mobile crowdsourcing: framework,\nchallenges, and solutions,”Concurrency Comput.: Practice Experience,\nvol. 29, no. 3, 2017, Art. no. e3789.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. ', ' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2197.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q12,Is the performance of the predictive models benchmarked or compared to a baseline?,"Yes, compared among ML and DL classifiers.",,,,"[11, 12, 4]","[' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the', ' (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the average testing time and training\ntime computed on the compressed dataset are signiﬁcantly less\nthan those for the uncompressed dataset. Hence, compressing the\ntraining datasets saves more time for building a new classiﬁer.\nEven if the accuracy obtained on the uncompressed dataset is\ngreater than that on the compressed dataset, it is time-consuming\nto build the classiﬁer and predict an activity. Moreover, the trust\ndegree η can be adjusted to increase the accuracy using (7).\nB. Online Learning Examination\nThis experiment aims to prove the updating ability of Ada-\nHAR system for recognizing a new activity. Two new activities\n(squats and twisting hips) were collected from one subject\ncarrying the smartphone on the waist or put in the left pocket,\nrespectively. To conﬁrm that the proposed signal processing al-\ngorithms could overcome the effect of changing the direction of\nmobile phone, we combined the collected data of two directions\nwith the same testing protocol (shown in each top graph of\nFig. 6). The DL-based HC algorithm was not adopted because\nthe training procedure is costly. The experiments adopted an\nonline batch learning mechanism. The SVM method used the\nGaussian kernel. The built ML-based classiﬁers for identifying\nthe 12 original activities were used to predict the new activity\non the four datasets. The following four experiments were per-\nformed to evaluate the adaptive performance of both compressed\nand uncompressed datasets.\nFig. 6(a) displays a comparison results of the online accuracy\nand computational time for predicting squats by carrying the\nmobile phone on the waist. The top graph is the collected 3-axis\nacceleration with two directions. The middle graph is the online\nclassiﬁcation accuracy computed by (1) and (2) at time (epoch)t.\nBoth k-NN and SVM methods obtain a higher accuracy than the\nother methods. The bottom graph is the online predicting time.\nThe amplitudes ofk-NN, DT, and SVM are 0.05 s, while the other\napproaches require more than 1 s to predict activity. Hence, the\nk-NN-based HC classiﬁer is the best method for recognizing\nthe new activity. Fig. 6(b) portrays the results of predicting\nsquats by putting the mobile phone in the left pant pocket. The\nHC classiﬁers based onk-NN, SVM, and DT obtain a higher\naccuracy for identifying squats, andk-NN spends less time to\nretrain the classiﬁer. Fig. 6(c) and (d) shows the validation\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 421\nTABLE III\nTHE COMPARATIVERESULTS AMONG HC CLASSIFIER BASED ON K-NN, DT', ',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2481.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q13,Which type of explainability techniques are used?,Unknown from this paper.,,,,"[3, 0, 2]","['iss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions was strongly dependent on the extracted\nfeatures from the used sensors. Different features were utilized\nfor achieving several kinds of HAR problems [22]. An efﬁcient\ngroup-based context-aware classiﬁcation method has been pro-\nposed to improve the classiﬁcation performance by exploiting\nhierarchical group-based scheme and context awareness rather\nthan the intensive computation [23].\nRecently, the advancement of HAR with inertial measurement\nunit (IMU) sensor broadly adopts DL algorithms. Ronao and\nCho [24] designed a three-(and four-) layer convolutional neural\nnetworks (CNN) with the short-time fast Fourier transform\nmethod for classifying six activities with a high classiﬁcation\naccuracy by comparing with the traditional ML algorithms, i.e.,\nNB, ANN, and j48 decision tree (DT). Hammerlaet al. [25]\nproved that bidirectional long short-term memory (Bi-LSTM)\nobtained more accurate results than deep NN, CNN, and other\nrecurrent neural network (RNN) by testing on three public\ndatasets. Although the DL-based approaches were widely known\nto extract features automatically, they were affected by the\nselected sensors. Wang et al. [8] compared the performance\nobtained using different inertial sensors in a smartphone. Using\nboth accelerometer and gyroscope was demonstrated to yield\naccuracy enhancement. In our previous work, we compared\nthe recognition rate for identifying 12 activities by using dif-\nferent combined sensors, such as accelerometer, gyroscope,\nmagnetometer, and orientation. By combining accelerometer,\ngyroscope, and direction, the designed deep CNN model could\nobtain high accuracy [26], [27]. The results proved that adopting\ntransferred signals and more sensors can perform the results.\nMoreover, the DL-based methods were time-consuming for\nclassiﬁer evolution in a dynamic situation [28].\nIII. PROBLEM STATEMENT\nThe following notations describe the data stream of the Ada-\nHAR system through training, testing, and updating procedures,\nas shown in Fig. 1. The embedded IMU sensors in a smartphone\ncan provide nine-dimensional (9-D) raw signalsS, including\nacceleration, magnetism, and angular velocity [29].S can be\ndivided into M segments, namely sp,p =1 ,2,...,M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile,', '414 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nA Smartphone-Based Adaptive Recognition and\nReal-Time Monitoring System for Human Activities\nWen Qi , Student Member , IEEE,H a n gS u, Member , IEEE, and Andrea Aliverti\nAbstract—Human activity recognition (HAR) using smart-\nphones provides signiﬁcant healthcare guidance for telemedicine\nand long-term treatment. Machine learning and deep learning\n(DL) techniques are widely utilized for the scientiﬁc study of the\nstatistical models of human behaviors. However, the performance\nof existing HAR platforms is limited by complex physical activity.\nIn this article, we proposed an adaptive recognition and real-time\nmonitoring system for human activities (Ada-HAR), which is ex-\npected to identify more human motions in dynamic situations. The\nAda-HAR framework introduces an unsupervised online learning\nalgorithm that is independent of the number of class constraints.\nFurthermore, the adopted hierarchical clustering and classiﬁca-\ntion algorithms label and classify 12 activities (ﬁve dynamics, six\nstatics, and a series of transitions) autonomously. Finally, practical\nexperiments have been performed to validate the effectiveness and\nrobustness of the proposed algorithms. Compared with the methods\nmentioned in the literature, the results show that the DL-based\nclassiﬁer obtains a higher recognition rate (95.15%, waist, and\n92.20%, pocket). The decision-tree-based classiﬁer is the fastest\nmethod for modal evolution. Finally, the Ada-HAR system can\nmonitor human activity in real time, regardless of the direction\nof the smartphone.\nIndex Terms —Data compression, deep learning (DL),\nhierarchical classiﬁcation (HC), human activity recognition\n(HAR).\nI. INTRODUCTION\nR\nECOGNIZING human activities using inertial sensors in\na smartphone has attracted increasing research interests\nduring the past decades in various domains, ranging from home\nhealthcare to sports monitoring, rehabilitation, personalized\nmedicine, and mental disorders [1]. Recently, with the develop-\nment of the Internet of Things, machine learning (ML), and deep\nlearning (DL) techniques, human activity recognition (HAR)\ncan be achieved by transferring data in the body area networks\nand wireless Ethernet, allowing the assessment of the human\nphysical and physiological status [2]. However, many studies\ndevelop appropriate tasks for a given HAR system by resorting\nto extensive heuristic knowledge [3]. They are applicable in a\nlaboratory or well-controlled situation via body-ﬁxed mobile\nManuscript received May 24, 2019; revised September 2, 2019 and January\n1, 2020; accepted February 23, 2020. Date of publication April 24, 2020; date of\ncurrent version September 15, 2020. This article was recommended by Associate\nEditor Bin Guo.(Corresponding author: Wen Qi.)\nThe authors are with the Dipartimento di Elettronica, Informazione\ne Bioingegneria, Politecnico di Milano, 20133 Milano, Italy (e-mail:\nwen.qi@polimi.it; hang.su@polimi.it; andrea.aliverti@polimi.it).\nColor versions of one or more of the ﬁgures in this article are available online\nat http://ieeexplore.ieee.org.\nDigital Object Identiﬁer 10.1109/THMS.2020.2984181\ndevices [4]. The variability of the disturbances of the mobile\ndevices affects the recognition rate of the HAR system, such as\nmovement artifacts, baseline noise, the happening of new activ-\nity, and the differences among users. For example, identifying\nthe same walking status of the elderly and', ' principal component analysis (PCA) [9], fast\n2168-2291 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 415\ncorrelation-based ﬁlter [10], and sequential forward selection\n(wrapper) [11] methods. By comparing the classiﬁcation rate of\nidentifying ﬁve daily activities carrying a smartphone on ﬁve\nbody positions, Khanet al.[12] proved that Kernel discriminant\nanalysis is better than linear discriminant analysis and signal\nmagnitude area. Shoaibet al. [13] demonstrated putting three\ntypes of sensors (i.e., accelerometer, gyroscope, and linear ac-\ncelerometer) on the wrist and pocket to identify less-repetitive\nactivities, such as smoking, eating, and drinking coffee. For\nsolving the problem of online time series segmentation, Ignatov\net al. [14] constructed the phase trajectory matrix and applied\nthe PCA technique to extract features of the ﬁrst period. Sousa\net al.[15] investigated the classiﬁcation capability with different\nfeature sets, such as time-domain (mathematical and statis-\ntical parameters), frequency-domain (wavelet transform), and\ndiscrete-domain (symbolic representations). Although the above\nworks achieved signiﬁcant progress for HAR, the limitations\nstill existed. Most of them ignored the affection of shift changes\nin position and orientation of the smartphone due to motion\nartifacts.\nDuring the past decades, many researchers have explored\nvarious ML and DL algorithms to identify more complex ac-\ntivities with higher recognition rates. Koseet al.[1] proved that\nk-nearest neighbor (k-NN) could obtain a higher classiﬁcation\naccuracy than Na¨ive Bayes (NB) by recognizing four kinds\nof activities, i.e., walking, running, sitting, and standing. Lee\nand Cho [16] designed a mixture-of-experts model for dealing\nwith uncertain and incomplete data. Reyes-Ortiz et al. [17]\nproposed a transition-aware HAR system based on the support\nvector machine (SVM) algorithm to classify a broad spectrum\n(up to 33 types) of activity with smartphone in real-time. Lee\nand Cho [18] presented a hierarchical hidden Markov model\n(HHMM) method to classify motions (standing, walking, go\nstairs, and running) and daily activities (shopping, taking a\nbus, and moving arms). A two-stage continuous hidden Markov\nmodel (HMM) algorithm was proposed to reduce the number of\nfeature subsets because it decomposed the complex activities\ninto several simpler ones [19]. Reiss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2525.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q14,Which evaluation metrics or outcome measures are used to assess the predictive models?,"Accuracy, training time, test time.",,,,"[11, 10, 5]","[' +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe same label, namely, ρr = corr(s∗\nt,s∗\nqˆyt ),r =1 ,2,...,q ˆyt\n(line 1). Ifρt <η is found, the classiﬁer will be rebuilt on the\nupdated training dataset{[s∗\ni;s∗\nt],[yi;nc +1]}, where the new\ntraining dataset includes the new inputs∗\nt with a new digital label\nnc +1 (line 3). In this article,η =0 .8.\nV. EXPERIMENTS AND RESULTS\nThree experiments were designed to validate the claims about\nthe Ada-HAR system in Section I. The ﬁrst experiment of the HC\nalgorithm evaluation aimed to compare the performance among\nML-based and DL-based HC classiﬁers. The second experiment\nwas to validate the evolution ability of the Ada-HAR system.\nThe demonstration revealed the identiﬁcation capability of Ada-\nHAR system in real time. The experiments have implemented\nthese methods in MATLAB 2018b with the hardware platform of\nIntel(R) i7 Core, 2.80-GHz CPU, and 16.0-GB RAM. A 64-bit\niPhone 6 s (2-core CPU) was utilized for collecting data with a\n50-Hz sampling frequency.\nA. HC Algorithm Evaluation\nTwenty-ﬁve subjects (13 females and 12 males, age range\n18–40 years old) were asked to perform the 12 original activities\ndescribed in Section I in a ﬁxed order (shown in Fig. 4). They\ncarried the smartphone on the waist or put it in their left pant\npocket, respectively. Finally, 50 datasets (25 datasets for each\nposition) were collected for further analysis.\nWe adopt the leave-one-out cross-validation strategy to vali-\ndate the performance of HC classiﬁers [40]. The length of the\ndetection window is 150 samples (3 s), and the overlap is 1 s\n(50 estimates). Table III reports a comparison of the average and\nstandard deviation in terms of the accuracy, training time, and\naverage test time among ML-based (i.e.,k-NN, DT, NB, ANN,\nand SVM) and DL-based (LSTM) HC classiﬁers. Meanwhile,\nall of the HC classiﬁers are evaluated on uncompressed and no-\ncompressed datasets. Although the LSTM-based HC classiﬁer\nobtains the highest accuracy, it is time-consuming for building\nthe model. Due to the SVM-based HC algorithm adopting the\nlinear kernel function, it takes more time to search the optimal\nsolution. In the same case, the k-NN-based approach is the\nfastest method to establish the HC classiﬁer, while the DT-based\nclassiﬁer is the fastest algorithm to predict the activity. Moreover,\nthe testing accuracy displays that putting the smartphone in the\npocket acquires more noises than carrying the smartphone on\nthe waist. The results for the', 'Ld +Lo)=1 ,\n2: identify dynamic or static activity byˆyt = f1(X,θ1)\nbased on featureσ(∑ ||Sα|l|2));\n3: if ˆyt ∈ ΛD, identify a dynamical activity on featuresD\nor segments∗\nt;\n4: else ˆyt ∈ ΛS, identify a static activity on featuresS or\nsegment s∗\nt;\n5: end if\n6: end while\nFor accuracy enhancement, the SVM model commonly utilizes\nthe radial basis function kernel and linear kernel commonly.\nAs a standard RNN method, the LSTM algorithm learns the\nlong-term dependencies based on the three gates, namely an\ninput gate, a forget gate to allow the LSTM unit to unlearn the\nprevious memory, and an output gate to decide the quantity of\nmemory transferring to the next hidden layers. However, it will\ncost ample time to optimize the parameters of the LSTM network\nand predict a result.\nC. Recognition Module\nFig. 5 shows the details of the communication network for\nmonitoring daily human activities in real time with a local LAN\nand a remote LAN. Algorithm 3 describes the procedure for\nidentifying human activities. When a new inputst is divided as a\ndynamical or static activity by the ﬁrst classiﬁerˆyt = f1(X,θ1)\n(line 1), it will be further identiﬁed as a dynamical or a static\nactivity by the second or third classiﬁer (lines 3 and 4).\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n420 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nAlgorithm 4: The online learning algorithm.\nInput: the old HC classiﬁersf(X,θ) and new predicted\ninput {s∗\nt,ˆyt};\nOutput: the updated HC classiﬁersˆf(X, ˆθ);\n1: compute covariance coefﬁcients ρr = corr(s∗\nt,s∗\nqˆyt );\n2: if ρr ≥ η\n3: update training dataset {[s∗\ni;s∗\nt],[yi;nc +1]};\n4: if ˆyt ∈ ΛD\n5: retrain the second classiﬁer ˆf2(X, ˆθ) on\n{F[s∗\ni;s∗\nt],[yi;nc +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n6: else ˆyt ∈ ΛS\n7: retrain the third classiﬁer ˆf3(X, ˆθ) on\n{F[s∗\ni;s∗\nt],[yi;nc +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe', ' ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label the activities au-\ntonomously. The HC [7] algorithms are used to establish the\nclassiﬁer for recognizing the 12 original activities (described in\nSection I). In the recognition module, the obtained HC classiﬁer\nis implemented for HAR in real time by carrying a smartphone\non the waist or putting it in the left pant pocket. In the online\nlearning module, a new activity that is not included in the 12\noriginal activities will be identiﬁed in an unsupervised learning\nmanner. Meanwhile, the old classiﬁer is updated. The signal\npreprocessing and feature extraction models are shared in both\ncreation and recognition modules.\nA. Signal Preprocessing and Feature Extraction\nThe collected signals typically exhibit various turbulence\n(e.g., magnetic ﬁelds and movement artifact), which affects the\nrecognition ability of the classiﬁer. The derived sensors signals\n(e.g., gravity and linear acceleration) that are transferred by the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 417\nFig. 3. Schematic diagram of signal preprocessing model. The 9-D raw signals [Sα (accelerometer), Sβ (gyroscope), andSγ (magnetometer)] are processed\nby the L1-norm (Manhattan distance) transfer, third-order zero phases low-pass elliptical ﬁlter (LPEF), attitude and heading reference system algorithm (AHRS\nﬁlter), and sum of gyroscope signals algorithms to provide the inputs, namely, the linear L1 norm acceleration|Sα|l, YZ-axis L1 norm orientation|S⊖ YZ |,s u m\nof gyroscope\n∑\nSβ, and L1-norm accelerometer signals|Sα|.\noriginal IMU sensors, i.e.,Sα (accelerometer), Sβ (gyroscope),\nand Sγ (magnetometer), can solve the problems.\nFirst, the linear L1-norm accelerometer|Sα| [by (4)] can ﬁx\nthe three axes of acceleration for overcoming the inﬂuence of\nthe changes in direction and position\n||Sα|| =\nLd∑\ni=1\n|sαi | (4)\nwhere sα is the subsegment of Sα with Ld length. Then, a\nthird-order zero phase low-pass elliptical ﬁlter (LPEF) [31] is\nimplemented to decompose|Sα| into the gravity and the linear\nacceleration (|Sα|l) vectors to remove the high-order noises.\nSecond, the attitude and heading reference system algo-\nrithm (AHRS ﬁlter) [32] calculates the orientation axesS⊖ and\ndetermine the smartphone’s reference system to increase the\nidentiﬁcation ability']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2463.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q15,What considerations were given to selected evaluation metrics or outcome measures in this study?,Unknown from this paper.,,,,"[3, 10, 18]","['iss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions was strongly dependent on the extracted\nfeatures from the used sensors. Different features were utilized\nfor achieving several kinds of HAR problems [22]. An efﬁcient\ngroup-based context-aware classiﬁcation method has been pro-\nposed to improve the classiﬁcation performance by exploiting\nhierarchical group-based scheme and context awareness rather\nthan the intensive computation [23].\nRecently, the advancement of HAR with inertial measurement\nunit (IMU) sensor broadly adopts DL algorithms. Ronao and\nCho [24] designed a three-(and four-) layer convolutional neural\nnetworks (CNN) with the short-time fast Fourier transform\nmethod for classifying six activities with a high classiﬁcation\naccuracy by comparing with the traditional ML algorithms, i.e.,\nNB, ANN, and j48 decision tree (DT). Hammerlaet al. [25]\nproved that bidirectional long short-term memory (Bi-LSTM)\nobtained more accurate results than deep NN, CNN, and other\nrecurrent neural network (RNN) by testing on three public\ndatasets. Although the DL-based approaches were widely known\nto extract features automatically, they were affected by the\nselected sensors. Wang et al. [8] compared the performance\nobtained using different inertial sensors in a smartphone. Using\nboth accelerometer and gyroscope was demonstrated to yield\naccuracy enhancement. In our previous work, we compared\nthe recognition rate for identifying 12 activities by using dif-\nferent combined sensors, such as accelerometer, gyroscope,\nmagnetometer, and orientation. By combining accelerometer,\ngyroscope, and direction, the designed deep CNN model could\nobtain high accuracy [26], [27]. The results proved that adopting\ntransferred signals and more sensors can perform the results.\nMoreover, the DL-based methods were time-consuming for\nclassiﬁer evolution in a dynamic situation [28].\nIII. PROBLEM STATEMENT\nThe following notations describe the data stream of the Ada-\nHAR system through training, testing, and updating procedures,\nas shown in Fig. 1. The embedded IMU sensors in a smartphone\ncan provide nine-dimensional (9-D) raw signalsS, including\nacceleration, magnetism, and angular velocity [29].S can be\ndivided into M segments, namely sp,p =1 ,2,...,M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile,', 'Ld +Lo)=1 ,\n2: identify dynamic or static activity byˆyt = f1(X,θ1)\nbased on featureσ(∑ ||Sα|l|2));\n3: if ˆyt ∈ ΛD, identify a dynamical activity on featuresD\nor segments∗\nt;\n4: else ˆyt ∈ ΛS, identify a static activity on featuresS or\nsegment s∗\nt;\n5: end if\n6: end while\nFor accuracy enhancement, the SVM model commonly utilizes\nthe radial basis function kernel and linear kernel commonly.\nAs a standard RNN method, the LSTM algorithm learns the\nlong-term dependencies based on the three gates, namely an\ninput gate, a forget gate to allow the LSTM unit to unlearn the\nprevious memory, and an output gate to decide the quantity of\nmemory transferring to the next hidden layers. However, it will\ncost ample time to optimize the parameters of the LSTM network\nand predict a result.\nC. Recognition Module\nFig. 5 shows the details of the communication network for\nmonitoring daily human activities in real time with a local LAN\nand a remote LAN. Algorithm 3 describes the procedure for\nidentifying human activities. When a new inputst is divided as a\ndynamical or static activity by the ﬁrst classiﬁerˆyt = f1(X,θ1)\n(line 1), it will be further identiﬁed as a dynamical or a static\nactivity by the second or third classiﬁer (lines 3 and 4).\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n420 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nAlgorithm 4: The online learning algorithm.\nInput: the old HC classiﬁersf(X,θ) and new predicted\ninput {s∗\nt,ˆyt};\nOutput: the updated HC classiﬁersˆf(X, ˆθ);\n1: compute covariance coefﬁcients ρr = corr(s∗\nt,s∗\nqˆyt );\n2: if ρr ≥ η\n3: update training dataset {[s∗\ni;s∗\nt],[yi;nc +1]};\n4: if ˆyt ∈ ΛD\n5: retrain the second classiﬁer ˆf2(X, ˆθ) on\n{F[s∗\ni;s∗\nt],[yi;nc +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n6: else ˆyt ∈ ΛS\n7: retrain the third classiﬁer ˆf3(X, ˆθ) on\n{F[s∗\ni;s∗\nt],[yi;nc +1]} (ML-based) or\n{[s∗\ni;s∗\nt],[yi;nc +1]} (DL-based);\n8: end if\n9: else continue end if\nD. Online Learning Module\nThe online learning module aims to update the previous HC\nclassiﬁer when a new activity is found. An unsupervised learning\napproach (Algorithm 4) is proposed to measure the similarity\ndegree between the new inputss∗\nt and saved training datasets∗\nqk.\nWhen the predicted activityˆyt is available, the similarity mea-\nsurement mechanism will compute the correlation coefﬁcients\nρ sequentially between s∗\nt and the saved training sets∗\nqˆyt with\nthe', ' inertial and magnetic sensing of human\nbody segment orientation,”IEEE Trans. Neural Syst. Rehabil. Eng., vol. 13,\nno. 3, pp. 395–405, Sep. 2005.\n[33] A. Rai and S. Upadhyay, “Bearing performance degradation assess-\nment based on a combination of empirical mode decomposition and\nk-medoids clustering,” Mech. Syst. Signal Process., vol. 93, pp. 16–29,\n2017.\n[34] R. Boddy and G. Smith,Statistical Methods in Practice: For Scientists\nand Technologists. Chichester, U.K.: Wiley, 2009.\n[35] C. Dai, W. Chen, and Y . Zhu, “Seeker optimization algorithm for digital\nIIR ﬁlter design,”IEEE Trans. Ind. Electron., vol. 57, no. 5, pp. 1710–1718,\nMay 2009.\n[36] N. M. Hewahi and M. K. Saad, “Class outliers mining: Distance-based\napproach.” Int. J. Intell. Syst. Techn., vol. 2, no. 1, pp. 55–68, 2007.\n[37] A. Liaw et al., “Classiﬁcation and regression by randomforest,”RN e w s,\nvol. 2, no. 3, pp. 18–22, 2002.\n[38] S. D. Jadhav and H. Channe, “Comparative study of k-NN, Naive Bayes\nand decision tree classiﬁcation techniques,”Int. J. Sci. Res., vol. 5, no. 1,\npp. 1842–1845, 2016.\n[39] S. J. Russell and P. Norvig,Artiﬁcial Intelligence: A Modern Approach.\nMalaysia: Pearson Education Limited, 2016.\n[40] Y . Wang, X. Jia, Q. Jin, and J. Ma, “Mobile crowdsourcing: framework,\nchallenges, and solutions,”Concurrency Comput.: Practice Experience,\nvol. 29, no. 3, 2017, Art. no. e3789.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. ']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2199.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q16,"How were robustness, confidence or statistical significance of the results assessed in this study?",Unknown from this paper.,,,,"[5, 18, 17]","[' ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label the activities au-\ntonomously. The HC [7] algorithms are used to establish the\nclassiﬁer for recognizing the 12 original activities (described in\nSection I). In the recognition module, the obtained HC classiﬁer\nis implemented for HAR in real time by carrying a smartphone\non the waist or putting it in the left pant pocket. In the online\nlearning module, a new activity that is not included in the 12\noriginal activities will be identiﬁed in an unsupervised learning\nmanner. Meanwhile, the old classiﬁer is updated. The signal\npreprocessing and feature extraction models are shared in both\ncreation and recognition modules.\nA. Signal Preprocessing and Feature Extraction\nThe collected signals typically exhibit various turbulence\n(e.g., magnetic ﬁelds and movement artifact), which affects the\nrecognition ability of the classiﬁer. The derived sensors signals\n(e.g., gravity and linear acceleration) that are transferred by the\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\nQI et al.: SMARTPHONE-BASED ADAPTIVE RECOGNITION AND REAL-TIME MONITORING SYSTEM FOR HUMAN ACTIVITIES 417\nFig. 3. Schematic diagram of signal preprocessing model. The 9-D raw signals [Sα (accelerometer), Sβ (gyroscope), andSγ (magnetometer)] are processed\nby the L1-norm (Manhattan distance) transfer, third-order zero phases low-pass elliptical ﬁlter (LPEF), attitude and heading reference system algorithm (AHRS\nﬁlter), and sum of gyroscope signals algorithms to provide the inputs, namely, the linear L1 norm acceleration|Sα|l, YZ-axis L1 norm orientation|S⊖ YZ |,s u m\nof gyroscope\n∑\nSβ, and L1-norm accelerometer signals|Sα|.\noriginal IMU sensors, i.e.,Sα (accelerometer), Sβ (gyroscope),\nand Sγ (magnetometer), can solve the problems.\nFirst, the linear L1-norm accelerometer|Sα| [by (4)] can ﬁx\nthe three axes of acceleration for overcoming the inﬂuence of\nthe changes in direction and position\n||Sα|| =\nLd∑\ni=1\n|sαi | (4)\nwhere sα is the subsegment of Sα with Ld length. Then, a\nthird-order zero phase low-pass elliptical ﬁlter (LPEF) [31] is\nimplemented to decompose|Sα| into the gravity and the linear\nacceleration (|Sα|l) vectors to remove the high-order noises.\nSecond, the attitude and heading reference system algo-\nrithm (AHRS ﬁlter) [32] calculates the orientation axesS⊖ and\ndetermine the smartphone’s reference system to increase the\nidentiﬁcation ability', ' inertial and magnetic sensing of human\nbody segment orientation,”IEEE Trans. Neural Syst. Rehabil. Eng., vol. 13,\nno. 3, pp. 395–405, Sep. 2005.\n[33] A. Rai and S. Upadhyay, “Bearing performance degradation assess-\nment based on a combination of empirical mode decomposition and\nk-medoids clustering,” Mech. Syst. Signal Process., vol. 93, pp. 16–29,\n2017.\n[34] R. Boddy and G. Smith,Statistical Methods in Practice: For Scientists\nand Technologists. Chichester, U.K.: Wiley, 2009.\n[35] C. Dai, W. Chen, and Y . Zhu, “Seeker optimization algorithm for digital\nIIR ﬁlter design,”IEEE Trans. Ind. Electron., vol. 57, no. 5, pp. 1710–1718,\nMay 2009.\n[36] N. M. Hewahi and M. K. Saad, “Class outliers mining: Distance-based\napproach.” Int. J. Intell. Syst. Techn., vol. 2, no. 1, pp. 55–68, 2007.\n[37] A. Liaw et al., “Classiﬁcation and regression by randomforest,”RN e w s,\nvol. 2, no. 3, pp. 18–22, 2002.\n[38] S. D. Jadhav and H. Channe, “Comparative study of k-NN, Naive Bayes\nand decision tree classiﬁcation techniques,”Int. J. Sci. Res., vol. 5, no. 1,\npp. 1842–1845, 2016.\n[39] S. J. Russell and P. Norvig,Artiﬁcial Intelligence: A Modern Approach.\nMalaysia: Pearson Education Limited, 2016.\n[40] Y . Wang, X. Jia, Q. Jin, and J. Ma, “Mobile crowdsourcing: framework,\nchallenges, and solutions,”Concurrency Comput.: Practice Experience,\nvol. 29, no. 3, 2017, Art. no. e3789.\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. ', ' 2017.\n[23] L. Cao, Y . Wang, B. Zhang, Q. Jin, and A. V . Vasilakos, “Gchar: An efﬁcient\ngroup-based contextaware human activity recognition on smartphone,”J.\nParallel Distrib. Comput., vol. 118, pp. 67–80, 2018.\n[24] C. A. Ronao and S.-B. Cho, “Human activity recognition with smartphone\nsensors using deep learning neural networks,”Expert Syst. Appl., vol. 59,\npp. 235–244, 2016.\n[25] N. Y . Hammerla, S. Halloran, and T. Ploetz, “Deep, convolutional, and\nrecurrent models for human activity recognition using wearables,” 2016,\narXiv:1604.08880.\n[26] W. Qi, H. Su, C. Yang, G. Ferrigno, E. De Momi, and A. Aliverti,\n“A fast and robust deep convolutional neural networks for complex\nhuman activity recognition using smartphone,”Sensors, vol. 19, 2019,\nArt. no. 3731.\n[27] W. Qi and A. Aliverti, “A multimodal wearable system for continuous\nand real-time breathing pattern monitoring during daily activity,”IEEE J.\nBiomed. Health Inform., 2019.\n[28] Y . Liang, Z. Cai, J. Yu, Q. Han, and Y . Li, “Deep learning based inference\nof private information using embedded sensors in smart devices,”IEEE\nNetw., vol. 32, no. 4, pp. 8–14, Jul./Aug. 2018.\n[29] O. Särkkä, T. Nieminen, S. Suuriniemi, and L. Kettunen, “A multi-position\ncalibration method for consumer-grade accelerometers, gyroscopes, and\nmagnetometers to ﬁeld conditions,” IEEE Sensors J., vol. 17, no. 11,\npp. 3470–3481, Jun. 2017.\n[30] J. O. Laguna, A. G. Olaya, and D. Borrajo, “A dynamic sliding window\napproach for activity recognition,” inProc. Int. Conf. User Model., Adap-\ntation, Personalization, 2011, pp. 219–230.\n[31] P. Kanjiya, V . M. Khadkikar, and M. S. ElMoursi, “Adaptive low-pass ﬁlter\nbased dc offset removal technique for three-phase PLLs,”IEEE Trans. Ind.\nElectron., vol. 65, no. 11, pp. 9025–9029, Nov. 2018.\n[32] D. Roetenberg, H. J. Luinge, C. T. Baten, and P. H. Veltink, “Compensation\nof magnetic disturbances improves inertial and magnetic sensing of human\nbody segment orientation,”IEEE Trans. Neural Syst. Rehabil. Eng., vol. 13,\nno. 3, pp. 395–405, Sep. 2005.\n[33] A. Rai and S. Upadhyay, “Bearing performance degradation assess-\nment based on a combination of empirical mode decomposition and\nk-medoids clustering,” Mech. Syst. Signal Process., vol. 93, pp. 16–29,\n2017.\n[34] R. Boddy and G. Smith,Statistical Methods in Practice: For Scientists\nand Technologists. Chichester, U.K.: Wiley, 2009.\n[35] C. Dai, W. Chen, and Y . Zhu']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2239.0,free_text
A_Smartphone_Based_Adaptive_Recognition_and_Real_Time_Monitoring_System_for_Human_Activities,Q17,What limitations of the study were discussed?,Unknown from this paper.,,,,"[3, 4, 6]","['iss et al. [20] found that\nthe ConfAdaBoost.M1 ensemble algorithm could obtain a high\nrecognition accuracy, but it was necessary to extract 561 features.\nNoor et al.[21] introduced an adaptive sliding window approach\nto recognize physical activity by computing the probability of\nsignals with an adjustable window length. All of the above\ncontributions were limited by the length of the detected signal\nsegments. Some of them focused on a short segment, which\ncannot express a whole activity. Some of them detected on the\nlong signal segment, which affected misclassiﬁcation due to a\nsegment including many activities. Hence, it is essential to ﬁnd a\nsuitable data length for classiﬁcation. However, the performance\nof ML-based solutions was strongly dependent on the extracted\nfeatures from the used sensors. Different features were utilized\nfor achieving several kinds of HAR problems [22]. An efﬁcient\ngroup-based context-aware classiﬁcation method has been pro-\nposed to improve the classiﬁcation performance by exploiting\nhierarchical group-based scheme and context awareness rather\nthan the intensive computation [23].\nRecently, the advancement of HAR with inertial measurement\nunit (IMU) sensor broadly adopts DL algorithms. Ronao and\nCho [24] designed a three-(and four-) layer convolutional neural\nnetworks (CNN) with the short-time fast Fourier transform\nmethod for classifying six activities with a high classiﬁcation\naccuracy by comparing with the traditional ML algorithms, i.e.,\nNB, ANN, and j48 decision tree (DT). Hammerlaet al. [25]\nproved that bidirectional long short-term memory (Bi-LSTM)\nobtained more accurate results than deep NN, CNN, and other\nrecurrent neural network (RNN) by testing on three public\ndatasets. Although the DL-based approaches were widely known\nto extract features automatically, they were affected by the\nselected sensors. Wang et al. [8] compared the performance\nobtained using different inertial sensors in a smartphone. Using\nboth accelerometer and gyroscope was demonstrated to yield\naccuracy enhancement. In our previous work, we compared\nthe recognition rate for identifying 12 activities by using dif-\nferent combined sensors, such as accelerometer, gyroscope,\nmagnetometer, and orientation. By combining accelerometer,\ngyroscope, and direction, the designed deep CNN model could\nobtain high accuracy [26], [27]. The results proved that adopting\ntransferred signals and more sensors can perform the results.\nMoreover, the DL-based methods were time-consuming for\nclassiﬁer evolution in a dynamic situation [28].\nIII. PROBLEM STATEMENT\nThe following notations describe the data stream of the Ada-\nHAR system through training, testing, and updating procedures,\nas shown in Fig. 1. The embedded IMU sensors in a smartphone\ncan provide nine-dimensional (9-D) raw signalsS, including\nacceleration, magnetism, and angular velocity [29].S can be\ndivided into M segments, namely sp,p =1 ,2,...,M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile,', ',M ,b yt h e\nsliding window method [30]. The segments can be labeled\nin a supervised manner as {sp,yp},p =1 ,2,...,M . A data\ncompression model is adopted to remove the similar segments\nfrom {sp,yp} with the same labels. The compressed datasets\n{si,yi},i =1 ,2,...,N (usually,N ≤ M) can be processed by\nthe signal preprocessing model for information enhancement as\n{s∗\ni,yi}. The proposed Ada-HAR system can build the classiﬁer\nf(X,θ)based on random ML or DL algorithms. The inputXcan\nbe either the extracted featuresFhs∗ or the processed signalss∗.\nMeanwhile, the dataset{s∗\ni,yi}will be reconstructed as a basic\ndataset s∗\nqk,k =1 ,2,...,n c;q ∈ R+ (where nc is the number\nof classes andqk is the label), for comparing with new activity.\nIn the testing procedure, the classiﬁer predicts a current ac-\ntivity ˆyt based on the extracted featureFhs∗\nt (ML-based) or the\nprocessed signals∗\nt (DL-based). The classiﬁcation errorεcan be\ncalculated through a supervised learning strategy by comparing\nˆyt with real labelyt as follows:\nε(s,y,X )= 1\nN\nN∑\nt=1\n[yt ̸= f(X,θ)] (1)\nwhere\n[yt ̸= f(X,θ)] =\n{\n1,y t ̸= f(X,θ)\n0,y t = f(X,θ) . (2)\nLet Θbe the overall set of classiﬁer parameters. The Ada-HAR\nsystem aims to ﬁnd the optimal parameter setθvia the following\nAuthorized licensed use limited to: Universiteit Leiden. Downloaded on May 13,2025 at 14:01:28 UTC from IEEE Xplore.  Restrictions apply. \n\n416 IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL. 50, NO. 5, OCTOBER 2020\nFig. 1. Data stream of Ada-HAR system. It includes training (blue line), testing (orange line), and updating (light tan line) procedures.\nFig. 2. Architecture of the adaptive real-time human activity recognition monitoring system (Ada-HAR). It includes a creation module IV(B) to labelactivities\nand builds the classiﬁer, a recognition module IV(C) to predict human activity in an online manner, and an online learning module IV(D) to update the classiﬁer in\nan unsupervised manner. Boxes with the same color adopt the same algorithms, such as the feature extraction (classiﬁcation) model in the creation andrecognition\nmodules. The shaded part is Section IV(A): Signal preprocessing and feature extraction.\nequation:\narg min\nθ∈Θ\nε(s,y,X ). (3)\nIn the updating procedure, the new processed input s∗\nt is\ncompared with the training datasets∗\nqˆyt\nby calculating the corre-\nlation coefﬁcientρ = corr(s∗\nt,s∗\nqˆyt\n).I fρ<η (we setη =0 .8),\nthe Ada-HAR system will retrain the classiﬁer on the updated\ntraining dataset{[s∗\ni;s∗\nt],[yi;nc +1]}.\nIV . METHODOLOGY\nFig. 2 shows the proposed Ada-HAR system with cre-\nation, recognition, and online learning modules. In the creation\nmodule, the Hk-mC [6] is adopted to label', ' three axes of acceleration for overcoming the inﬂuence of\nthe changes in direction and position\n||Sα|| =\nLd∑\ni=1\n|sαi | (4)\nwhere sα is the subsegment of Sα with Ld length. Then, a\nthird-order zero phase low-pass elliptical ﬁlter (LPEF) [31] is\nimplemented to decompose|Sα| into the gravity and the linear\nacceleration (|Sα|l) vectors to remove the high-order noises.\nSecond, the attitude and heading reference system algo-\nrithm (AHRS ﬁlter) [32] calculates the orientation axesS⊖ and\ndetermine the smartphone’s reference system to increase the\nidentiﬁcation ability on static activities. Instead of tracking the\norientation directly, the indirect Kalman ﬁlter models the error\nprocess x with a recursive update as follows:\nxt =\n⎡\n⎢⎢⎢⎣\nS⊖ t\nSαt\nSβt\nSγt\n⎤\n⎥⎥⎥⎦ = Ft\n⎡\n⎢⎢⎢⎣\nS⊖ t−1\nSαt−1\nSβt−1\nSγt−1\n⎤\n⎥⎥⎥⎦ +ωt (5)\nwhere the 12-by-1 vector xt is the output at time t, con-\nsisting of the original three sensors and the orientation com-\nposition S⊖ t . ωt is 12-by-1 additive noise vector and Ft is\nthe state transition model. Third, the sum of angular velocity,\nnamely ∑ Sβ = Sβx +Sβy +Sβz , can extract more dynamical\ninformation.\nFor accuracy enhancement, we select parts of the sig-\nnals as the outputs of signal preprocessing model. They are\nthe tri-axial linear L1 norm acceleration (|Sα|l), YZ-axis L1\nnorm orientation ( |S⊖ YZ |), the sum of gyroscope signals\n(∑ Sβ), and the L1-norm accelerometer (|Sα|), namely S∗ =\n[|Sα|l;|S⊖ YZ |;|S⊖ YZ |;∑ Sβ|Sα|] (shown in Fig. 3).\nIn addition, it is better to extract effective features (Table I)\nfor building an ML-based classiﬁer and divide them into a\ndynamical set D, a static set S, and a single feature D/S :\nTABLE I\nTHE EXTRACTED FEATURES INTIME DOMAIN\nσ(∑ ||Sα|l|2))for establishing an HC classiﬁer.|Sα|l and ∑ Sβ\nare used to provide dynamical features, while|S⊖ YZ | and |Sα|\nare selected to compute the static features. The meanings of\nthe symbols are as follows: Standard deviationσ, average (e.g.,\n¯∑ Sβ), maximummax, minimummin, difference (recurrence\nrelation) ▽ , and the whole numbers (N).\nB. Creation Module\nThe creation module includes two steps: One aims to achieve\nautomatic labeling using the Hk-mC algorithm, and the other\none is to build the HC classiﬁer for HAR. The Hk-mC algo-\nrithm 1 is implemented in an unsupervised manner with the\nﬁxed structure. It labels the extracted features setFmjj\ns∗p\n,jj =\n1,2,...,Ly m computed on the processed segments s']",kamalkraj/BioSimCSE-BioLinkBERT-BASE,deepseek/deepseek-chat-v3.1:free,stop,2459.0,free_text
